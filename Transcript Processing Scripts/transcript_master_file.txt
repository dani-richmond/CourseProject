We can compute this maximum estimate by using the EM algorithm.
So in the e step, we now have to introduce more hidden variables because we have more topics, so our hidden variable z now, which is a topic indicator can take more than two values.
So specifically will take a k plus one values, with b in the noting the background.
And once locate, to denote other k topics, right.
So, now the e step, as you can recall is your augmented data, and by predicting the values of the hidden variable.
So we're going to predict for a word, whether the word has come from one of these k plus one distributions.
This equation allows us to predict the probability that the word w in document d is generated from topic zero sub j.
And the bottom one is the predicted probability that this word has been generated from the background.
Note that we use document d here to index the word.
Why?
Because whether a word is from a particular topic actually depends on the document.
Can you see why?
Well, it's through the pi's.
The pi's are tied to each document.
Each document can have potentially different pi's, right.
The pi's will then affect our prediction.
So, the pi's are here.
And this depends on the document.
And that might give a different guess for a word in different documents, and that's desirable.
In both cases we are using the Baye's Rule, as I explained, basically assessing the likelihood of generating word from each of this division and there's normalize.
What about the m step?
Well, we may recall the m step is we take advantage of the inferred z values.
To split the counts.
And then collected the right counts to re-estimate the parameters.
So in this case, we can re-estimate our coverage of probability.
And this is re-estimated based on collecting all the words in the document.
And that's why we have the count of the word in document.
And sum over all the words.
And then we're going to look at to what extent this word belongs to the topic theta sub j.
And this part is our guess from each step.
This tells us how likely this word is actually from theta sub j.
And when we multiply them together, we get the discounted count that's located for topic theta sub j.
And when we normalize this over all the topics, we get the distribution of all the topics to indicate the coverage.
And similarly, the bottom one is the estimated probability of word for a topic.
And in this case we are using exact the same count, you can see this is the same discounted account, ] it tells us to what extend we should allocate this word [INAUDIBLE] but then normalization is different.
Because in this case we are interested in the word distribution, so we simply normalize this over all the words.
This is different, in contrast here we normalize the amount all the topics.
It would be useful to take a comparison between the two.
This give us different distributions.
And these tells us how to improve the parameters.
And as I just explained, in both the formula is we have a maximum estimate based on allocated word counts [INAUDIBLE].
Now this phenomena is actually general phenomena in all the EM algorithms.
In the m-step, you general with the computer expect an account of the event based on the e-step result, and then you just and then count to four, particular normalize it, typically.
So, in terms of computation of this EM algorithm, we can actually just keep accounting various events and then normalize them.
And when we thinking this way, we also have a more concise way of presenting the EM Algorithm.
It actually helps us better understand the formulas.
So I'm going to go over this in some detail.
So as a algorithm we first initialize all the unknown perimeters randomly, all right.
So, in our case, we are interested in all of those coverage perimeters, pi's and awarded distributions [INAUDIBLE], and we just randomly normalize them.
This is the initialization step and then we will repeat until likelihood converges.
Now how do we know whether likelihood converges?
We can do compute likelihood at each step and compare the current likelihood with the previous likelihood.
If it doesn't change much and we're going to say it stopped, right.
So, in each step we're going to do e-step and m-step.
In the e-step we're going to do augment the data by predicting the hidden variables.
In this case, the hidden variable, z sub d, w, indicates whether the word w in d is from a topic or background.
And if it's from a topic, which topic.
So if you look at the e-step formulas, essentially we're actually normalizing these counts, sorry, these probabilities of observing the word from each distribution.
So you can see, basically the prediction of word from topic zero sub j is based on the probability of selecting that theta sub j as a word distribution to generate the word.
Multiply by the probability of observing the word from that distribution.
And I said it's proportional to this because in the implementation of EM algorithm you can keep counter for this quantity, and in the end it just normalizes it.
So the normalization here is over all the topics and then you would get a probability.
Now, in the m-step, we do the same, and we are going to collect these.
Allocated account for each topic.
And we split words among the topics.
And then we're going to normalize them in different ways to obtain the real estimate.
So for example, we can normalize among all the topics to get the re-estimate of pi, the coverage.
Or we can re-normalize based on all the words.
And that would give us a word distribution.
So it's useful to think algorithm in this way because when implemented, you can just use variables, but keep track of these quantities in each case.
And then you just normalize these variables to make them distribution.
Now I did not put the constraint for this one.
And I intentionally leave this as an exercise for you.
And you can see, what's the normalizer for this one?
It's of a slightly different form but it's essentially the same as the one that you have seen here in this one.
So in general in the envisioning of EM algorithms you will see you accumulate the counts, various counts and then you normalize them.
So to summarize, we introduced the PLSA model.
Which is a mixture model with k unigram language models representing k topics.
And we also added a pre-determined background language model to help discover discriminative topics, because this background language model can help attract the common terms.
And we select the maximum estimate that we cant discover topical knowledge from text data.
In this case PLSA allows us to discover two things, one is k worded distributions, each one representing a topic and the other is the proportion of each topic in each document.
And such detailed characterization of coverage of topics in documents can enable a lot of photo analysis.
For example, we can aggregate the documents in the particular pan period to assess the coverage of a particular topic in a time period.
That would allow us to generate the temporal chains of topics.
We can also aggregate topics covered in documents associated with a particular author and then we can categorize the topics written by this author, etc.
And in addition to this, we can also cluster terms and cluster documents.
In fact, each topic can be regarded as a cluster.
So we already have the term clusters.
In the higher probability, the words can be regarded as belonging to one cluster represented by the topic.
Similarly, documents can be clustered in the same way.
We can assign a document to the topic cluster that's covered most in the document.
So remember, pi's indicate to what extent each topic is covered in the document, we can assign the document to the topical cluster that has the highest pi.
And in general there are many useful applications of this technique.
So, looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data.
And we're going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in real world, especially for decision making, or for completing whatever tasks that require text data to support.
Because, in general, in many real world problems of data mining we also tend to have other kinds of data that are non-textual.
So a more general picture would be to include non-text data as well.
And for this reason we might be concerned with joint mining of text and non-text data.
And so in this course we're going to focus more on text mining, but we're also going to also touch how do to joint analysis of both text data and non-text data.
With this problem definition we can now look at the landscape of the topics in text mining and analytics.
Now this slide shows the process of generating text data in more detail.
More specifically, a human sensor or human observer would look at the word from some perspective.
Different people would be looking at the world from different angles and they'll pay attention to different things.
The same person at different times might also pay attention to different aspects of the observed world.
And so the humans are able to perceive the world from some perspective.
And that human, the sensor, would then form a view of the world.
And that can be called the Observed World.
Of course, this would be different from the Real World because of the perspective that the person has taken can often be biased also.
Now the Observed World can be represented as, for example, entity-relation graphs or in a more general way, using knowledge representation language.
But in general, this is basically what a person has in mind about the world.
And we don't really know what exactly it looks like, of course.
But then the human would express what the person has observed using a natural language, such as English.
And the result is text data.
Of course a person could have used a different language to express what he or she has observed.
In that case we might have text data of mixed languages or different languages.
The main goal of text mining Is actually to revert this process of generating text data.
We hope to be able to uncover some aspect in this process.
Specifically, we can think about mining, for example, knowledge about the language.
And that means by looking at text data in English, we may be able to discover something about English, some usage of English, some patterns of English.
So this is one type of mining problems, where the result is some knowledge about language which may be useful in various ways.
If you look at the picture, we can also then mine knowledge about the observed world.
And so this has much to do with mining the content of text data.
We're going to look at what the text data are about, and then try to get the essence of it or extracting high quality information about a particular aspect of the world that we're interested in.
For example, everything that has been said about a particular person or a particular entity.
And this can be regarded as mining content to describe the observed world in the user's mind or the person's mind.
If you look further, then you can also imagine we can mine knowledge about this observer, himself or herself.
So this has also to do with using text data to infer some properties of this person.
And these properties could include the mood of the person or sentiment of the person.
And note that we distinguish the observed word from the person because text data can't describe what the person has observed in an objective way.
But the description can be also subjected with sentiment and so, in general, you can imagine the text data would contain some factual descriptions of the world plus some subjective comments.
So that's why it's also possible to do text mining to mine knowledge about the observer.
Finally, if you look at the picture to the left side of this picture, then you can see we can certainly also say something about the real world.
Right?
So indeed we can do text mining to infer other real world variables.
And this is often called a predictive analytics.
And we want to predict the value of certain interesting variable.
So, this picture basically covered multiple types of knowledge that we can mine from text in general.
When we infer other real world variables we could also use some of the results from mining text data as intermediate results to help the prediction.
For example, after we mine the content of text data we might generate some summary of content.
And that summary could be then used to help us predict the variables of the real world.
Now of course this is still generated from the original text data, but I want to emphasize here that often the processing of text data to generate some features that can help with the prediction is very important.
And that's why here we show the results of some other mining tasks, including mining the content of text data and mining knowledge about the observer, can all be very helpful for prediction.
In fact, when we have non-text data, we could also use the non-text data to help prediction, and of course it depends on the problem.
In general, non-text data can be very important for such prediction tasks.
For example, if you want to predict stock prices or changes of stock prices based on discussion in the news articles or in social media, then this is an example of using text data to predict some other real world variables.
But in this case, obviously, the historical stock price data would be very important for this prediction.
And so that's an example of non-text data that would be very useful for the prediction.
And we're going to combine both kinds of data to make the prediction.
Now non-text data can be also used for analyzing text by supplying context.
When we look at the text data alone, we'll be mostly looking at the content and/or opinions expressed in the text.
But text data generally also has context associated.
For example, the time and the location that associated are with the text data.
And these are useful context information.
And the context can provide interesting angles for analyzing text data.
For example, we might partition text data into different time periods because of the availability of the time.
Now we can analyze text data in each time period and then make a comparison.
Similarly we can partition text data based on locations or any meta data that's associated to form interesting comparisons in areas.
So, in this sense, non-text data can actually provide interesting angles or perspectives for text data analysis.
And it can help us make context-sensitive analysis of content or the language usage or the opinions about the observer or the authors of text data.
We could analyze the sentiment in different contexts.
So this is a fairly general landscape of the topics in text mining and analytics.
In this course we're going to selectively cover some of those topics.
We actually hope to cover most of these general topics.
First we're going to cover natural language processing very briefly because this has to do with understanding text data and this determines how we can represent text data for text mining.
Second, we're going to talk about how to mine word associations from text data.
And word associations is a form of use for lexical knowledge about a language.
Third, we're going to talk about topic mining and analysis.
And this is only one way to analyze content of text, but it's a very useful ways of analyzing content.
It's also one of the most useful techniques in text mining.
Then we're going to talk about opinion mining and sentiment analysis.
So this can be regarded as one example of mining knowledge about the observer.
And finally we're going to cover text-based prediction problems where we try to predict some real world variable based on text data.
So this slide also serves as a road map for this course.
And we're going to use this as an outline for the topics that we'll cover in the rest of this course.
This lecture is a continued discussion of Latent Aspect Rating Analysis.
Earlier, we talked about how to solve the problem of LARA in two stages.
But we first do segmentation of different aspects.
And then we use a latent regression model to learn the aspect ratings and then later the weight.
Now it's also possible to develop a unified generative model for solving this problem, and that is we not only model the generational over-rating based on text.
We also model the generation of text, and so a natural solution would be to use topic model.
So given the entity, we can assume there are aspects that are described by word distributions.
Topics.
And then we an use a topic model to model the generation of the reviewed text.
I will assume words in the review text are drawn from these distributions.
In the same way as we assumed for generating model like PRSA.
And then we can then plug in the latent regression model to use the text to further predict the overrating.
And that means when we first predict the aspect rating and then combine them with aspect weights to predict the overall rating.
So this would give us a unified generated model, where we model both the generation of text and the overall ready condition on text.
So we don't have time to discuss this model in detail as in many other cases in this part of the cause where we discuss the cutting edge topics, but there's a reference site here where you can find more details.
So now I'm going to show you some simple results that you can get by using these kind of generated models.
First, it's about rating decomposition.
So here, what you see are the decomposed ratings for three hotels that have the same overall rating.
So if you just look at the overall rating, you can't really tell much difference between these hotels.
But by decomposing these ratings into aspect ratings we can see some hotels have higher ratings for some dimensions, like value, but others might score better in other dimensions, like location.
And so this can give you detailed opinions at the aspect level.
Now here, the ground-truth is shown in the parenthesis, so it also allows you to see whether the prediction is accurate.
It's not always accurate but It's mostly still reflecting some of the trends.
The second result you compare different reviewers on the same hotel.
So the table shows the decomposed ratings for two reviewers about same hotel.
Again their high level overall ratings are the same.
So if you just look at the overall ratings, you don't really get that much information about the difference between the two reviewers.
But after you decompose the ratings, you can see clearly that they have high scores on different dimensions.
So this shows that model can review differences in opinions of different reviewers and such a detailed understanding can help us understand better about reviewers and also better about their feedback on the hotel.
This is something very interesting, because this is in some sense some byproduct.
In our problem formulation, we did not really have to do this.
But the design of the generating model has this component.
And these are sentimental weights for words in different aspects.
And you can see the highly weighted words versus the negatively loaded weighted words here for each of the four dimensions.
Value, rooms, location, and cleanliness.
The top words clearly make sense, and the bottom words also make sense.
So this shows that with this approach, we can also learn sentiment information directly from the data.
Now, this kind of lexicon is very useful because in general, a word like long, let's say, may have different sentiment polarities for different context.
So if I say the battery life of this laptop is long, then that's positive.
But if I say the rebooting time for the laptop is long, that's bad, right?
So even for reviews about the same product, laptop, the word long is ambiguous, it could mean positive or it could mean negative.
But this kind of lexicon, that we can learn by using this kind of generated models, can show whether a word is positive for a particular aspect.
So this is clearly very useful, and in fact such a lexicon can be directly used to tag other reviews about hotels or tag comments about hotels in social media like Tweets.
And what's also interesting is that since this is almost completely unsupervised, well assuming the reviews whose overall rating are available And then this can allow us to learn form potentially larger amount of data on the internet to reach sentiment lexicon.
And here are some results to validate the preference words.
Remember the model can infer wether a reviewer cares more about service or the price.
Now how do we know whether the inferred weights are correct?
And this poses a very difficult challenge for evaluation.
Now here we show some interesting way of evaluating.
What you see here are the prices of hotels in different cities, and these are the prices of hotels that are favored by different groups of reviewers.
The top ten are the reviewers was the highest inferred value to other aspect ratio.
So for example value versus location, value versus room, etcetera.
Now the top ten of the reviewers that have the highest ratios by this measure.
And that means these reviewers tend to put a lot of weight on value as compared with other dimensions.
So that means they really emphasize on value.
The bottom ten on the other hand of the reviewers.
The lowest ratio, what does that mean?
Well it means these reviewers have put higher weights on other aspects than value.
So those are people that cared about another dimension and they didn't care so much the value in some sense, at least as compared with the top ten group.
Now these ratios are computer based on the inferred weights from the model.
So now you can see the average prices of hotels favored by top ten reviewers are indeed much cheaper than those that are favored by the bottom ten.
And this provides some indirect way of validating the inferred weights.
It just means the weights are not random.
They are actually meaningful here.
In comparison, the average price in these three cities, you can actually see the top ten tend to have below average in price, whereas the bottom half, where they care a lot about other things like a service or room condition tend to have hotels that have higher prices than average.
So with these results we can build a lot of interesting applications.
For example, a direct application would be to generate the rated aspect, the summary, and because of the decomposition we have now generated the summaries for each aspect.
The positive sentences the negative sentences about each aspect.
It's more informative than original review that just has an overall rating and review text.
Here are some other results about the aspects that's covered from reviews with no ratings.
These are mp3 reviews, and these results show that the model can discover some interesting aspects.
Commented on low overall ratings versus those higher overall per ratings.
And they care more about the different aspects.
Or they comment more on the different aspects.
So that can help us discover for example, consumers' trend in appreciating different features of products.
For example, one might have discovered the trend that people tend to like larger screens of cell phones or light weight of laptop, etcetera.
Such knowledge can be useful for manufacturers to design their next generation of products.
Here are some interesting results on analyzing users rating behavior.
So what you see is average weights along different dimensions by different groups of reviewers.
And on the left side you see the weights of viewers that like the expensive hotels.
They gave the expensive hotels 5 Stars, and you can see their average rates tend to be more for some service.
And that suggests that people like expensive hotels because of good service, and that's not surprising.
That's also another way to validate it by inferred weights.
If you look at the right side where, look at the column of 5 Stars.
These are the reviewers that like the cheaper hotels, and they gave cheaper hotels five stars.
As we expected and they put more weight on value, and that's why they like the cheaper hotels.
But if you look at the, when they didn't like expensive hotels, or cheaper hotels, then you'll see that they tended to have more weights on the condition of the room cleanness.
So this shows that by using this model, we can infer some information that's very hard to obtain even if you read all the reviews.
Even if you read all the reviews it's very hard to infer such preferences or such emphasis.
So this is a case where text mining algorithms can go beyond what humans can do, to review interesting patterns in the data.
And this of course can be very useful.
You can compare different hotels, compare the opinions from different consumer groups, in different locations.
And of course, the model is general.
It can be applied to any reviews with overall ratings.
So this is a very useful technique that can support a lot of text mining applications.
Finally the results of applying this model for personalized ranking or recommendation of entities.
So because we can infer the reviewers weights on different dimensions, we can allow a user to actually say what do you care about.
So for example, I have a query here that shows 90% of the weight should be on value and 10% on others.
So that just means I don't care about other aspect.
I just care about getting a cheaper hotel.
My emphasis is on the value dimension.
Now what we can do with such query is we can use reviewers that we believe have a similar preference to recommend a hotels for you.
How can we know that?
Well, we can infer the weights of those reviewers on different aspects.
We can find the reviewers whose weights are more precise, of course inferred rates are similar to yours.
And then use those reviewers to recommend hotels for you and this is what we call personalized or rather query specific recommendations.
Now the non-personalized recommendations now shown on the top, and you can see the top results generally have much higher price, than the lower group and that's because when the reviewer's cared more about the value as dictated by this query they tended to really favor low price hotels.
So this is yet another application of this technique.
It shows that by doing text mining we can understand the users better.
And once we can handle users better we can solve these users better.
So to summarize our discussion of opinion mining in general, this is a very important topic and with a lot of applications.
And as a text sentiment analysis can be readily done by using just text categorization.
But standard technique tends to not be enough.
And so we need to have enriched feature implementation.
And we also need to consider the order of those categories.
And we'll talk about ordinal regression for some of these problem.
We have also assume that the generating models are powerful for mining latent user preferences.
This in particular in the generative model for mining latent regression.
And we embed some interesting preference information and send the weights of words in the model as a result we can learn most useful information when fitting the model to the data.
Now most approaches have been proposed and evaluated.
For product reviews, and that was because in such a context, the opinion holder and the opinion target are clear.
And they are easy to analyze.
And there, of course, also have a lot of practical applications.
But opinion mining from news and social media is also important, but that's more difficult than analyzing review data, mainly because the opinion holders and opinion targets are all interested.
So that calls for natural management processing techniques to uncover them accurately.
Here are some suggested readings.
The first two are small books that are of some use of this topic, where you can find a lot of discussion about other variations of the problem and techniques proposed for solving the problem.
The next two papers about generating models for rating the aspect rating analysis.
The first one is about solving the problem using two stages, and the second one is about a unified model where the topic model is integrated with the regression model to solve the problem using a unified model.
.
This lecture is about the syntagmatic relation discovery, and entropy.
In this lecture, we're going to continue talking about word association mining.
In particular, we're going to talk about how to discover syntagmatic relations.
And we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations.
By definition, syntagmatic relations hold between words that have correlated co-occurrences.
That means, when we see one word occurs in context, we tend to see the occurrence of the other word.
So, take a more specific example, here.
We can ask the question, whenever eats occurs, what other words also tend to occur?
Looking at the sentences on the left, we see some words that might occur together with eats, like cat, dog, or fish is right.
But if I take them out and if you look at the right side where we only show eats and some other words, the question then is.
Can you predict what other words occur to the left or to the right?
Right so this would force us to think about what other words are associated with eats.
If they are associated with eats, they tend to occur in the context of eats.
More specifically our prediction problem is to take any text segment which can be a sentence, a paragraph, or a document.
And then ask I the question, is a particular word present or absent in this segment?
Right here we ask about the word W. Is W present or absent in this segment?
Now what's interesting is that some words are actually easier to predict than other words.
If you take a look at the three words shown here, meat, the, and unicorn, which one do you think is easier to predict?
Now if you think about it for a moment you might conclude that the is easier to predict because it tends to occur everywhere.
So I can just say, well that would be in the sentence.
Unicorn is also relatively easy because unicorn is rare, is very rare.
And I can bet that it doesn't occur in this sentence.
But meat is somewhere in between in terms of frequency.
And it makes it harder to predict because it's possible that it occurs in a sentence or the segment, more accurately.
But it may also not occur in the sentence, so now let's study this problem more formally.
So the problem can be formally defined as predicting the value of a binary random variable.
Here we denote it by X sub w, w denotes a word, so this random variable is associated with precisely one word.
When the value of the variable is 1, it means this word is present.
When it's 0, it means the word is absent.
And naturally, the probabilities for because a word is either present or absent in a segment.
There's no other choice.
So the intuition with this concept earlier can be formally stated as follows.
The more random this random variable is, the more difficult the prediction will be.
Now the question is how does one quantitatively measure the randomness of a random variable like X sub w?
How in general, can we quantify the randomness of a variable and that's why we need a measure called entropy and this measure introduced in information theory to measure the randomness of X.
There is also some connection with information here but that is beyond the scope of this course.
So for our purpose we just treat entropy function as a function defined on a random variable.
In this case, it is a binary random variable, although the definition can be easily generalized for a random variable with multiple values.
Now the function form looks like this, there's the sum of all the possible values for this random variable.
Inside the sum for each value we have a product of the probability that the random variable equals this value and log of this probability.
And note that there is also a negative sign there.
Now entropy in general is non-negative.
And that can be mathematically proved.
So if we expand this sum, we'll see that the equation looks like the second one.
Where I explicitly plugged in the two values, 0 and 1.
And sometimes when we have 0 log of 0, we would generally define that as 0, because log of 0 is undefined.
So this is the entropy function.
And this function will give a different value for different distributions of this random variable.
And it clearly depends on the probability that the random variable taking value of 1 or 0.
If we plot this function against the probability that the random variable is equal to 1.
And then the function looks like this.
At the two ends, that means when the probability of X equals 1 is very small or very large, then the entropy function has a low value.
When it's 0.5 in the middle then it reaches the maximum.
Now if we plot the function against the probability that X is taking a value of 0 and the function would show exactly the same curve here, and you can imagine why.
And so that's because the two probabilities are symmetric, and completely symmetric.
So an interesting question you can think about in general is for what kind of X does entropy reach maximum or minimum.
And we can in particular think about some special cases.
For example, in one case, we might have a random variable that always takes a value of 1.
The probability is 1.
Or there's a random variable that is equally likely taking a value of one or zero.
So in this case the probability that X equals 1 is 0.5.
Now which one has a higher entropy?
It's easier to look at the problem by thinking of a simple example using coin tossing.
So when we think about random experiments like tossing a coin, it gives us a random variable, that can represent the result.
It can be head or tail.
So we can define a random variable X sub coin, so that it's 1 when the coin shows up as head, it's 0 when the coin shows up as tail.
So now we can compute the entropy of this random variable.
And this entropy indicates how difficult it is to predict the outcome of a coin toss.
So we can think about the two cases.
One is a fair coin, it's completely fair.
The coin shows up as head or tail equally likely.
So the two probabilities would be a half.
Right?
So both are equal to one half.
Another extreme case is completely biased coin, where the coin always shows up as heads.
So it's a completely biased coin.
Now let's think about the entropies in the two cases.
And if you plug in these values you can see the entropies would be as follows.
For a fair coin we see the entropy reaches its maximum, that's 1.
For the completely biased coin, we see it's 0.
And that intuitively makes a lot of sense.
Because a fair coin is most difficult to predict.
Whereas a completely biased coin is very easy to predict.
We can always say, well, it's a head.
Because it is a head all the time.
So they can be shown on the curve as follows.
So the fair coin corresponds to the middle point where it's very uncertain.
The completely biased coin corresponds to the end point where we have a probability of 1.0 and the entropy is 0.
So, now let's see how we can use entropy for word prediction.
Let's think about our problem is to predict whether W is present or absent in this segment.
Again, think about the three words, particularly think about their entropies.
Now we can assume high entropy words are harder to predict.
And so we now have a quantitative way to tell us which word is harder to predict.
Now if you look at the three words meat, the, unicorn, again, and we clearly would expect meat to have a higher entropy than the unicorn.
In fact if you look at the entropy of the, it's close to zero.
Because it occurs everywhere.
So it's like a completely biased coin.
Therefore the entropy is zero.
In this lecture, we continue the discussion of Vector Space Model.
In particular, we are going to talk about the TF transformation.
In the previous lecture, we have derived a TF-IDF weighting formula using the vector space model.
And we have shown that this model actually works pretty well for these examples as shown on this slide except for d5, which has received a very high score.
Indeed, it has received the highest score among all these documents.
But this document is intuitively non-relevant, so this is not desirable.
In this lecture, we're going to talk about how would you use TF transformation to solve this problem.
Before we discuss the details, let's take a look at the formula for this symbol here for IDF weighting ranking function and see why this document has received such a high score.
So this is the formula, and if you look at the formula carefully, then you will see it involves a sum over all the matched query terms.
And inside the sum, each matched query sum has a particular weight.
And this weight is TF-IDF weighting.
So it has an IDF component where we see 2 variables.
One is the total number of documents in the collection, and that is m. The other is the documentive frequency.
This is the number of documents that contain this word w. The other variables in, involving the formula, include the count of the query term.
W in the query, and the count of the word in the document.
If you look at this document again, now it's not hard to realize that the reason why it has received a high score is because it has a very high count of campaign.
So the count of campaign in this document is a four, which is much higher than the other documents, and has contributed to the high score of this document.
So intriguingly, in order to lower the score for this document, we need to somehow restrict the contribution of, the matching of this term in the document.
And if you think about the matching of terms in the document carefully you actually would realize we probably shouldn't reward multiple occurrences so generously.
And by that I mean the first occurrence of a term says a lot about the, the matching of this term, because it goes from zero count to a count of one, and that increase means a lot.
Once we see a word in the document, it's very likely that the document is talking about this word.
If we see an extra occurrence on top of the first occurrence, that is to go from one to two, then we also can say that well, the second occurrence kind of confirmed that it's not a accidental mention of the word.
Now, we are more sure that this document is talking about this word.
But imagine we have seen, let's say, Then, adding one extra occurrence is not going to test more about evidence because we are already sure that this document is about this word.
So if you're thinking this way it seems that we should restrict the contributing of a high account of term.
And that is the idea of TF Transformation.
So this transformation function is going to turn the raw count of word into a Term Frequency Weight, for the word in the document.
So here I show in x-axis, that raw count, and in y-axis I show the Term Frequency Weight.
So, in the previous ranking functions we actually have increasingly, used some kind of transformation.
So for example in the zero-one bit vector retentation we actually use the Suchier transformation function as shown here.
Basically if the count is zero then it has zero weight.
Otherwise it would have a weight of one.
It's flat.
Now what about using Term Count as a TF weight.
Well that's a linear function, right?
So it has just exactly the same weight as the count.
Now we have just seen that this is not desirable.
So what we want is something like this.
So for example with a logarithm function, we can have a sub-linear transformation that looks like this.
And this will control the influence of really high weight because it's going to lower its inference, yet it will retain the inference of small count.
Or we might want to even bend the curve more by applying logarithm twice.
Now people have tried all these methods and they are indeed working better than the linear form of the transformation, but so far what works the best seems to be this special transformation called a BM25 transformation.
BM stands for best matching.
Now in this transformation, you can see there's a parameter k here.
And this k controls the upper bound of this function.
It's easy to see this function has a upper bound because if you look at the x divided by x plus k where k is not an active number, then the numerator will never be able to exceed the denominator, right?
So, it's upper bounded by k plus 1.
Now, this is also difference between this transformation function and the logarithm transformation.
Which it doesn't have upperbound.
Now furthermore, one interesting property of this function is that as we vary K, we can actually simulate different transformation functions, including the two extremes that are shown here.
That is a zero one bit transformation, and the unit transformation.
So for example, if we set k to zero, now you can see the function value would be one.
So we precisely, recover the zero one bit transformation.
If you set k to a very large number, on the other hand, other hand, it's going to look more like the linear transformation function.
So in this sense, this transformation is very flexible, it allows us to control the shape of the transformation.
It also has a nice property of the upper bound.
And this upper bound is useful to control the inference of a particular term.
And so that we can prevent a, a spammer from just increasing the count of that might match this term.
In other words this upper bound might also ensure that all terms will be counted when we aggregate the, the weights, to compute a score.
As I said, this transformation function has worked well, so far.
So to summarise this lecture, the main point is that we need to do some sub linearity of TF Transformation.
And this is needed to capture the intuition of diminishing return from high Term Counts.
It's also to avoid a dominance by one single term over all others.
This BM25 Transformation, Transformation that we talked about is very interesting.
It's so far one of the best performing TF Transforming formation formulas.
It has upper bound, and it's also robust and effective.
Now, if we're plug in this function into our TF-IDF weighting vector space model then we would end up having the following ranking function, which has a BM25 TF component.
Now this is already very close to a state of the art ranking function called a BM25.
And we will discuss how we can further improve this formula in the next lecture.
This lecture is a continued discussion of Discriminative Classifiers for Text Categorization.
So, in this lecture, we're going to introduce, yet another Discriminative Classifier called the Support Vector Machine or SVM.
Which is a very popular classification method and it has been also shown to be effective for text categorization.
So to introduce this classifier, let's also think about the simple case of two categories.
We have two topic categories, And we want to classify documents into these two categories and we're going to represent again a document by a feature factor x here.
Now, the idea of this classifier is to design also a linear separator here that you'll see and it's very similar to what you have seen not just for regression, right?
And we're going to do also say that if the sign of this function value is positive then we're going to say the objective is in category one.
Otherwise, we're going to say it's in category 2.
So that makes 0 that is the decision boundary between the few categories.
So, in generally hiding marginal space such as, 0. corresponds to a hyper plain.
Now I've shown you a simple case of two dimensional space it was just X1 and X2 and this case this corresponds to a line that you can see here.
So, this is a line defined by just three parameters here, beta zero, beta one, and beta two.
Now, this line is heading in this direction so it shows that as we increase X1, X2 will also increase.
So we know that beta one and beta two have different assigns, one is negative and the other is positive.
So let's just assume that beta one is negative and beta two Is positive.
Now, it's interesting to examine, then, the data instances on the two sides of the slide.
So, here, the data instance are visualized as circles for one class and diamonds for the other class.
Now, one question is to take a point like this one and to ask the question what's the value of this expression, or this classifier, for this data point?
So what do you think?
Basically, we're going to evaluate its value by using this function.
And as we said, if this value's positive we're going to say this is in category one, and if it's negative, it's going to be in category two.
Intuitively, this line separates these two categories, so we expect the points on one side would be positive and the points on the other side would be negative.
Our question is under the assumption that I just mentioned, let's examine a particular point like this one.
So what do you think is the sine of this expression?
Well, to examine the sine we can simply look at this expression here.
And we can compare this with let's say, value on the line, let's see, compare this with this point.
While they have identical X1, but then one has a higher value for X2.
Now, let's look at the sin of the coefficient for X2.
Well, we know this is a positive.
So, what that means is that the f value for this point should be higher than the f value for this point on the line that means this will be positive, right?
So we know in general of all points on this side, the function's value will be positive and you can also verify all the points on this side will be negative.
And so this is how this kind of linear classifier or linear separator can then separate the points in the two categories.
So, now the natural question is, which linear separator is the best?
Now, I've get you one line here that can separate the two classes.
And this line, of course, is determined by the vector beta, the coefficients.
Different coefficients will give us different lines.
So, we could imagine there are other lines that can do the same job.
Gamma, for example, could give us another line that counts a separator to these instances.
Of course, there are also lines that won't separate to them and those are bad lines.
But, the question is, when we have multiple lines that can separate both clauses, which align the best?
In fact, you can imagine, there are many different ways of choosing the line.
So, the logistical regression classifier that you have seen earlier actually uses some criteria to determine where this line should be and so linear separate as well.
And uses a conditional likelihood on the training that it determines which line is the best.
But in SVM we're going to look at another criteria for determining which line is the best.
And this time, the criteria is more tied to the classification arrow as you will see.
So, the basic idea is to choose the separator to maximize the margin.
So what is a margin?
So, I choose some dotted lines here to indicate the boundaries of those data points in each class.
And the margin is simply the distance between the line, the separator, and the closest point from each class.
So you can see the margin of this side is as I've shown here and you can also define the margin on the other side.
In order for the separator to maximize the margin, it has to be kind of in the middle of the two boundaries and you don't want this separator to be very close to one side, and that in intuition makes a lot of sense.
So this is basic idea of SVM.
We're going to choose a linear separator to maximize the margin.
Now on this slide, I've also changed the notation so that I'm not going to use beta to denote the parameters.
But instead, I'm going to use w although w was used to denote the words before so don't be confused here.
W here is actually a width, a certain width.
So I'm also using lowercase b to denote the beta 0, a biased constant.
And there are instances do represent that as x and I also use the vector form of multiplication here.
So we see a transpose of w vector multiply by the future vector.
So b is a bias constant and w is a set of weights with one way for each feature.
We have m features and so we have m weights and that will represent as a vector.
And similarly, the data instance here, the text object, is represented by also a feature vector of the same number of elements.
Xi is a feature value.
For example, word count and you can verify, when we.
Multiply these two vectors together, take the dot product, we get the same form of the linear separator as you have seen before.
It's just a different way of representing this.
Now I use this way so that it's more consistent with what notations people usually use when they talk about SVM.
This way you can better connect the slides with some other readings you might do.
Okay, so when we maximize the margins of a separator, it just means the boundary of the separator is only determined by a few data points, and these are the data points that we call support vectors.
So here illustrated are two support vectors for one class and two for the other class.
And these quotas define the margin basically, and you can imagine once we know which are supportive vectors then this center separator line will be determined by them.
So the other data points actually don't really matter that much.
And you can see if you change the other data points it won't really affect the margin, so the separator will stay the same.
Mainly affected by the the support vector machines.
Sorry, it's mainly affected by the support vectors and that's why it's called a support vector machine.
Okay, so now the next question is, of course, how can we set it up to optimize the line?
How can we actually find the line or the separator?
Now this is equivalent to finding values for w and b, because they will determine where exactly the separator is.
So in the simplest case, the linear SVM is just a simple optimization problem.
So again, let's recall that our classifier is such a linear separator, where we have weights for all the features, and the main goal is remove these weights w and b.
And the classifier will say X is in category theta 1 if it's positive.
Otherwise, it's going to say it's in the other category.
So this is our assumption, our setup.
So in the linear SVM, we are going to then seek these parameter values to optimize the margins and then the training error.
The training data would be basically like in other classifiers.
We have a set of training points where we know the x vector, and then we also know the corresponding label, y i.
And here we define y i as two values, but these values are not 0, 1 as you have seen before, but rather -1 and positive 1, and they're corresponding to these two categories, as I've shown here.
Now you might wonder why we don't define them as 0 and And this is purely for mathematical convenience, as you will see in a moment.
So the goal of optimization first is to make sure the labeling of training data is all correct.
So that just means if y i, the norm label for instance x i, is 1, we would like this classified value to be large.
And here we just choose a threshold of 1 here.
But if you use another threshold, you can easily fit that constant into the parameter values b and w to make the right-hand side just 1.
Now if, on the other hand, y i is -1, that means it's in a different class, then we want this classifier to give us a very small value, in fact a negative value, and we want this value to be less than or equal to -1.
Now these are the two different instances, different kinds of cases.
How can we combine them together?
Now this is where it's convenient when we have chosen y i as -1 for the other category, because it turns out that we can either combine the two into one constraint.
y i multiplied by the classifier value must be larger than or equal to 1.
And obviously when y i is just 1, you see this is the same as the constraint on the left-hand side.
But when y i is -1, you also see that this is equivalent to the other inequality.
So this one actually captures both constraints in a unified way, and that's a convenient way of capturing these constraints.
What's our second goal?
Well, that's to maximize margin, so we want to ensure that separator can do well on the training data.
But then, among all the cases where we can separate the data, we also would like to choose the separator that has the largest margin.
Now the margin can be assumed to be related to the magnitude of the weight.
And so w transform multiplied by w would give us basically the sum of squares of all those weights.
So to have a small value for this expression, it means all the w i's must be small.
So we've just assumed that we have a constraint for getting the data on the training set to be classified correctly.
Now we also have the objective that's tied into a maximization of margin, and this is simply to minimize w transpose multiplied by w, and we often denote this by phi of w. So now you can see this is basically a optimization problem.
We have some variables to optimize, and these are the weights and b and we have some constraints.
These are linear constraints and the objective function is a quadratic function of the weights.
So this a quadratic program with linear constraints, and there are standard algorithm that are variable for solving this problem.
And once we solve the problem we obtain the weights w and b.
And then this would give us a well-defined classifier.
So we can then use this classifier to classify any new text objects.
Now the previous formulation did not allow any error in the classification, but sometimes the data may not be linear to the separator.
That means that they may not look as nice as you have seen on the previous slide where a line can separate all of them.
And what would happen if we allowed some errors?
Well, the principle can stay.
We want to minimize the training error but try to also maximize the margin.
But in this case we have a soft margin, because the data points may not be completely separable.
So it turns out that we can easily modify SVM to accommodate this.
So what you see here is very similar to what you have seen before, but we have introduced the extra variable xi i.
And we in fact will have one for each data instance, and this is going to model the error that we allow for each instance.
But the optimization problem would be very similar.
So specifically, you will see we have added something to the optimization problem.
First we have added some error to the constraint so that now we allow a Allow the classifier to make some mistakes here.
So, this Xi i is allowed an error.
If we set Xi i to 0, then we go back to the original constraint.
We want every instance to be classified accurately.
But, if we allow this to be non-zero, then we allow some errors here.
In fact, if the length of the Xi i is very large, the error can be very, very large.
So naturally, we don't want this to happen.
So we want to then also minimize this Xi i.
So, because Xi i needs to be minimized in order to control the error.
And so, as a result, in the objective function, we also add more to the original one, which is only W, by basically ensuring that we not only minimize the weights, but also minimize the errors, as you see here.
Here we simply take a sum over all the instances.
Each one has a Xi i to model the error allowed for that instance.
And when we combine them together, we basically want to minimize the errors on all of them.
Now you see there's a parameter C here, and that's a constant to control the trade-off between minimizing the errors and maximizing the margin.
If C is set to zero, you can see, we go back to the original object function where we only maximize the margin.
We don't really optimize the training errors and then Xi i can be set to a very large value to make the constraints easy to satisfy.
That's not very good of course, so C should be set to a non-zero value, a positive value.
But when C is set to a very, very large value, we'll see the object of the function will be dominated mostly by the training errors and so the optimization of margin will then play a secondary role.
So if that happens, what would happen is then we will try to do our best to minimize the training errors, but then we're not going to take care of the margin and that affects the generalization factors of the classify for future data.
So it's also not good.
So in particular, this parameter C has to be actually set carefully.
And this is just like in the case of k-nearest neighbor where you need to optimize a number of neighbors.
Here you need to optimize the C. And this is, in general, also achievable by doing cross-validation.
Basically, you look at the empirical data and see what value C should be set to in order to optimize the performance.
Now with this modification, the problem is still quadratic programming with linear constraints so the optimizing algorithm can be actually applied to solve this different version of the program.
Again, once we have obtained the weights and the bias, then we can have classifier that's ready for classifying new objects.
So that's the basic idea of SVM.
So to summarize the text categorization methods, where we introduce the many methods, and some are generative models.
Some are discriminative methods.
And these tend to perform similarly when optimized.
So there's still no clear winner, although each one has its pros and cons.
And the performance might also vary on different data sets for different problems.
And one reason is also because the feature representation is very critical and these methods all require effective feature representation.
And to design an effective feature set, we need domain knowledge and humans definitely play an important role here, although there are new machine learning methods and algorithm representation learning that can help with learning features.
And another common thing is that they might be performing similarly on the data set, but with different mistakes.
And so, their performance might be similar, but then the mistakes they make might be different.
So that means it's useful to compare different methods for a particular problem and then maybe combine multiple methods because this can improve the robustness and they won't make the same mistakes.
So assemble approaches that would combine different methods tend to be more robust and can be useful in practice.
Most techniques that we introduce use the supervised machine learning, which is a very general method.
So that means that these methods can be actually applied to any text or categorization problem.
As long as we have humans to help annotate some training data sets and design features, then supervising machine learning and all these classifiers can be easily applied to those problems to solve the categorization problem to allow us to characterize content of text concisely with categories.
Or to predict the sum properties of real world variables that are associated with text data.
The computers, of course, here are trying to optimize the combinations of the features provided by human.
And as I said, there are many different ways of combining them and they also optimize different object or functions.
But in order to achieve good performance, they all require effective features and also plenty of training data.
So as a general rule, and if you can improve the feature representation, and then provide more training data, then you can generally do better.
Performance is often much more affected by the effectiveness of features than by the choice of specific classifiers.
So feature design tends to be more important than the choice of specific classifier.
So, how do we design effective features?
Well, unfortunately, this is very application-specific.
So there's no really much general thing to say here.
But we can do some analysis of the categorization problem and try to understand what kind of features might help us distinguish categories.
And in general, we can use a lot of domain knowledge to help us design features.
And another way to figure out the effective features is to do error analysis on the categorization results.
You could, for example, look at which category tends to be confused with which other categories.
And you can use a confusion matrix to examine the errors systematically across categories.
And then, you can look into specific instances to see why the mistake has been made and what features can prevent the mistake.
And this can allow you to obtain insights for design new features.
So error analysis is very important in general, and that's where you can get the insights about your specific problem.
And finally, we can leverage this on machine learning techniques.
So, for example, feature selection is a technique that we haven't really talked about, but is very important.
And it has to do with trying to select the most useful features before you actually train a full classifier.
Sometimes training a classifier will also help you identify which features have high values.
There are also other ways to ensure this sparsity.
Of the model, meaning to recognize the widths.
For example, the SVM actually tries to minimize the weights on features.
But you can further force some features, force to use only a small number of features.
There are also techniques for dimension reduction.
And that's to reduce a high dimensional feature space into a low dimensional space typically by clustering of features in various ways.
So metrics factorization has been used to do such a job, and this is some of the techniques are actually very similar to the talking models that we'll discuss.
So talking morals like psa or lda can actually help us reduce the dimension of features.
Like imagine the words our original feature.
But the can be matched to the topic space .Let's say we have k topics.
So a document can now be represented as a vector of just k values corresponding to the topics.
So we can let each topic define one dimension, so we have a k dimensional space instead of the original high dimensional space corresponding to words.
And this is often another way to learn effective features.
Especially, we could also use the categories to supervise the learning of such low dimensional structures.
And so, the original worth features can be also combined with such amazing dimension features or lower dimensional space features to provide a multi resolution which is often very useful.
Deep learning is a new technique that has been developed the machine learning.
It's particularly useful for learning representations.
So deep learning refers to deep neural network, it's another kind of classifier, where you can have intermediate features embedded in the models.
That it's highly non-linear transpire, and some recent events that's allowed us to train such a complex network effectively.
And the technique has been shown to be quite effective for speech recognition, computer reasoning, and recently has been applied to text as well.
It has shown some promise.
And one important advantage of this approach in relationship with the featured design, is that they can learn intermediate replantations or compound the features automatically.
And this is very valuable for learning effective replantation, for text recalibration.
Although in text domain, because words are exemplary representation of text content, because these are human's imaging for communication.
And they are generally sufficient for For representing content for many tasks.
If there's a need for some new representation, people would have invented a new word.
So because of this we think of value of deep learning for text processing tends to be lower than for [INAUDIBLE].
And the speech revenue where they are anchored corresponding where the design that worked as features.
But people only still very promising for learning effective features especially for complicated tasks.
Like a analysis it has been shown to be effective because it can provide that goes beyond that of words.
Now regarding the training examples.
It's generally hard to get a lot of training examples because it involves human labor.
But there are also some ways to help with this.
So one is to assume in some low quality training examples can also be used.
So, those can be called pseudo training examples.
For example, if you take reviews from the internet, they might have overall ratings.
So, to train a of categorizer, meaning we want to positive or negative.
And categorize these reviews into these two categories.
Then we could assume five star reviews are all positive training samples.
One star are negative.
But of course, sometimes even five star reviews will also mention negative opinions so the training sample is not all of that high quality, but they can still be useful.
Another idea is to exploit the unlabeled data and there are techniques called the semi-supervised machine learning techniques that can allow you to combine labeled data with unlabeled data.
So, in other case it's easy to see the next model can be used For both text plus read and the categorization.
So you can imagine, if you have a lot of unlabeled text data for categorization, then you can actually do clustering on these text data, learn categories.
And then try to somehow align these categories.
With the categories defined by the training data, where we already know which documents are in which category.
So you can in fact use the Algorithm to actually combine both.
That would allow you essentially also pick up useful words and label the data.
You can think of this in another way.
Basically, we can use let's say a to classify all of the unlabeled text documents, and then we're going to assume the high confidence Classification results are actually liable.
Then you suddenly have more training data because from the enabler that we now know some are labeled as category one, some are labeled as category two.
All though the label is not completely reliable But then they can still be useful.
So let's assume they are actually training label examples, and then we combine them with true training examples through improved categorization method.
And so this idea is very powerful.
When the enabled data and the training data are very different, and we might need to use other advanced machine learning techniques called domain adaptation or transfer learning.
This is when we can Borrow some training examples from a related problem that may be different.
Or, from a categorization password that follow very different distribution from what we are working on.
But basically, when the two domains are very different, then we need to be careful and not overfit the training domain.
But yet, we can still want to use some signals from the related training data.
So for example, training categorization on news might not give you Effective plus y for class vine topics and tweets.
But you can still learn something from news to help look at writing tweets.
So there are mission learning techniques that can help you do that effectively.
Here's a suggested reading where you can find more details about some more of the methods is that we have covered.
This lecture is about the methods for text categorization.
So in this lecture we're going to discuss how to do text for categorization.
First, there're many methods for text categorization.
In such a method the idea is to determine the category based on some rules that we design carefully to reflect the domain knowledge about the category prediction problem.
So for example, if you want to do topic categorization for news articles you can say well, if the news article mentions word like a game and sports three times.
That we're going to say it's about sports things like that and this would allow us to deterministically decide which category a document that should be put into.
Now such a strategy would work well if the following conditions hold.
First the categories must be very well defined and this allows the person to clearly decide the category based on some clear rules.
A certainly the categories as half to be easy to distinguished at the based on a surface features in text.
So that means some official features like keywords or punctuations or whatever, you can easily identify in text to data.
For example, if there is some special vocabulary that is known to only occur in a particular category.
And that would be most effective because we can easily use such a vocabulary or padding of such a vocabulary to recognize this category.
Now we also should have sufficient knowledge for designing these words, and so if that's the case then such a can be effective.
And so it does have a in some domains and sometimes.
However, in general, there are several problems with this approach.
First off, because it's label intensive it requires a lot of manual work.
Obviously, we can't do this for all kinds of categorization problems.
We have to do it from scratch for a different problem.
problem because given the rules, what they need.
So it doesn't scale up well.
Secondly, it cannot handle uncertainty in rules, often the rules Aren't 100% reliable.
Take for example looking at occurrences of words in texts and trying to decide the topic.
It's actually very hard to have 100% correct rule.
So for example you can say well, if it has game, sports, basketball Then for sure it's about sports.
But one can also imagine some types of articles that mention these cures, but may not be exactly about sports or only marginally touching sports.
The main topic could be another topic, a different topic than sports.
So that's one disadvantage of this approach.
And then finally, the rules maybe inconsistent and this would lead to robustness.
More specifically, and sometimes, the results of categorization may be different that depending on which rule to be applied.
So as in that case that you are facing uncertainty.
And you will also have to decide an order of applying the rules, or combination of results that are contradictory.
So all these are problems with this approach.
And it turns out that both problems can be solved or alleviated by using machine learning.
So these machine learning methods are more automatic.
But, I still put automatic in quotation marks because they are not really completely automatic cause it still require many work.
More specifically we have to use a human experts to help in two ways.
First the human experts must annotate data cells was category labels.
And would tell the computer which documents should receive which categories.
And this is called training data.
And then secondly, the human experts also need to provide a set of features to represent each text object.
That can potentially provide a clue about the category.
So, we need to provide some basic features for the computers to look into.
In the case of tax a natural choice would be the words.
So, using each has a feature is a very common choice to start with, but of course there are other sophisticated features like phrases or even parts of ancients tags or even syntax to the structures.
So once human experts can provide this then we can use machine running to learn soft rules for categorization from the training data.
So, soft rules just means, we're going to get decided which category we should be assigned for a document, but it's not going to be use using a rule that is deterministic.
So we might use something similar to saying that if it matches games, sports many times, it's likely to be sports.
But, we're not going to say exactly for sure but instead, we're going to use probabilities or weights.
So that we can combine much more evidences.
So, the learning process, basically is going to figure out which features are most useful for separating different categories.
And it's going to also figure out how to optimally combine features to minimize errors of the categorization of the training data.
So the training data, as you can see here, is very important.
It's the basis for learning.
And then, the trained classifier can be applied to a new text object to predict the most likely category.
And that's to simulate the prediction of what human Would assign to this text object.
If the human were to make a judgement.
So when we use machine learning for text categorization we can also talk about the problem in the general setting of supervisement.
So the set up is to learn a classifier to map a value of X.
Into a map of Y so here X is all the text objects and Y is all the categories, a set of categories.
So the class phi will take any value in x as input and would generate a value in y as output.
We hope that output y with this right category for x.
And here correct, of course, is judged based on the training data.
So that's a general goal in machine learning problems or supervised learning problems where you are given some examples of input and output for a function.
And then the computer's going to figure out the, how the function behaves like based on this examples.
And then try to be able to compute the values for future x's that when we have not seen.
So in general all methods would rely on discriminative features of text objects to distinguish different categories.
So that's why these features are very important and they have to be provided by humans.
And they will also combine multiple features in a weight map with weights to be optimized to minimize errors on the training data.
So after the learning processes optimization problem.
An objective function is often tied into the errors on the training data.
Different methods tend to vary in their ways of measuring the errors on the training data.
They might optimize a different objective function, which is often also called a loss function or cost function.
They also tend to vary in their ways of combining the features.
So a linear combination for example is simple, is often used.
But they are not as powerful as nonlinear combinations.
But nonlinear models might be more complex for training, so there are tradeoffs as well.
But that would lead to different variations of many variations of these learning methods.
So in general we can distinguish two kinds of classifiers at a high level.
One is called generative classifiers.
The other is called discriminative classifiers.
The generative classifiers try to learn what the data looks like in each category.
So it attempts to model the joint distribution of the data and the label x and y and this can then be factored out to a product of why the distribution of labels.
And the joint probability of sorry the conditional probability of X given Y, so it's Y.
So we first model the distribution of labels and then we model how the data is generate a particular label here.
And once we can estimate these models, then we can compute this conditional probability of label given data based on the probability of data given label.
And the label distribution here by using the Bayes Rule.
Now this is the most important thing, because this conditional probability of the label can then be used directly to decide which label is most likely.
So in such approaches objective function is actually likelihood.
And so, we model how the data are generated.
So it only indirectly captures the training errors.
But if we can model the data in each category accurately, then we can also classify accurately.
One example is Nave Bayes classifier, in this case.
The other kind of approaches are called discriminative classifies, and these classifies try to learn what features separate categories.
So they direct or attack the problem of categorization for separation of classes.
So sorry for the problem.
So, these discriminative classifiers attempt to model the conditional probability of the label given the data point directly.
So, the objective function tends to directly measure the errors of categorization on the training data.
Some examples include a logistical regression, support vector machines, and k-nearest neighbors.
We will cover some of these classifiers in detail in the next few lectures.
This lecture is about evaluation of text clustering.
So far we have talked about multiple ways of doing text clustering but how do we know which method works the best?
So this has to do with evaluation.
Now to talk about evaluation one must go back to the clustering bias that we introduced at the beginning.
Because two objects can be similar depending on how you look at them, we must clearly specify the perspective of similarity.
Without that, the problem of clustering is not well defined.
So this perspective is also very important for evaluation.
If you look at this slide, and you can see we have two different ways to cluster these shapes, and if you ask a question, which one is the best, or which one is better?
You actually see, there's no way to answer this question without knowing whether we'd like to cluster based on shapes, or cluster based on sizes.
And that's precisely why the perspective on clustering bias is crucial for evaluation.
In general, we can evaluate text clusters in two ways, one is direct evaluation, and the other indirect evaluation.
So in direct evaluation, we want to answer the following questions, how close are the system-generated clusters to the ideal clusters that are generated by humans?
So the closeness here can be assessed from multiple perspectives and that will help us characterize the quality of cluster result in multiple angles, and this is sometimes desirable.
Now we also want to quantify the closeness because this would allow us to easily compare different measures based on their performance figures.
And finally, you can see, in this case, we essentially inject the clustering bias by using humans, basically humans would bring in the the need or desire to clustering bias.
Now, how do we do that exactly?
Well, the general procedure would look like this.
Given a test set which consists of a lot of text objects, we can have humans to create the ideal clustering result, that is, we're going to ask humans to partition the objects to create the gold standard.
And they will use their judgments based on the need of a particular application to generate what they think are the best clustering results, and this would be then used to compare with the system generated clusters from the same test set.
And ideally, we want the system results to be the same as the human generated results, but in general, they are not going to be the same.
So we would like to then quantify the similarity between the system-generated clusters and the gold standard clusters.
And this similarity can also be measure from multiple perspectives and this will give us various meshes to quantitatively evaluate a cluster, a clustering result.
And some of the commonly used measures include the purity, which measures whether a cluster has a similar object from the same cluster, in the gold standard.
And normalized mutual information is a commonly used measure which basically measures based on the identity of cluster of object in the system generally.
How well can you predict the cluster of the object in the gold standard or vice versa?
And mutual information captures, the correlation between these cluster labels and normalized mutual information is often used for quantifying the similarity for this evaluation purpose, F measure is another possible measure.
Now again a thorough discussion of this evaluation and these evaluation issues would be beyond the scope of this course.
I've suggested some reading in the end that you can take a look at to know more about that.
So here I just want to discuss some high level ideas that would allow you to think about how to do evaluation in your applications.
The second way to evaluate text clusters is to do indirect evaluation.
So in this case the question to answer is, how useful are the clustering results for the intended applications?
Now this of course is application specific question, so usefulness is going to depend on specific applications.
In this case, the clustering bias is imposed by the independent application as well, so what counts as a best cluster result would be dependent on the application.
Now procedure wise we also would create a test set with text objects for the intended application to quantify the performance of the system.
In this case, what we care about is the contribution of clustering to some application so we often have a baseline system to compare with.
This could be the current system for doing something, and then you hope to add a clustering to improve it, or the baseline system could be using a different clustering method.
And then what you are trying to experiment with, and you hope to have better idea of word clustering.
So in any case you have a baseline system work with, and then you add a clustering algorithm to the baseline system to produce a clustering system.
And then we have to compare the performance of your clustering system and the baseline system in terms of the performance measure for that particular application.
So in this case we call it indirect evaluation of clusters because there's no explicit assessment of the quality of clusters, but rather it's to assess the contribution of clusters to a particular application.
So, to summarize text clustering, it's a very useful unsupervised general text mining technique, and it's particularly useful for obtaining an overall picture of the text content.
And this is often needed to explore text data, and this is often the first step when you deal with a lot of text data.
The second application or second kind of applications is through discover interesting clustering structures in text data and these structures can be very meaningful.
There are many approaches that can be used to form text clustering and we discussed model based approaches and some narrative based approaches.
In general, strong clusters tend to show up no matter what method is used.
Also the effectiveness of a method highly depends on whether the desired clustering bias is captured appropriately, and this can be done either through using the right generating model, the model design appropriate for the clustering, or the right similarity function expressly define the bias.
Deciding the optimal number of customers is a very difficult problem for order cluster methods, and that's because it's unsupervised algorithm, and there's no training there how to guide us to select the best number of clusters.
Now sometimes you may see some methods that can automatically determine the number of clusters, but in general that has some implied application of clustering bias there and that's just not specified.
Without clearly defining a clustering bias, it's just impossible to say the optimal number of cluster is what, so this important to keep in mind.
And I should also say sometimes we can also use application to determine the number of clusters, for example, if you're clustering search results, then obviously you don't want to generate the 100 clusters, so the number can be dictated by the interface design.
In other situations, we might be able to use the fitness to data to assess whether we've got a good number of clusters to explain our data well.
And to do that, you can vary the number of clusters and watch how well you can fit the data.
In general when you add a more components to a mixed model you should fit the data better because you, you don't, you can always set the probability of using the new component as zero.
So you can't in general fit the data worse than before, but the question is as you add more components would you be able to significantly improve the fitness of the data and that can be used to determine the right number of clusters.
And finally evaluation of clustering results, this kind can be done both directly and indirectly, and we often would like to do both in order to get a good sense about how well our method works.
So here's some suggested reading and this is particularly useful to better understand how the matches are calculated and clustering in general
This lecture is about the Feedback in Text Retrieval.
So, in this lecture, we're going to continue the discussion on text retrieval methods.
In particular, we're going to talk about Feedback in Text Retrieval.
This is a diagram that shows the retrieval process.
We can see the user would typed in a query and then the query would be sent to a Retrieval Engine or search engine and the engine would return results.
These results would be shown to the user.
After the user has seen these results, the user can actually make judgments.
So for example, the user has say, well, this is good and this document is not very useful.
This is good again, et cetera.
Now this is called a relevance judgment or Relevance Feedback, because we've got some feedback information from the user based on the judgments.
This can be very useful to the system.
Learn what exactly is interesting to the user.
So the feedback module would then take this as input and also use the document collection to try to improve ranking.
Typically, it would involve updating the query.
So the system can now rank the results more accurately for the user.
So this is called Relevance Feedback.
The feedback is based on relevance judgements made by the users.
Now these judgements are reliable, but the users generally don't want to make extra effort, unless they have to.
So the downside's that involves some extra effort by the user.
There is another form of feedback called a Pseudo Relevance Feedback, or a blind feedback also called an automatic feedback.
In this case, you can see once the user has got without an effect, we don't have to involve users.
So you can see there's no user involved here.
And we simply assume that the top ranked documents to be relevant.
Let's say, we have assumed the top ten is relevant.
And then we will then use these assumed documents to learn and to improve the query.
Now you might wonder, you know, how could this help if we simply assume the top rank documents would be random.
Well you can imagine these top rank documents are actually similar to relevant documents, even if they are not relevant, they look like relevant documents.
So, it's possible to learn some related terms to the query from this set.
In fact, you may recall that we talked about using language model to analyze word association to learn related words to the word computer.
Right?
And then what we did is first, use computer to retrieve all the documents that contain computer.
So, imagine now the query here is a computer.
Right?
And then the results will be those documents that contain computer.
And what we can do then is to take the top end results.
They can match computer very well and we're going to count the terms in this set and then we're going to then use the background language model to choose the terms that are frequent the in this set, but not frequent the in the whole collection.
So, if we make a contrast between these two, what we can find is that we'll learn some related terms too, the work computer as what I've seen before.
And these related words can then be added to the original query to expand the query.
And this would help us free documents that don't necessarily match computer, but match other words like program and software.
So this is factored for improving the search doubt.
But of course, pseudo relevancy feedback is completely unreliable.
We have to arbitrarily set a cutoff.
So there is also something in between called Implicit Feedback.
In this case, what we do, we do involve users, but we don't have to ask users to make judgements.
Instead, we are going to observe how the user interacts with the search results.
So, in this case, we're going to look at the clickthroughs.
So the user clicked on this one and the, the user viewed this one.
And the user skipped this one and the user viewed this one again.
Now this also is a clue about whether a document is useful to the user and we can even assume that we're going to use only the snippet here in this document.
The text that's actually seen by the user, instead of the actual document of this entry in the link.
There that same web search may be broken, but that, it doesn't matter.
If the user tries to fetch this document that because of the displayed text, we can assume this displayed text is probably relevant is interesting to user, so we can learn from such information.
And this is called Implicit Feedback and we can again, use the information to update the query.
This is a very important technique used in modern search engines.
You know, think about Google and Bing and they can collect a lot of user activities.
Why they are serverless?
Right.
So they would observe what documents we click on, what documents we skip.
And this information is very valuable and they can use this to encode the search engine.
So to summarize, we would talk about the three kinds of feedback here rather than feedback.
Where the use exquisite judgement, it takes some used effort, but the judgement that information is reliable.
We talked about the Pseudo Feedback, where we simply assumed top random documents.
We get random, we don't have to involve the user.
Therefore, we could do that actually before we, we return the results to the user.
And the third is Implicit Feedback, where we use clickthroughs.
Where we don't, we involve users, but the user doesn't have to make explicit effort to make judgement.
This lecture is about the specific smoothing methods for language models used in Probabilistic Retrieval Model.
In this lecture we will continue the discussion of language models for information retrieval, particularly the query likelihood retrieval method.
And we're going to talk about the specific smoothing methods used for such a retrieval function.
So, this is a slide from a previous lecture where we show that with query likelihood ranking and the smoothing with the collection language model.
We end up having a retrieval function that looks like the following.
So, this is the retrieval function, based on these assumptions that we have discussed.
You can see it's a sum of all the matched query terms here.
And inside the sum it's a count of term in the query, and some weight for the term in the document.
We have TFI, TF weight here.
And then we have another constant here, in n. So clearly, if we want to implement this function using a programming language, we'll still need to figure out a few variables.
In particular, we're going to need to know how to estimate the, probability of would exactly.
And how do we set alpha?
So in order to answer these questions, we have to think about this very specific smoothing methods, and that is the main topic of this lecture.
We're going to talk about two smoothing methods.
The first is the simple linear interpolation, with a fixed coefficient.
And this is also called a Jelinek and Mercer smoothing.
So the idea is actually very simple.
This picture shows how we estimate document language model by using maximum [INAUDIBLE] method, that gives us word counts normalized by the total number of words in the text.
The idea of using this method is to maximize the probability of the observed text.
As a result, if a word like network, is not observed in the text.
It's going to get zero probability, as shown here.
So the idea of smoothing, then, is to rely on collection average model, where this word is not going to have a zero probability to help us decide what non-zero probability should be assigned to such a word.
So, we can know that network as a non-zero probability here.
So, in this approach what we do is, we do a linear interpolation between the maximum likelihood or estimate here and the collection language model.
And this controlled by the smoothing parameter, lambda.
Which is between 0 and 1.
So this is a smoothing parameter.
The larger lambda is the two the more smoothing we have, we will have.
So by mixing them together, we achieve the goal of assigning non-zero probability.
And these two are word in our network.
So let's see how it works for some of the words here.
For example if we compute to the smallest probability for text.
Now, the next one right here is made give us 10 over 100, and that's going to be here.
But the connection probability is this, so we just combine them together with this simple formula.
We can also see a, the word network.
Which used to have zero probability now is getting a non-zero probability of this value.
And that's because the count is going to be zero for network here, but this part is non zero and that's basically how this method works.
If you think about this and you can easily see now the alpha sub d in this smoothing method is basically lambda because that's, remember, the coefficient in front of the probability of the word given by the collection language model here, right?
Okay, so this is the first smoothing method.
The second one is similar, but it has a find end for manual interpretation.
It's often called a duration of the ply or Bayesian smoothing.
So again here, we face the problem of zero probability for like network.
Again we'll use the collection language model, but in this case we're going to combine them in a somewhat different ways.
The formula first can be seen as a interpolation of the maximum and the collection language model as before.
As in the J M's [INAUDIBLE].
Only and after the coefficient [INAUDIBLE] is not the lambda, a fixed lambda, but a dynamic coefficient in this form, when mu is a parameter, it's a non, negative value.
And you can see if we set mu to a constant, the effect is that a long document would actually get smaller coefficient here.
Right?
Because a long document we have a longer length.
Therefore, the coefficient is actually smaller.
And so a long document would have less smoothing as we would expect.
So this seems to make more sense than a fixed coefficient smoothing.
Of course, this part would be of this form, so that the two coefficients would sum to 1.
Now, this is one way to understand that this is smoothing.
Basically, it means that it's a dynamic coefficient interpolation.
There is another way to understand this formula.
Which is even easier to remember and that's this side.
So it's easy to see we can rewrite this modern method in this form.
Now, in this form, we can easily see what change we have made to the maximum estimator, which would be this part, right?
So it normalizes the count by the top elements.
So, in this form, we can see what we did, is we add this to the count of every word.
So, what does this mean?
Well, this is basically something relative to the probability of the word in the collection.. And we multiply that by the parameter mu.
And when we combine this with the count here, essentially we are adding pseudo counts to the observed text.
We pretend every word, has got this many pseudocount.
So the total count would be the sum of these pseudocount and the actual count of the word in the document.
As a result, in total, we would have added this minute pseudocount.
Why?
Because if you take a sum of this, this one, move over all the words and we'll see the probability of the words would sum to 1, and that gives us just mu.
So this is the total number of pseudo counters that we added.
And, and so these probabilities would still sum to 1.
So in this case, we can easily see the method is essentially to add these as a pseudocount to this data.
Pretend we actually augment the data by including by some pseudo data defined by the collection language model.
As a result, we have more counts.
It's the, the total counts for, for word, a word that would be like this.
And, as a result, even if a word has zero counts here.
And say if we have zero come here and that it would still have none, zero count because of this part, right?
And so this is how this method works.
Let's also take a look at this specific example here.
All right, so for text again, we will have 10 as original count.
That we actually observe but we also added some pseudocount.
And so, the probability of text would be of this form.
Naturally the probability of network would be just this part.
And so, here you can also see what's alpha sub d here.
Can you see it?
If you want to think about you can pause the video.
Have you noticed that this part is basically of a sub t?
So we can see this case of our sub t does depend on the document, right?
Because this lens depends on the document whereas in the linear interpolation.
The James move method this is the constant.
This lecture is about, Opinion Mining and Sentiment Analysis, covering, Motivation.
In this lecture, we're going to start, talking about, mining a different kind of knowledge.
Namely, knowledge about the observer or humans that have generated the text data.
In particular, we're going to talk about the opinion mining and sentiment analysis.
As we discussed earlier, text data can be regarded as data generated from humans as subjective sensors.
In contrast, we have other devices such as video recorder that can report what's happening in the real world objective to generate the viewer data for example.
Now the main difference between test data and other data, like video data, is that it has rich opinions, and the content tends to be subjective because it's generated from humans.
Now, this is actually a unique advantaged of text data, as compared with other data, because the office is a great opportunity to understand the observers.
We can mine text data to understand their opinions.
Understand people's preferences, how people think about something.
So this lecture and the following lectures will be mainly about how we can mine and analyze opinions buried in a lot of text data.
So let's start with the concept of opinion.
It's not that easy to formally define opinion, but mostly we would define opinion as a subjective statement describing what a person believes or thinks about something.
Now, I highlighted quite a few words here.
And that's because it's worth thinking a little bit more about these words.
And that will help us better understand what's in an opinion.
And this further helps us to define opinion more formally.
Which is always needed to computation to resolve the problem of opinion mining.
So let's first look at the key word of subjective here.
This is in contrast with objective statement or factual statement.
Those statements can be proved right or wrong.
And this is a key differentiating factor from opinions which tends to be not easy to prove wrong or right, because it reflects what the person thinks about something.
So in contrast, objective statement can usually be proved wrong or correct.
For example, you might say this computer has a screen and a battery.
Now that's something you can check.
It's either having a battery or not.
But in contrast with this, think about the sentence such as, this laptop has the best battery or this laptop has a nice screen.
Now these statements are more subjective and it's very hard to prove whether it's wrong or correct.
So opinion, is a subjective statement.
And next lets look at the keyword person here.
And that indicates that is an opinion holder.
Because when we talk about opinion, it's about an opinion held by someone.
And then we notice that there is something here.
So that is the target of the opinion.
The opinion is expressed on this something.
And now, of course, believes or thinks implies that an opinion will depend on the culture or background and the context in general.
Because a person might think different in a different context.
People from different background may also think in different ways.
So this analysis shows that there are multiple elements that we need to include in order to characterize opinion.
So, what's a basic opinion representation like?
Well, it should include at least three elements, right?
Firstly, it has to specify what's the opinion holder.
So whose opinion is this?
Second, it must also specify the target, what's this opinion about?
And third, of course, we want opinion content.
And so what exactly is opinion?
If you can identify these, we get a basic understanding of opinion and can already be useful sometimes.
You want to understand further, we want enriched opinion representation.
And that means we also want to understand that, for example, the context of the opinion and what situation was the opinion expressed.
For example, what time was it expressed?
We, also, would like to, people understand the opinion sentiment, and this is to understand that what the opinion tells us about the opinion holder's feeling.
For example, is this opinion positive, or negative?
Or perhaps the opinion holder was happy or was sad, and so such understanding obvious to those beyond just Extracting the opinion content, it needs some analysis.
So let's take a simple example of a product review.
In this case, this actually expressed the opinion holder, and expressed the target.
So its obviously whats opinion holder and that's just reviewer and its also often very clear whats the opinion target and that's the product review for example iPhone 6.
When the review is posted usually you can't such information easier.
Now the content, of course, is a review text that's, in general, also easy to obtain.
So you can see product reviews are fairly easy to analyze in terms of obtaining a basic opinion of representation.
But of course, if you want to get more information, you might know the Context, for example.
The review was written in 2015.
Or, we want to know that the sentiment of this review is positive.
So, this additional understanding of course adds value to mining the opinions.
Now, you can see in this case the task is relatively easy and that's because the opinion holder and the opinion target have already been identified.
Now let's take a look at the sentence in the news.
In this case, we have a implicit holder and a implicit target.
And the tasker is in general harder.
So, we can identify opinion holder here, and that's the governor of Connecticut.
We can also identify the target.
So one target is Hurricane Sandy, but there is also another target mentioned which is hurricane of 1938.
So what's the opinion?
Well, there's a negative sentiment here that's indicated by words like bad and worst.
And we can also, then, identify context, New England in this case.
Now, unlike in the playoff review, all these elements must be extracted by using natural RAM processing techniques.
So, the task Is much harder.
And we need a deeper natural language processing.
And these examples also suggest that a lot of work can be easy to done for product reviews.
That's indeed what has happened.
Analyzing and assembling news is still quite difficult, it's more difficult than the analysis of opinions in product reviews.
Now there are also some other interesting variations.
In fact, here we're going to examine the variations of opinions, more systematically.
First, let's think about the opinion holder.
The holder could be an individual or it could be group of people.
Sometimes, the opinion was from a committee.
Or from a whole country of people.
Opinion target accounts will vary a lot.
It can be about one entity, a particular person, a particular product, a particular policy, ect.
But it could be about a group of products.
Could be about the products from a company in general.
Could also be very specific about one attribute, though.
An attribute of the entity.
For example, it's just about the battery of iPhone.
It could be someone else's opinion.
And one person might comment on another person's Opinion, etc.
So, you can see there is a lot of variation here that will cause the problem to vary a lot.
Now, opinion content, of course, can also vary a lot on the surface, you can identify one-sentence opinion or one-phrase opinion.
But you can also have longer text to express an opinion, like the whole article.
And furthermore we identify the variation in the sentiment or emotion damage that's above the feeding of the opinion holder.
So, we can distinguish a positive versus negative or mutual or happy versus sad, separate.
Finally, the opinion context can also vary.
We can have a simple context, like different time or different locations.
But there could be also complex contexts, such as some background of topic being discussed.
So when opinion is expressed in particular discourse context, it has to be interpreted in different ways than when it's expressed in another context.
So the context can be very [INAUDIBLE] to entire discourse context of the opinion.
From computational perspective, we're mostly interested in what opinions can be extracted from text data.
So, it turns out that we can also differentiate, distinguish, different kinds of opinions in text data from computation perspective.
First, the observer might make a comment about opinion targeting, observe the word So in case we have the author's opinion.
For example, I don't like this phone at all.
And that's an opinion of this author.
In contrast, the text might also report opinions about others.
So the person could also Make observation about another person's opinion and reported this opinion.
So for example, I believe he loves the painting.
And that opinion is really about the It is really expressed by another person here.
So, it doesn't mean this author loves that painting.
So clearly, the two kinds of opinions need to be analyzed in different ways, and sometimes in product reviews, you can see, although mostly the opinions are false from this reviewer.
Sometimes, a reviewer might mention opinions of his friend or her friend.
Another complication is that there may be indirect opinions or inferred opinions that can be obtained.
By making inferences on what's expressed in the text that might not necessarily look like opinion.
For example, one statement that might be, this phone ran out of battery in just one hour.
Now, this is in a way a factual statement because It's either true or false, right?
You can even verify that, but from this statement, one can also infer some negative opinions about the quality of the battery of this phone, or the feeling of the opinion holder about the battery.
The opinion holder clearly wished that the battery do last longer.
So these are interesting variations that we need to pay attention to when we extract opinions.
Also, for this reason about indirect opinions, it's often also very useful to extract whatever the person has said about the product, and sometimes factual sentences like these are also very useful.
So, from a practical viewpoint, sometimes we don't necessarily extract the subject of sentences.
Instead, again, all the sentences that are about the opinions are useful for understanding the person or understanding the product that we commend.
So the task of opinion mining can be defined as taking textualized input to generate a set of opinion representations.
Each representation we should identify opinion holder, target, content, and the context.
Ideally we can also infer opinion sentiment from the comment and the context to better understand.
The opinion.
Now often, some elements of the representation are already known.
I just gave a good example in the case of product we'd use where the opinion holder and the opinion target are often expressly identified.
And that's not why this turns out to be one of the simplest opinion mining tasks.
Now, it's interesting to think about the other tasks that might be also simple.
Because those are the cases where you can easily build applications by using opinion mining techniques.
So now that we have talked about what is opinion mining, we have defined the task.
Let's also just talk a little bit about why opinion mining is very important and why it's very useful.
So here, I identify three major reasons, three broad reasons.
The first is it can help decision support.
It can help us optimize our decisions.
We often look at other people's opinions, look at read the reviews in order to make a decisions like buying a product or using a service.
We also would be interested in others opinions when we decide whom to vote for example.
And policy makers, may also want to know people's opinions when designing a new policy.
So that's one general, kind of, applications.
And it's very broad, of course.
The second application is to understand people, and this is also very important.
For example, it could help understand people's preferences.
And this could help us better serve people.
For example, we optimize a product search engine or optimize a recommender system if we know what people are interested in, what people think about product.
It can also help with advertising, of course, and we can have targeted advertising if we know what kind of people tend to like what kind of plot.
Now the third kind of application can be called voluntary survey.
Now this is most important research that used to be done by doing surveys, doing manual surveys.
Question, answer it.
People need to feel informs to answer their questions.
Now this is directly related to humans as sensors, and we can usually aggregate opinions from a lot of humans through kind of assess the general opinion.
Now this would be very useful for business intelligence where manufacturers want to know where their products have advantages over others.
What are the winning features of their products, winning features of competitive products.
Market research has to do with understanding consumers oppinions.
And this create very useful directive for that.
Data-driven social science research can benefit from this because they can do text mining to understand the people's opinions.
And if you can aggregate a lot of opinions from social media, from a lot of, popular information then you can actually do some study of some questions.
For example, we can study the behavior of people on social media on social networks.
And these can be regarded as voluntary survey done by those people.
In general, we can gain a lot of advantage in any prediction task because we can leverage the text data as extra data above any problem.
And so we can use text based prediction techniques to help you make predictions or improve the accuracy of prediction.
This lecture is about the ordinal logistic regression for sentiment analysis.
So, this is our problem set up for a typical sentiment classification problem.
Or more specifically a rating prediction.
We have an opinionated text document d as input, and we want to generate as output, a rating in the range of 1 through k so it's a discrete rating, and this is a categorization problem.
We have k categories here.
Now we could use a regular text for categorization technique to solve this problem.
But such a solution would not consider the order and dependency of the categories.
Intuitively, the features that can distinguish category 2 from 1, or rather rating 2 from 1, may be similar to those that can distinguish k from k-1.
For example, positive words generally suggest a higher rating.
When we train categorization problem by treating these categories as independent we would not capture this.
So what's the solution?
Well in general we can order to classify and there are many different approaches.
And here we're going to talk about one of them that called ordinal logistic regression.
Now, let's first think about how we use logistical regression for a binary sentiment.
A categorization problem.
So suppose we just wanted to distinguish a positive from a negative and that is just a two category categorization problem.
So the predictors are represented as X and these are the features.
And there are M features all together.
The feature value is a real number.
And this can be representation of a text document.
And why it has two values, binary response variable 0 or 1.
And then of course this is a standard two category categorization problem.
We can apply logistical regression.
You may recall that in logistical regression, we assume the log of probability that the Y is equal to one, is assumed to be a linear function of these features, as shown here.
So this would allow us to also write the probability of Y equals one, given X in this equation that you are seeing on the bottom.
So that's a logistical function and you can see it relates this probability to, probability that y=1 to the feature values.
And of course beta i's are parameters here, so this is just a direct application of logistical regression for binary categorization.
What if we have multiple categories, multiple levels?
Well we have to use such a binary logistical regression problem to solve this multi level rating prediction.
And the idea is we can introduce multiple binary class files.
In each case we asked the class file to predict the, whether the rating is j or above, or the rating's lower than j.
So when Yj is equal to 1, it means rating is j or above.
When it's 0, that means the rating is Lower than j.
So basically if we want to predict a rating in the range of 1-k, we first have one classifier to distinguish a k versus others.
And that's our classifier one.
And then we're going to have another classifier to distinguish it.
At k-1 from the rest.
That's Classifier 2.
And in the end, we need a Classifier to distinguish between 2 and 1.
So altogether we'll have k-1 classifiers.
Now if we do that of course then we can also solve this problem and the logistical regression program will be also very straight forward as you have just seen on the previous slide.
Only that here we have more parameters.
Because for each classifier, we need a different set of parameters.
So now the logistical regression classifies index by J, which corresponds to a rating level.
And I have also used of J to replace beta 0.
And this is to.
Make the notation more consistent, than was what we can show in the ordinal logistical regression.
So here we now have basically k minus one regular logistic regression classifiers.
Each has it's own set of parameters.
So now with this approach, we can now do ratings as follows.
After we have trained these k-1 logistic regression classifiers, separately of course, then we can take a new instance and then invoke a classifier sequentially to make the decision.
So first let look at the classifier that corresponds to level of rating K. So this classifier will tell us whether this object should have a rating of K or about.
If probability according to this logistical regression classifier is larger than point five, we're going to say yes.
The rating is K. Now, what if it's not as large as twenty-five?
Well, that means the rating's below K, right?
So now, we need to invoke the next classifier, which tells us whether it's above K minus one.
It's at least K minus one.
And if the probability is larger than twenty-five, then we'll say, well, then it's k-1.
What if it says no?
Well, that means the rating would be even below k-1.
And so we're going to just keep invoking these classifiers.
And here we hit the end when we need to decide whether it's two or one.
So this would help us solve the problem.
Right?
So we can have a classifier that would actually give us a prediction of a rating in the range of 1 through k. Now unfortunately such a strategy is not an optimal way of solving this problem.
And specifically there are two problems with this approach.
So these equations are the same as.
You have seen before.
Now the first problem is that there are just too many parameters.
There are many parameters.
Now, can you count how many parameters do we have exactly here?
Now this may be a interesting exercise.
To do.
So you might want to just pause the video and try to figure out the solution.
How many parameters do I have for each classifier?
And how many classifiers do we have?
Well you can see the, and so it is that for each classifier we have n plus one parameters, and we have k minus one classifiers all together, so the total number of parameters is k minus one multiplied by n plus one.
That's a lot.
A lot of parameters, so when the classifier has a lot of parameters, we would in general need a lot of data out to actually help us, training data, to help us decide the optimal parameters of such a complex model.
So that's not ideal.
Now the second problems is that these problems, these k minus 1 plus fives, are not really independent.
These problems are actually dependent.
In general, words that are positive would make the rating higher for any of these classifiers.
For all these classifiers.
So we should be able to take advantage of this fact.
Now the idea of ordinal logistical regression is precisely that.
The key idea is just the improvement over the k-1 independent logistical regression classifiers.
And that idea is to tie these beta parameters.
And that means we are going to assume the beta parameters.
These are the parameters that indicated the inference of those weights.
And we're going to assume these beta values are the same for all the K- 1 parameters.
And this just encodes our intuition that, positive words in general would make a higher rating more likely.
So this is intuitively assumptions, so reasonable for our problem setup.
And we have this order in these categories.
Now in fact, this would allow us to have two positive benefits.
One is it's going to reduce the number of families significantly.
And the other is to allow us to share the training data.
Because all these parameters are similar to be equal.
So these training data, for different classifiers can then be shared to help us set the optimal value for beta.
So we have more data to help us choose a good beta value.
So what's the consequence, well the formula would look very similar to what you have seen before only that, now the beta parameter has just one index that corresponds to the feature.
It no longer has the other index that corresponds to the level of rating.
So that means we tie them together.
And there's only one set of better values for all the classifiers.
However, each classifier still has the distinct R for value.
The R for parameter.
Except it's different.
And this is of course needed to predict the different levels of ratings.
So R for sub j is different it depends on j, different than j, has a different R value.
But the rest of the parameters, the beta i's are the same.
So now you can also ask the question, how many parameters do we have now?
Again, that's an interesting question to think about.
So if you think about it for a moment, and you will see now, the param, we have far fewer parameters.
Specifically we have M plus K minus one.
Because we have M, beta values, and plus K minus one of our values.
So let's just look basically, that's basically the main idea of ordinal logistical regression.
So, now, let's see how we can use such a method to actually assign ratings.
It turns out that with this, this idea of tying all the parameters, the beta values.
We also end up by having a similar way to make decisions.
And more specifically now, the criteria whether the predictor probabilities are at least 0.5 above, and now is equivalent to whether the score of the object is larger than or equal to negative authors of j, as shown here.
Now, the scoring function is just taking the linear combination of all the features with the divided beta values.
So, this means now we can simply make a decision of rating, by looking at the value of this scoring function, and see which bracket it falls into.
Now you can see the general decision rule is thus, when the score is in the particular range of all of our values, then we will assign the corresponding rating to that text object.
So in this approach, we're going to score the object by using the features and trained parameter values.
This score will then be compared with a set of trained alpha values to see which range the score is in.
And then, using the range, we can then decide which rating the object should be getting.
Because, these ranges of alpha values correspond to the different levels of ratings, and that's from the way we train these alpha values.
Each is tied to some level of rating.
This lecture is about smoothing of language models.
In this lecture we're going to continue talking about the probabilistic retrieval model.
In particular, we're going to talk about smoothing of language model and the query likelihood of it, which will method.
So you have seen this slide from a previous lecture.
This is the ranking function based on the query likelihood.
Here we assume that the independence of generating each query word and the formula would look like the following.
Where we take a sum over all of the query words and inside is the sum there is a log of probability of a word given by the document, or document language model.
So the main task now is to estimate this document language model.
As we said before different methods for estimating this model would lead to different retrieval functions.
So, in this lecture we're going to look into this in more detail.
So, how do I estimate this language model?
Well, the obvious choice would be the Maximum Likelihood Estimate that we have seen before.
And that is we're going to normalize the word frequencies in the document.
And the estimated probability would look like this.
This is a step function here.
Which means all the words that have the same frequency count will have an equal probability.
This is another frequency in the count that has a different probability.
Note that for words that have not occurred in the document here, they all have zero probability.
So we know this is just like a model that we assume earlier in the lecture, where we assume the user with the sample word from the document to formulate the query.
And there is no chance of sampling any word that is not in the document.
And we know that's not good.
So how would we improve this?
Well, in order to assign a non-zero probability to words that have not been observed in the document, we would have to take away some probability to mass from the words that are observing the document.
So for example here, we have to take away some [INAUDIBLE] mass, because we need some extra problem in the mass for the unseen words.
Otherwise, they won't sum to 1.
So all these probabilities must be sum to 1.
So to make this transformation, and to improve the maximum [INAUDIBLE].
By assigning nonzero probabilities to words that are not observed in the data.
We have to do smoothing, and smoothing has to do with improving the estimate by considering the possibility that, if the author had been written.
Helping, asking to write more words for the document.
The user, the author might have rethink other words.
If you think about this factor then a smoothed LM model would be a more accurate representation of the actual topic.
Imagine you have seen abstract of such article.
Let's say this document is abstract.
Right.
If we assume and see words in this abstract we have or, or probability of 0 that would mean it's no chance of sampling a word outside the abstract that the formula to query.
But imagine the user who is interested in the topic of this abstract, the user might actually choose a word that is not in the abstractor to to use as query.
So obviously if we had asked this author to write more, the author would have written a full text of that article.
So smoothing of the language model is attempted to to try to recover the model for the whole, whole article.
And then of course we don't have written knowledge about any words are not observed in the abstract there, so that's why smoothing is actually a tricky problem.
So let's talk a little more about how to smooth a LM word.
The key question here is what probability should be assigned to those unseen words.
Right.
And there are many different ways of doing that.
One idea here, that's very useful for retrieval is let the probability of an unseen word be proportional to its probability given by a reference language model.
That means if you don't observe the word in the data set, we're going to assume that its probability is kind of governed by another reference language model that we were constructing.
It will tell us which unseen words we have likely a higher probability.
In the case of retrieval a natural choice would be to take the Collection Language Model as a Reference Language Model.
That is to say if you don't observe a word in the document we're going to assume that.
The probability of this word would be proportional to the probability of the word in the whole collection.
So, more formally, we'll be estimating the probability of a word getting a document as follows.
If the word is seen in the document, then the probability would be a discounted the maximum [INAUDIBLE] estimated p sub c here.
Otherwise, if the word is not seen in the document, we'll then let probability be proportional to the probability of the word in the collection, and here the coefficient of is to control the amount of probability mass that we assign to unseen words.
Obviously all these probabilities must sum to 1.
So, alpha sub d is constrained in some way.
So, what if we plug in this smoothing formula into our query likelihood Ranking Function?
This is what we would get.
In this formula, you can see, right, we have this as a sum over all the query words.
And note that we have written in the form of a sum over all the vocabulary.
You see here this is a sum of all the words in the vocabulary, but note that we have a count of the word in the query.
So, in effect we are just taking a sum of query words, right.
This is in now a common way that we will use because of its convenience in some transformations.
So, this is as I said, this is sum of all the query words.
In our smoothing method, we're assuming the words that are not observed in the document, that we have a somewhat different form of probability.
And then it's for this form.
So we're going to then decompose this sum into two parts.
One sum is over all the query words that are matched in the document.
That means in this sum, all the words have a non zero probability, in the document, sorry.
It's, the non zero count of the word in the document.
They all occur in the document.
And they also have to, of course, have a non-zero count in the query.
So, these are the words that are matched.
These are the query words that are matched in the document.
On the other hand in this sum we are s, taking the sum over all the words that are note our query was not matched in the document.
So they occur in the query due to this term but they don't occur in the document.
In this case, these words have this probability because of our assumption about the smoothing.
But that here, these c words have a different probability.
Now we can go further by rewriting the second sum as a difference of two other sums.
Basically the first sum is actually the sum over all the query words.
Now we know that the original sum is not over the query words.
This is over all the query words that are not matched in the document.
So here we pretend that they are actually over all the query words.
So, we take a sum over all the query words.
Obviously this sum has extra terms that are, this sum has extra terms that are not in this sum.
Because here we're taking sum over all the query words.
There it's not matched in the document.
So in order to make them equal, we have to then subtract another sum here.
And this is a sum over all the query words that are mentioned in the document.
And this makes sense because here we're considering all query words.
And then we subtract the query that was matched in the document.
That will give us the query rules that not matched in the document.
And this is almost a reverse process of the first step here.
And you might wonder why we want to do that.
Well, that's because if we do this then we'll have different forms of terms inside these sums.
So, now we can see in the sum we have, all the words, matched query words, matched in the document with this kind of terms.
Here we have another sum over the same set of terms.
Matched query terms in document.
But inside the sum it's different.
But these two sums can clearly be merged.
So, if we do that we'll get another form of the formula that looks like the following at the bottom here.
And note that this is a very interesting, because here we combine the, these two, that are a sum of the query words matched in the document in the one sum here.
And the other sum, now is the compose [INAUDIBLE] to two parts, and, and these two parts look much simpler.
Just because these are the probabilities of unseen words.
But this formula is very interesting, because you can see the sum is now over all the matched query terms.
And just like in the vector space model, we take a sum of terms, that intersection of query vector and the document vector.
So it all already looks a little bit like the vector space model.
In fact there is even more severity here.
As we, we explain on this slide.
This lecture is about probabilistic and latent Semantic Analysis or PLSA.
In this lecture we're going to introduce probabilistic latent semantic analysis, often called PLSA.
This is the most basic topic model, also one of the most useful topic models.
Now this kind of models can in general be used to mine multiple topics from text documents.
And PRSA is one of the most basic topic models for doing this.
So let's first examine this power in the e-mail for more detail.
Here I show a sample article which is a blog article about Hurricane Katrina.
And I show some simple topics.
For example government response, flood of the city of New Orleans.
Donation and the background.
You can see in the article we use words from all these distributions.
So we first for example see there's a criticism of government response and this is followed by discussion of flooding of the city and donation et cetera.
We also see background words mixed with them.
So the overall of topic analysis here is to try to decode these topics behind the text, to segment the topics, to figure out which words are from which distribution and to figure out first, what are these topics?
How do we know there's a topic about government response.
There's a topic about a flood in the city.
So these are the tasks at the top of the model.
If we had discovered these topics can color these words, as you see here, to separate the different topics.
Then you can do a lot of things, such as summarization, or segmentation, of the topics, clustering of the sentences etc.
So the formal definition of problem of mining multiple topics from text is shown here.
And this is after a slide that you have seen in an earlier lecture.
So the input is a collection, the number of topics, and a vocabulary set, and of course the text data.
And then the output is of two kinds.
One is the topic category, characterization.
Theta i's.
Each theta i is a word distribution.
And second, it's the topic coverage for each document.
These are pi sub i j's.
And they tell us which document it covers.
Which topic to what extent.
So we hope to generate these as output.
Because there are many useful applications if we can do that.
So the idea of PLSA is actually very similar to the two component mixture model that we have already introduced.
The only difference is that we are going to have more than two topics.
Otherwise, it is essentially the same.
So here I illustrate how we can generate the text that has multiple topics and naturally in all cases of Probabilistic modelling would want to figure out the likelihood function.
So we would also ask the question, what's the probability of observing a word from such a mixture model?
Now if you look at this picture and compare this with the picture that we have seen earlier, you will see the only difference is that we have added more topics here.
So, before we have just one topic, besides the background topic.
But now we have more topics.
Specifically, we have k topics now.
All these are topics that we assume that exist in the text data.
So the consequence is that our switch for choosing a topic is now a multiway switch.
Before it's just a two way switch.
We can think of it as flipping a coin.
But now we have multiple ways.
First we can flip a coin to decide whether we're talk about the background.
So it's the background lambda sub B versus non-background.
us the probability of actually choosing a non-background topic.
After we have made this decision, we have to make another decision to choose one of these K distributions.
So there are K way switch here.
And this is characterized by pi, and this sum to one.
This is just the difference of designs.
Which is a little bit more complicated.
But once we decide which distribution to use the rest is the same we are going to just generate a word by using one of these distributions as shown here.
So now lets look at the question about the likelihood.
So what's the probability of observing a word from such a distribution?
What do you think?
Now we've seen this problem many times now and if you can recall, it's generally a sum.
Of all the different possibilities of generating a word.
So let's first look at how the word can be generated from the background mode.
Well, the probability that the word is generated from the background model is lambda multiplied by the probability of the word from the background mode.
Model, right.
Two things must happen.
First, we have to have chosen the background model, and that's the probability of lambda, of sub b.
Then second, we must have actually obtained the word w from the background, and that's probability of w given theta sub b.
Okay, so similarly, we can figure out the probability of observing the word from another topic.
Like the topic theta sub k. Now notice that here's the product of three terms.
And that's because of the choice of topic theta sub k, only happens if two things happen.
One is we decide not to talk about background.
So, that's a probability of 1 minus lambda sub B.
Second, we also have to actually choose theta sub K among these K topics.
So that's probability of theta sub K, or pi.
And similarly, the probability of generating a word from the second.
The topic and the first topic are like what you are seeing here.
And so in the end the probability of observing the word is just a sum of all these cases.
And I have to stress again this is a very important formula to know because this is really key to understanding all the topic models and indeed a lot of mixture models.
So make sure that you really understand the probability of w is indeed the sum of these terms.
So, next, once we have the likelihood function, we would be interested in knowing the parameters.
All right, so to estimate the parameters.
But firstly, let's put all these together to have the complete likelihood of function for PLSA.
The first line shows the probability of a word as illustrated on the previous slide.
And this is an important formula as I said.
So let's take a closer look at this.
This actually commands all the important parameters.
So first of all we see lambda sub b here.
This represents a percentage of background words that we believe exist in the text data.
And this can be a known value that we set empirically.
Second, we see the background language model, and typically we also assume this is known.
We can use a large collection of text, or use all the text that we have available to estimate the world of distribution.
Now next in the next stop this formula.
[COUGH] Excuse me.
You see two interesting kind of parameters, those are the most important parameters.
That we are.
So one is pi's.
And these are the coverage of a topic in the document.
And the other is word distributions that characterize all the topics.
So the next line, then is simply to plug this in to calculate the probability of document.
This is, again, of the familiar form where you have a sum and you have a count of a word in the document.
And then log of a probability.
Now it's a little bit more complicated than the two component.
Because now we have more components, so the sum involves more terms.
And then this line is just the likelihood for the whole collection.
And it's very similar, just accounting for more documents in the collection.
So what are the unknown parameters?
I already said that there are two kinds.
One is coverage, one is word distributions.
Again, it's a useful exercise for you to think about.
Exactly how many parameters there are here.
How many unknown parameters are there?
Now, try and think out that question will help you understand the model in more detail.
And will also allow you to understand what would be the output that we generate when use PLSA to analyze text data?
And these are precisely the unknown parameters.
So after we have obtained the likelihood function shown here, the next is to worry about the parameter estimation.
And we can do the usual think, maximum likelihood estimator.
So again, it's a constrained optimization problem, like what we have seen before.
Only that we have a collection of text and we have more parameters to estimate.
And we still have two constraints, two kinds of constraints.
One is the word distributions.
All the words must have probabilities that's sum to one for one distribution.
The other is the topic coverage distribution and a document will have to cover precisely these k topics so the probability of covering each topic that would have to sum to 1.
So at this point though it's basically a well defined applied math problem, you just need to figure out the solutions to optimization problem.
There's a function with many variables.
and we need to just figure out the patterns of these variables to make the function reach its maximum.
>>
Hello welcome to the course in Text Retrieval and Search Engines.
I'm Cheng Xiang Zhai.
I have a nickname Cheng.
I'm a professor of the Department of Computer Science at the University of Illinois at Urbana-Champaign.
this first lecture is a basic introduction to the course.
A brief introduction to what we we'll cover in the course.
We're going to first talk about the data mining specialization since this course is part of that specialization.
And then we'll cover motivation objectives of the course.
This will be followed by pre-requisites and course format and reference books.
And then finally we'll talk about the course schedule, which has number of topics to be covered in the rest of this course.
So the data mining specialization offered by the University of Illinois at Urbana-Champaign is really to address the need for data mining techniques to handle a lot of big data, to turn the big data into knowledge.
There are five lecture-based courses, as you see on the slide.
Plus one capstone, project course in the end.
I'm teaching two of them which is this course, Text Retrieval and Search Engines and this one.
So the two courses that I cover here are all about the text data.
In contrast, the other courses are covering more general techniques that can be applied to all kinds of data.
So Patent Discovery taught by the Professor Jowi Han and Cluster Analysis again taught by him about the general data mining techniques to handle structure.
The end and structure text data.
And data mine, data visualization covered by professor Jung Hart is about the general visualization techniques.
Again applicable to all kinds of data.
So the motivation for this course.
In fact also for the other courses that I'm teaching is that we have a lot of text data.
And the data is everywhere, is growing rapidly, so you must have been experiencing this growth.
Just think about how much text data you're dealing with every day.
I listed some data types here, for example, on the internet we see a lot of web pages, news articles etcetera.
And then we have block articles, emails, scientific literature, tweets, as well speaking, maybe a lot of tweets are being written, and a lot of emails are, are being sent.
So, the amount of text data is beyond our capacity to understand them.
Also, the amount of data makes it possible to actually analyze the data to discover interesting knowledge and that's what we meant by, harnessing big text data.
.
This lecture is about recommender systems.
So, so far we have talked about a lot of aspects of search engines.
And we have talked about the problem of search and the ranking problem, different methods for ranking, implementation of search engine and how to evaluate the search engine, et cetera.
This is partly because we know that web search engines are, by far, the most important applications of text retrieval.
And they are the most useful tools to help people convert big raw text data into a small set of relevant documents.
Another reason why we spend so many lectures on search engines is because many techniques used in search engines are actually also very useful for recommender systems, which is the topic of this lecture.
And so overall the two systems are actually well connected, and there are many techniques that are shared by them.
So this is a slide that you have seen before when we talked about the two different modes of text access pull and push.
And, we mentioned that recommender systems are the main systems to serve users in the push mode, where the systems will take the initiative to recommend the information to user, or to push the relevant information to the user.
And this often works well when the user has a relatively stable information need, when the system has good knowledge about what a user wants.
So a recommender system is sometimes called a filtering system.
And it's because recommending useful items to people is like discarding or filtering out the useless articles.
So in this sense, they are kind of similar.
And in all the cases, the system must make a binary decision.
And usually, there is a dynamic source of information items, and you have some knowledge about the user's interest, and then the system would make a delivery decision, whether this item is interesting to the user.
And then if it is interesting then the system would recommend the article to the user.
So the basic filtering question here is really, will this user like, this item?
Will U like item X?
And there are two ways to answer this question if you think about it, right?
One is look at what items U likes, and then we can see if X is actually like those items.
The other is to look at who likes X ,and we can see if this user looks like a, one of those users, or like most of those users.
And these strategies can be combined.
If we follow the first strategy and look at item similarity in the case of recommended text objects, then we are talking about a content-based filtering or content-based recommendation.
If we look at the second strategy then, this will compare users.
And in this case, we're exploiting user similarity, and the technique is often called a collaborative filtering.
So let's first look at the content-based filtering system.
This is what a system would look like.
Inside the system, there would be a binary classifier that would have some knowledge about the user's interests, and it's called the user interest profile.
It maintains the profile to keep track of the user's interest.
And then there is a utility function to guide the user to make decisions, and I'll explain the utility of the function in a moment.
It helps the system decide where to set the threshold.
And then the accepted documents will be those that have passed the threshold according to the classifier.
There should be also an initialization module that would take a user's input, maybe from a user's, specified keywords, or a chosen category, et cetera.
And this will be, to feed the system with a initial user profile.
There is also typically a learning module that will learn from users' feedback over time.
Now note that in this case, typically users' information need is stable so the system would have a lot of opportunities to observe the users, you know, if the user has taken a recommended item as viewed that, and this is a cu, a signal to indicate that the recommended item may be relevant.
If the user discarded it, no, it's not relevant.
And so, such feedback can be a long-term feedback and can last for a long time and the system can clock, collect a lot of information about this user's interests.
And this can then be used to improve the classifier.
Now whats the criteria for evaluating such a system?
How do we know this filtering system actually performs well?
Now in this case we cannot use the ranking evaluation measures, like a map, because we can't afford waiting for a lot of documents, and then rank the documents to make a decision for the user.
And so, the system must make a decision, in real time, in general to decide whether the item is above the threshold or not.
So in other words, we're trying to decide absolute relevance.
So in this case one common use strategy is to use a utility function through a valid system.
So here I show a linear utility function that's defined as, for example, good items that you delivered, minus 2 multiplied by the number of bad items you delete, that you delivered.
So in other words, we, we could kind of just treat this as almost a, in a gambling game.
If you delete, if you deliver one good item, let's say you win $3, you gain $3.
But if you deliver a bad one, you would lose $2.
And this utility function basically kind of measures, how much money you would, get by doing this kind of game, right.
And so it's clear that if you want to maximize this utility function, your strategy should be to deliver as many good articles as possible, and minimize the delivery of bad articles.
That, that's obvious, right.
Now one interesting question here is, how should we set these coefficients?
Now I just showed a 3 and a negative 2, as the possible coefficients, but one can ask the question, are they reasonable?
So what do you think?
Do you think that's a reasonable choice?
What about other choices?
So for example, we can have 10 and minus 1, or 1 minus 10.
What's the difference?
What do you think?
How would this utility function affect the system's threshold of this issue?
Right, you can think of these two extreme cases, 10 minus 1 versus 1 minus 10.
Which one do we think it would encourage the system to over-deliver?
And which one would encourage the system to be conservative?
Yeah?
If you think about it, you will see that when we get a big award for delivering a good document, you incur only a small penalty for delivering a bad one.
Intuitively, you would be encouraging to deliver more, right?
And you can try to deliver more in hope of getting a good one delivered, and then you'll get a big award.
Right, so on the other hand, if you choose 1 minus 10, you don't really get such a big prize if you deliver a good document.
On the other hand, you will have a big loss if you deliver bad one.
You can imagine that the system would be very reluctant to deliver lot of documents.
It has to be absolutely sure that it's a non-relevant one.
So this utility function has to be designed based on a specific application.
The three basic problems in content-based filtering are the following.
First has to make a filtering decision.
So it has to be a binary decision maker, a binary classifier.
Given a text, a text document, and a profile description of the user, it has to say yes or no, whether this document should be delivered or not.
So that's a decision module, and there should be a initialization module as you have seen earlier.
And this is to get the system started.
And we have to initialize the system based on only very limited text description, or very few examples from the user.
And the third component is a learning module which ha, has to be able to learn from limited relevance judgments because we can only learn from the user about their preferences on the delivery documents.
If we don't deliver a document to the user, we'd never know we would never be able to know whether the user likes it or not, right.
And we can accumulate a lot of documents, we can learn from the entire history.
Now, all these models would have to be optimized to maximize the utility.
So how can we build a such a system?
And there are many different approaches.
Here we are going to talk about how to extend a retrieval system, a search engine for information filtering.
Again, here's why we've spent a lot of times talk about the search engines.
Because it's actually not very hard to extend the search engine for information filtering.
So, here is the basic idea for extending a retrieval system for information filtering.
First, we can reuse a lot of retrieval techniques to do scoring.
All right, so we know how to score documents against queries et cetera.
We can measure the similarity between a profile text description and a document.
And then we can use a score threshold for the filtering decision.
We, we do retrieval and then we kind of find the scores of documents, and then we apply a threshold to, to say, to see whether a document is passing this threshold or not.
And if it's passing the threshold, we are going to say it's relevant and we are going to deliver it to the user.
And another component that we have to add is, of course, to learn from the history.
And here we can use the traditional feedback techniques to learn to improve scoring.
And we know Rocchio can be used for scoring improvement, right?
And, but we have to develop new approaches to learn how to set the threshold.
And you know, we need to set it initially, and then we have to learn how to update the threshold over time.
So here's what the system might look like if we just generalized a vector-space model for filtering problems, right?
So you can see the document vector could be fed into a scoring module, which it already exists in in a search engine that implements a vector-space model.
And the profile will be treated as a query essentially.
And then the profile vector can be matched with the document vector, to generate the score.
And then this score will be fed into a thresholding module that would say yes or no.
And then the evaluation would be based on the utility for the filtering results.
If it says yes, and then the document will be sent to the user, and then the user could give some feedback.
And the feedback information would have been use, would be used to both adjust to the threshold and adjust the vector representation.
So the vector learning is essentially the same as query modification or feedback in the case of search.
The threshold learning is a no, new component in that we need to talk a little bit more about.
.
This lecture is about the vector space retrieval model.
We're going to give an introduction to its basic idea.
In the last lecture we talked about the different ways of designing a retrieval model which would give us a different the ranking function.
In this lecture, we're going to talk about the, the specific way of design the ramping function called a vector space mutual model.
And we're going to give a brief introduction to the basic idea.
Vector space model is a special case of similarity based models as we discussed before.
Which means, we assume relevance is roughly similarity between a document and a query.
Now whether this assumption is true, is actually a question.
But in order to solve our search problem we have to convert the vague notion of relevance into a more precise definition that can be implemented with the programming language.
So in this process we have to make a number of assumptions.
This is the first assumption that we make here.
Basically we assume that if a document is more similar to a query than another document, then the first document would be assumed to be more relevant than the second one.
And this is the basis for ranking documents in this approach.
Again, it's questionable whether this is really the best definition for relevance.
As we will see later there are other ways to model relevance.
The first idea of vector space retrieval model is actually very easy to understand.
Imagine a high dimensional space, where each dimension corresponds to a term.
So, here, I show a three dimensional space with three words, programming, library, and presidential.
So each term, here, defines one dimension.
Now we can consider vectors in this three dimensional space.
And we're going to assume all our documents and the query will be placed in this vector space.
So, for example, one document that might be represented at by this vector, d1.
Now this means this document probably covers library and presidential.
But it doesn't really talk about programming.
All right, what does this mean in terms of presentation of document?
That just means, we're going to look at our document from the perspective of this vector.
We're going to ignore everything else.
Basically what we see here is only the vector of the document.
Of course the document has other information.
For example, the orders of words are simply ignored and that's because we're assume that the words.
So with this representation you have already seen, d1, seems to suggest a topic in either presidential library.
Now this is different from another document.
Which might be represented as a different vector, d2 here.
Now in this case, the document that covers programming and library, but it doesn't talk about presidential.
So what does this remind you?
Well, you can probably guess, the topic is likely about program language and the library is software library, library.
So this shows that by using this vector space representation, we can actually capture the differences between topics of documents.
Now you can also imagine there are other vectors.
For example, d3 is pointing in that direction, that might be about presidential programming.
And in fact we're going to place all the documents in this vector space.
And they will be pointing to all kinds of directions.
And similarly, we're going to place our query also in this space, as another vector.
And then we're going to measure the similarity between the query vector and every document vector.
So, in this case for example, we can easily see d2 seems to be the closest of, to this query factor and therefore d2 will be ranked above others.
So this was a, basically the main idea of the, the vector space model.
So to be more pri, precise, be more precise.
Vector space model is a framework.
In this framework, we make the following assumptions.
First, we represent a document and query by a term vector.
So here a term can be any basic concept.
For example, a word or a phrase, or even enneagram of characters.
Those are a sequence of characters inside a word.
Each term is assumed to define one dimension.
Therefore N terms.
In our vocabulary, we define N-dimensional space.
A query vector would consist of a number of elements corresponding to the weights of different terms.
Each document vector is also similar.
It has a number of elements and each value of each element is indicating that weight of the corresponding term.
Here you can see, we have seen there are N dimensions.
Therefore, there are N elements, each corresponding to the weight on the particular term.
So the relevance in this case would be assume to be the similarity between the two vectors, therefore our range in function is also defined as the similarity between the query vector and document vector.
Now, if I ask you to write the program to the internet this approach in the search engine.
You would realize that this was far from clear, right?
We haven't seen a lot of things in detail therefore it's impossible to actually write the program to implement this.
That's why I said this is a framework.
And this has to be refined in order to actually suggest a particular function, that you can implement on the computer.
So, what does this framework not serve?
Well, it actually hasn't set many things that would be required in order to implement this function.
First, it did not say how we should define or select the basic concepts exactly.
We clearly assume the concepts are orthogonal, otherwise there will be redundancy.
For example, if two synonyms are somehow distinguished as two different concepts.
Then they would be defined in two different dimensions.
And then that would clearly cause a redundancy here.
Or overemphasizing of matching this concept.
Because it would be as if you matched the two dimensions when you actually matched one semantic concept.
Secondly, it did not say how we exactly should place documents and query in this space.
Basically I show you some examples of query and document vectors.
But where exactly should the vector for a particular document point to?
[INAUDIBLE] So this is equivalent to how to define the term weights.
How do you computer use element values in those vectors?
This is a very important question because term weight in the query vector indicates the importance of term.
So depending on how you assign the weight, you might prefer some terms to be matched over others.
Similarly, term weight in the document is also very meaningful.
It indicates how well the term characterizes the document.
If you got it wrong, then you clearly don't represent this document accurately.
Finally, how we define the similarity measure is also not clear.
So these questions must be addressed before we can have an operational function that we can actually implement using a program language.
So how do we solve these problems is the main topic of the next lecture.
This lecture is about the mixture of unigram language models.
In this lecture we will continue discussing probabilistic topic models.
In particular, what we introduce a mixture of unigram language models.
This is a slide that you have seen earlier.
Where we talked about how to get rid of the background words that we have on top of for one document.
So if you want to solve the problem, it would be useful to think about why we end up having this problem.
Well, this obviously because these words are very frequent in our data and we are using a maximum likelihood to estimate.
Then the estimate obviously would have to assign high probability for these words in order to maximize the likelihood.
So, in order to get rid of them that would mean we'd have to do something differently here.
In particular we'll have to say this distribution doesn't have to explain all the words in the tax data.
What were going to say is that, these common words should not be explained by this distribution.
So one natural way to solve the problem is to think about using another distribution to account for just these common words.
This way, the two distributions can be mixed together to generate the text data.
And we'll let the other model which we'll call background topic model to generate the common words.
This way our target topic theta here will be only generating the common handle words that are characterised the content of the document.
So, how does this work?
Well, it is just a small modification of the previous setup where we have just one distribution.
Since we now have two distributions, we have to decide which distribution to use when we generate the word.
Each word will still be a sample from one of the two distributions.
Text data is still generating the same way.
Namely, look at the generating of the one word at each time and eventually we generate a lot of words.
When we generate the word, however, we're going to first decide which of the two distributions to use.
And this is controlled by another probability, the probability of theta sub d and the probability of theta sub B here.
So this is a probability of enacting the topic word of distribution.
This is the probability of enacting the background word of distribution denoted by theta sub B.
On this case I just give example where we can set both to 0.5.
So you're going to basically flip a coin, a fair coin, to decide what you want to use.
But in general these probabilities don't have to be equal.
So you might bias toward using one topic more than the other.
So now the process of generating a word would be to first we flip a coin.
Based on these probabilities choosing each model and if let's say the coin shows up as head, which means we're going to use the topic two word distribution.
Then we're going to use this word distribution to generate a word.
Otherwise we might be going slow this path.
And we're going to use the background word distribution to generate a word.
So in such a case, we have a model that has some uncertainty associated with the use of a word distribution.
But we can still think of this as a model for generating text data.
And such a model is called a mixture model.
So now let's see.
In this case, what's the probability of observing a word w?
Now here I showed some words.
like "the" and "text".
So as in all cases, once we setup a model we are interested in computing the likelihood function.
The basic question is, so what's the probability of observing a specific word here?
Now we know that the word can be observed from each of the two distributions, so we have to consider two cases.
Therefore it's a sum over these two cases.
The first case is to use the topic for the distribution to generate the word.
And in such a case then the probably would be theta sub d, which is the probability of choosing the model multiplied by the probability of actually observing the word from that model.
Both events must happen in order to observe.
We first must have choosing the topic theta sub d and then, we also have to actually have sampled the word the from the distribution.
And similarly, the second part accounts for a different way of generally the word from the background.
Now obviously the probability of text the same is all similar, right?
So we also can see the two ways of generating the text.
And in each case, it's a product of the probability of choosing a particular word is multiplied by the probability of observing the word from that distribution.
Now whether you will see, this is actually a general form.
So might want to make sure that you have really understood this expression here.
And you should convince yourself that this is indeed the probability of obsolete text.
So to summarize what we observed here.
The probability of a word from a mixture model is a general sum of different ways of generating the word.
In each case, it's a product of the probability of selecting that component model.
Multiplied by the probability of actually observing the data point from that component of the model.
And this is something quite general and you will see this occurring often later.
So the basic idea of a mixture model is just to retrieve thesetwo distributions together as one model.
So I used a box to bring all these components together.
So if you view this whole box as one model, it's just like any other generative model.
It would just give us the probability of a word.
But the way that determines this probability is quite the different from when we have just one distribution.
And this is basically a more complicated mixture model.
So the more complicated is more than just one distribution.
And it's called a mixture model.
So as I just said we can treat this as a generative model.
And it's often useful to think of just as a likelihood function.
The illustration that you have seen before, which is dimmer now, is just the illustration of this generated model.
So mathematically, this model is nothing but to just define the following generative model.
Where the probability of a word is assumed to be a sum over two cases of generating the word.
And the form you are seeing now is a more general form that what you have seen in the calculation earlier.
Well I just use the symbol w to denote any water but you can still see this is basically first a sum.
Right?
And this sum is due to the fact that the water can be generated in much more ways, two ways in this case.
And inside a sum, each term is a product of two terms.
And the two terms are first the probability of selecting a component like of D Second, the probability of actually observing the word from this component of the model.
So this is a very general description of all the mixture models.
I just want to make sure that you understand this because this is really the basis for understanding all kinds of on top models.
So now once we setup model.
We can write down that like functioning as we see here.
The next question is, how can we estimate the parameter, or what to do with the parameters.
Given the data.
Well, in general, we can use some of the text data to estimate the model parameters.
And this estimation would allow us to discover the interesting knowledge about the text.
So you, in this case, what do we discover?
Well, these are presented by our parameters and we will have two kinds of parameters.
One is the two worded distributions, that result in topics, and the other is the coverage of each topic in each.
The coverage of each topic.
And this is determined by probability of C less of D and probability of theta, so this is to one.
Now, what's interesting is also to think about special cases like when we send one of them to want what would happen?
Well with the other, with the zero right?
And if you look at the likelihood function, it will then degenerate to the special case of just one distribution.
Okay so you can easily verify that by assuming one of these two is 1.0 and the other is Zero.
So in this sense, the mixture model is more general than the previous model where we have just one distribution.
It can cover that as a special case.
So to summarize, we talked about the mixture of two Unigram Language Models and the data we're considering here is just One document.
And the model is a mixture model with two components, two unigram LM models, specifically theta sub d, which is intended to denote the topic of document d, and theta sub B, which is representing a background topic that we can set to attract the common words because common words would be assigned a high probability in this model.
So the parameters can be collectively called Lambda which I show here you can again think about the question about how many parameters are we talking about exactly.
This is usually a good exercise to do because it allows you to see the model in depth and to have a complete understanding of what's going on this model.
And we have mixing weights, of course, also.
So what does a likelihood function look like?
Well, it looks very similar to what we had before.
So for the document, first it's a product over all the words in the document exactly the same as before.
The only difference is that inside here now it's a sum instead of just one.
So you might have recalled before we just had this one there.
But now we have this sum because of the mixture model.
And because of the mixture model we also have to introduce a probability of choosing that particular component of distribution.
And so this is just another way of writing, and by using a product over all the unique words in our vocabulary instead of having that product over all the positions in the document.
And this form where we look at the different and unique words is a commutative that formed for computing the maximum likelihood estimate later.
And the maximum likelihood estimator is, as usual, just to find the parameters that would maximize the likelihood function.
And the constraints here are of course two kinds.
One is what are probabilities in each [INAUDIBLE] must sum to 1 the other is the choice of each [INAUDIBLE] must sum to 1.
This lecture is about generating probabilistic models for text clustering.
In this lecture, we're going to continue discussing text clustering, and we're going to introduce generating probabilistic models as a way to do text clustering.
So this is the overall plan for covering text clustering.
In the previous lecture, we have talked about what is text clustering and why text clustering is interesting.
In this lecture, we're going to talk about how to do text clustering.
In general, as you see on this slide, there are two kinds of approaches.
One is generating probabilistic models, which is the topic of this lecture.
And later, we'll also discuss similarity-based approaches.
So to talk about generating models for text clustering, it would be useful to revisit the topic mining problem using topic models, because the two problems are very similar.
This is a slide that you have seen earlier in the lecture on topic model.
Here we show that we have input of a text collection C and a number of topics k, and vocabulary V. And we hope to generate as output two things.
One is a set of topics denoted by Theta i's, each is awarded distribution and the other is pi i j.
These are the probabilities that each document covers each topic.
So this is a topic coverage and it's also visualized here on this slide.
You can see that this is what we can get by using a topic model.
Now, the main difference between this and the text clustering problem is that here, a document is assumed to possibly cover multiple topics.
And indeed, in general, a document will be covering more than one topic with nonzero probabilities.
In text clustering, however, we only allow a document to cover one topic, if we assume one topic is a cluster.
So that means if we change the problem definition just slightly by assuming that each document that can only be generated by using precisely one topic.
Then we'll have a definition of the clustering problem as you'll hear.
So here the output is changed so that we no longer have the detailed coverage distributions pi i j.
But instead, we're going to have a cluster assignment decisions, Ci.
And Ci is a decision for the document i.
And C sub i is going to take a value from 1 through k to indicate one of the k clusters.
And basically tells us that d i is in which cluster.
As illustrated here, we no longer have multiple topics covered in each document.
It is precisely one topic.
Although which topic is still uncertain.
There is also a connection with the problem of mining one topic that we discussed earlier.
So here again, it's a slide that you have seen before and here we hope to estimate a topic model or distribution based on precisely one document.
And that's when we assume that this document, it covers precisely one topic.
But we can also consider some variations of the problem.
For example, we can consider there are N documents, each covers a different topic, so that's N documents, and topics.
Of course, in this case, these documents are independent, and these topics are also independent.
But, we can further allow these documents with share topics, and we can also assume that we are going to assume there are fewer topics than the number of documents, so these documents must share some topics.
And if we have N documents that share k topics, then we'll again have precisely the document clustering problem.
So because of these connections, naturally we can think about how to use a probabilistically generative model to solve the problem of text clustering.
So the question now is what generative model can be used to do clustering?
As in all cases of designing a generative model, we hope the generative model would adopt the output that we hope to generate or the structure that we hope to model.
So in this case, this is a clustering structure, the topics and each document that covers one topic.
And we hope to embed such preferences in the generative model.
But, if you think about the main difference between this problem and the topic model that we talked about earlier.
And you will see a main requirement is how can we force every document to be generated from precisely one topic, instead of k topics, as in the topic model?
So let's revisit the topic model again in more detail.
So this is a detailed view of a two component mixture model.
When we have k components, it looks similar.
So here we see that when we generate a document, we generate each word independent.
And when we generate each word, but first make a choice between these distributions.
We decide to use one of them with probability.
So p of theta 1 is the probability of choosing the distribution on the top.
Now we first make this decision regarding which distribution should be used to generate the word.
And then we're going to use this distribution to sample a word.
Now note that in such a generative model, the decision on which distribution to use for each word is independent.
So that means, for example, the here could have generated from the second distribution, theta 2 whereas text is more likely generated from the first one on the top.
That means the words in the document that could have been generated in general from multiple distributions.
Now this is not what we want, as we said, for text clustering, for document clustering, where we hoped this document will be generated from precisely one topic.
So now that means we need to modify the model.
But how?
Well, let's first think about why this model cannot be used for clustering.
And as I just said, the reason is because it has allowed multiple topics to contribute a word to the document.
And that causes confusion because we're not going to know which cluster this document is from.
And it's, more importantly it's violating our assumption about the partitioning of documents in the clusters.
If we really have one topic to correspond it to one cluster of documents, then we would have a document that we generate from precisely one topic.
That means all the words in the document must have been generated from precisely one distribution.
And this is not true for such a topic model that we're seeing here.
And that's why this cannot be used for clustering because it did not ensure that only one distribution has been used to generate all the words in one document.
So if you realize this problem, then we can naturally design alternative mixture model for doing clustering.
So this is what you're seeing here.
And we again have to make a decision regarding which distribution to use to generate this document because the document could potentially be generated from any of the k word distributions that we have.
But this time, once we have made a decision to choose one of the topics, we're going to stay with this regime to generate all the words in the document.
And that means, once we have made a choice of the distribution in generating the first word, we're going to go stay with this distribution in generating all of the other words in the document.
So, in other words, we only make the choice once for, basically, we make the decision once for this document and this state was just to generate all the words.
Similarly if I had choosing the second distribution, theta sub 2 here, you can see which state was this one.
And then generate the entire document of d. Now, if you compare this picture with the previous one, you will see the decision of using a particular distribution is made just once for this document, in the case of document clustering.
But in the case of topic model, we have to make as many decisions as the number of words in the document.
Because for each word, we can make a potentially different decision.
And that's the key difference between the two models.
But this is obviously also a mixed model so we can just group them together as one box to show that this is the model that will give us a probability of the document.
Now, inside of this model, there is also this switch of choosing a different distribution.
And we don't observe that so that's a mixture model.
And of course a main problem in document clustering is to infer which distribution has been used to generate a document and that would allow us to recover the cluster identity of a document.
So it will be useful to think about the difference from the topic model as I have also mentioned multiple times.
And there are mainly two differences, one is the choice of using that particular distribution is made just once for document clustering.
Whereas in the topic model, it's made it multiple times for different words.
The second is that word distribution, here, is going to be used to regenerate all the words for a document.
But, in the case of one distribution doesn't have to generate all the words in the document.
Multiple distribution could have been used to generate the words in the document.
Let's also think about a special case, when one of the probability of choosing a particular distribution is equal to 1.
Now that just means we have no uncertainty now.
We just stick with one particular distribution.
Now in that case, clearly, we will see this is no longer mixture model, because there's no uncertainty here and we can just use precisely one of the distributions for generating a document.
And we're going back to the case of estimating one order distribution based on one document.
So that's a connection that we discussed earlier.
Now you can see it more clearly.
So as in all cases of using a generative model to solve a problem, we first look at data and then think about how to design the model.
But once we design the model, the next step is to write down the likelihood function.
And after that we're going to look at the how to estimate the parameters.
So in this case, what's the likelihood function?
It's going to be very similar to what you have seen before in topic models but it will be also different.
Now if you still recall what the likelihood function looks like in then you will realize that in general, the probability of observing a data point from mixture model is going to be a sum of all the possibilities of generating the data.
In this case, so it's going to be a sum over these k topics, because every one can be user generated document.
And then inside is the sum you can still recall what the formula looks like, and it's going to be the product of two probabilities.
One is the probability of choosing the distribution, the other is the probability of observing a particular datapoint from that distribution.
So if you map this kind of formula to our problem here, you will see the probability of observing a document d is basically a sum in this case of two different distributions because we have a very simplified situation of just two clusters.
And so in this case, you can see it's a sum of two cases.
In each case, it's indeed the probability of choosing the distribution either theta 1 or theta 2.
And then, the probability is multiplied by the probability of observing this document from this particular distribution.
And if you further expanded this probability of observing the whole document, we see that it's a product of observing each word X sub i.
And here we made the assumption that each word is generated independently, so the probability of the whole document is just a product of the probability of each word in the document.
So this form should be very similar to the topic model.
But it's also useful to think about the difference and for that purpose, I am also copying the probability of topic model of these two components here.
So here you can see the formula looks very similar or in many ways, they are similar.
But there is also some difference.
And in particular, the difference is on the top.
You see for the mixture model for document clustering, we first take a product, and then take a sum.
And that's corresponding to our assumption of first make a choice of choosing one distribution and then stay with the distribution, it'll generate all the words.
And that's why we have the product inside the sum.
The sum corresponds to the choice.
Now, in topic model, we see that the sum is actually inside the product.
And that's because we generated each word independently.
And that's why we have the product outside, but when we generate each word we have to make a decision regarding which distribution we use so we have a sum there for each word.
But in general, these are all mixture models and we can estimate these models by using the Algorithm, as we will discuss more later.
This lecture is about the text retrieval problem.
This picture shows our overall plan for lectures.
In the last lecture, we talked about the high level strategies for text access.
We talked about push versus pull.
Search engines are the main tools for supporting the pull mode.
Starting from this lecture, we're going to talk about the how search engines work in detail.
So first, it's about the text retrieval problem.
We're going to talk about the three things in this lecture.
First, we'll define text retrieval.
Second, we're going to make a comparison between text retrieval and the related task, database retrieval.
Finally, we're going to talk about the document selection versus document ranking as two strategies for responding to a user's query.
So what is text retrieval?
It should be a task that's familiar to most of us because we're using web search engines all the time.
So text retrieval is basically a task where the system would respond to a user's query with relevant lock-ins, basically through supported querying as one way to implement the poor mold of information access.
So the scenario's the following.
You have a collection of text documents.
These documents could be all the web pages on the web.
Or all the literature articles in the digital library or maybe all the text files in your computer.
A user will typically give a query to the system to express the information need.
And then the system would return relevant documents to users.
Relevant documents refer to those documents that are useful to the user who typed in the query.
Now this task is a often called information retrieval.
But literally, information retrieval would broadly include the retrieval of other non-textual information as well.
For example, audio, video, et cetera.
It's worth noting that text retrieval is at the core of information retrieval in the sense that other medias such as video can be retrieved by exploiting the companion text data.
So for example, can the image search engines actually match a user's query with the companion text data of the image?
This problem is also called the, the search problem, and the technology is often called search technology in industry.
If you ever take on course in databases, it'll be useful to pause the lecture at this point and think about the differences between text retrieval and database retrieval.
Now these two tasks are similar in many ways.
But there are some important differences.
So, spend a moment to think about the differences between the two.
Think about the data and information managed by a search engine versus those that are man, managed by a database system.
Think about the difference between the queries that you typically specify for a database system versus the queries that typed in by users on the search engine.
And then finally think about the answers.
What's the difference between the two?
Okay, so if we think probably the information out there are managed by the two systems.
We will see that in text retrieval, the data is unstructured, it's free text.
But in databases, they are structured data, where there is a clear defined schema to tell you this column is the names of people and that column is ages, et cetera.
In unstructured text, it's not obvious what are the names of people mentioned in the text.
Because of this difference, we can also see that text information tends to be more ambiguous.
And we'll talk about that in the natural language processing lecture.
Whereas in databases, the data tend to have well-defined semantics.
There is also important difference in the queries, and this is partly due to the difference in the information, or data.
So text queries tend to be ambiguous, whereas in their research, the queries are particularly well-defined.
Think about the SQL query, that would clear the specify what records to be returned.
So it has very well defined semantics.
Queue all queries or naturally ending queries tend to be incomplete.
Also in that it doesn't really, fully specify what documents should be retrieved.
Whereas, in the database search, the SQL query can be regarded as a computer specification for what should be returned.
And because of these differences, the answers would be also different.
In the case of text retrieval, we're looking for relevant documents.
In the database search, we are retrieving records or matched records with the SQL query, more precisely.
Now in the case of text retrieval, what should be the right answers to a query is not very well specified, as we just discussed.
So it's unclear what should be the right answers to a query.
And this has very important consequences, and that is text retrieval is an empirically defined problem.
And so this a problem because if it's empirically defined, then we cannot mathematically prove one method is better than another method.
That also means we must rely on emperical evaluation more than users to know which method works better.
And that's why we have one lecture, actually more than one lectures to cover the issue of evaluation.
Because this is a very important topic for search engines.
Without knowing how to evaluate an algorithm appropriately, there's no way to tell whether we have got the better algorithm or whether one system is better than another.
So now let's look at the problem in a formal way.
So this slide shows a formal formulation of the text retrieval problem.
First, we have our vocabulary set which is just a set of words in a language.
Now here, we're considering just one language, but in reality on the web there might be multiple natural languages.
We have text that are in all kinds of languages.
But here for simplicity, we just assume there is one kind of language.
As the techniques used for retrieving data from multiple languages,.
Are more or less similar to the techniques used for retrieving documents in one language.
Although there is important difference, the principles and methods are very similar.
Next we have the query, which is a sequence of words.
And so here you can see the query is defined as a sequence of words.
Each q sub i is a word in the vocabulary.
A document is defined in the same way.
So it's also a sequence of words.
And here, d sub ij is also a word in the vocabulary.
Now typically, the documents are much longer than queries.
But there are also cases where the documents may be very short.
So you can think about the, what might be a example of that case.
I hope you can think of, of twitter search, all right?
Tweets are very short.
But in general, documents are longer then the queries.
Now, then we have a collection of documents.
And this collection can be very large.
So think about the web.
It could, could be very large.
And then the goal of text retrieval is you'll find the set of relevant documents, which we denote by R of q, because it depends on the query.
And this is, in general, a subset of all the documents in the collection.
Unfortunately, this set of random documents is generally unknown, and usually depend in the sense that for the same query typed in by different users, the expected relevant documents may be different.
The query given to us by the user is only a hint on which document should be in this set.
And indeed, the user is generally unable to specify what exactly should be in the set, especially in the case of a web search where the collection is so large.
The user doesn't have complete knowledge about the whole collection.
So, the best a search system can do is to compute an approximation of this relevent document set.
So we denote it by R prime of q.
So, formally, we can see the task is to compute this R prime of q, an approximation of the relevant documents.
So how can we do that?
Now, imagine if you are now asked to write a program to do this.
What would you do?
Now think for a moment.
Right, so these are your input.
With the query, the documents and then you will have computed the answers to this query, which is set of documents that would be useful to the user.
So how would you solve the problem?
In general there are two strategies that we can use.
All right, the first strategy is to do document selection.
And that is, we're going to have a binary classification function, or binary classified.
That's a function that will take a document and query as input, and then give a zero or one as output, to indicate whether this document is relevant to the query, or not.
So in this case, you can see the document.
The, the relevant document set is defined as follows.
It basically, all the documents that have a value of one by this function.
And so in this case, you can see the system must have decided if a document is relevant or not.
Basically, that has to say whether it's one or zero.
And this is called absolute relevance.
Basically, it needs to know exactly whether it's going to be useful to the user.
Alternatively, there's another strategy called document ranking.
Now in this case, the system is not going to make a call whether a document is relevant or not.
Rather, the system's going to use a real value function, f, here that would simply give us a value.
That would indicate which document is more likely relevant.
So it's not going to make a call whether this document is relevant or not, but rather it would say which document is more likely relevant.
So this function then can be used to rank the documents.
And then we're going to let the user decide where to stop when the user looks at the documents.
So we have a threshold, theta, here to determine what documents should be in this approximation set.
And we're going to assume that all the documents that are ranked above this threshold are in the set.
Because in effect, these are the documents that we delivered to the user.
And theta is a cutoff determined by the user.
So here we've got some collaboration from the user in some sense because we don't really make a cutoff, and the user kind of helped the system make a cutoff.
So in this case, the system only needs to decide if one document is more likely relevant than another.
And that is, it only needs for determined relative relevance as opposed to absolute relevance.
Now you can probably already sense that relevant, relative relevance would be easier to determine the absolute relevance.
Because in the first case, we have to say exactly whether a document is relevant or not, right?
And it turns out that ranking is indeed generally preferred to document selection.
So let's look this these two strategies in more detail.
So this pictures shows how it works.
So on the left side, we see these documents.
And we use the pluses to indicate the relevant documents.
So we can see the true relevant documents here consists this set of true relevant documents consists of these pluses, these documents.
And with the document selection function, we can do, basically classify them into two groups, relevant documents and non-relevant ones.
Of course, the classifier will not be perfect, so it will make mistakes.
So here we can see in the approximation of the relevant documents we have got some non-relevant documents.
And similarly, there's a relevant document that that's misclassified as non-relevant.
In the case of document ranking, we can see the system seems like simply ranks all the documents in the descending order of the scores.
And we're going to let the user stop wherever the user wants to stop.
So if a user wants to examine more documents, then the user will go down the list to examine more and stop at the lower position.
But if the user only wants to read a few random documents, the user might stop at the top position.
So in this case, the user stops at d4, so the effect, we have delivered these four documents to our user.
So as I said, ranking is generally preferred.
And one of the reasons is because the classifier, in the case of document selection, is unlikely accurate.
Why?
Because the only clue is usually the query.
But the query may not be accurate, in the sense that it could be overly constrained.
For example, you might expect the relevant documents to talk about all these topics you, by using specific vocabulary, and as a result, you might match no random documents, because in the collection, no others have discussed the topic using these vocabularies.
All right.
So in this case, we'll see there is this problem of no relevant documents to return in the case of overly constrained query.
On the other hand, if the query is under constrained, for example, if the query does not have sufficient discriminating words you'll find in relevant documents, you may actually end up having.
over delivery.
And this is when you thought these words might be sufficient to help you find the relevant documents, but it turns out that they're not sufficient.
And there are many distraction documents using similar words.
And so this is the case of over delivery.
Unfortunately, it's very hard to find the right position between these two extremes.
Why?
Because, when the users looking for the information in general, the user does not have a good knowledge about the the information to be found.
And in that case, the user does not have a good knowledge about what vocabularies will be used in those random documents.
So it's very hard for a user to pre-specify the right level of of constraints.
Even if the class file is accurate, we also still want to rank these relevant documents because they are generally not equally relevant.
Relevance is often a matter of degree.
So we must prioritize these documents for user to exam.
And this, note that this prioritization is very important, because a user cannot digest all the contents at once.
The user generally would have to look at each document sequentially.
And therefore, it would make sense to feed users with the most relevant documents, and that's what ranking is doing.
So for these reasons ranking is generally preferred.
Now, this preference also has a theoretical justification, and this is given by the probability ranking principle.
In the end of this lecture there is a reference for this.
This principal says, returning a ranked list of documents in descending order of probability, that a document is relevant to the query, is the optimal strategy under the following two assumptions.
First, the utility of a document to a user Is independent of the utility of any other document.
Second, a user would be assumed to browse the results sequentially.
Now it's easy to understand why these two assumptions are needed, in order to justify for the ranking, strategy.
Because, if the documents are independent, then we can evaluate the utility of each document that's separate.
And this would allow us to compute a score for each document independently.
And then we're going to rank these documents based on those scores.
The second assumption is to say that the user would indeed follow the rank list.
If the user is not going to follow the ranked list, is not going to examine the documents sequentially, then obviously the ordering would not be optimal.
So under these two assumptions, we can theoretically justify the ranking strategy is in fact the best that you could do.
Now I've put one question here.
Do these 2 assumptions hold?
Now I suggest you to pause the lecture for a moment to think about these.
Now can you think of some examples that would suggest these assumptions aren't necessarily true?
Now if you think for a moment you may realize none of the assumptions is actually true.
For example in the case of independence assumption, we might have identical documents that have similar content or exactly the same content.
If you look at each of them alone, each is relevant.
But if the user has already seen one of them, we assume it's generally not very useful for the user to see another similar or duplicate one.
So clearly the utility of a document is dependent on other documents that the user has seen.
In some other cases, you might see a scenario where one document that may not be useful to the user, but when three particular documents are put together, they provide answer to the user's question.
So this is collective relevance.
And that also suggests that the value of the document might depend on other documents.
Sequential browsing generally would make sense if you have a ranked list there.
But even if you have a run list, there is evidence showing that users don't always just go strictly sequentially through the entire list.
They sometimes would look at the bottom for example, or skip some.
And if you think about the more complicated interfaces that would possibly use like, two dimensional interface where you can put additional information on the screen, then sequential browsing is a very restrictive assumption.
So the point here is that, none of these assumptions is really true, but nevertheless, the probability ranking principle establishes some solid foundation for ranking as a primary task for search engines.
And this has actually been the basis for a lot of research work in information retrieval.
And many algorithms have been designed based on this assumption.
Despite that the assumptions aren't necessarily true.
And we can, address this problem by doing post processing of a ranked list.
For example, to remove redundancy.
So to summarize this lecture, the main points that you can take away are the following.
First, text retrieval is an empirically defined problem.
And that means which algorithm is better must be judged by the users.
Second, document ranking is generally prefer and this is, will help users prioritize examination of search results.
And this is also to bypass the difficulty in determining absolute relevance, because we can get some help from users in determining where to make the cut off.
It's more flexible.
So this further suggests that the main technical challenge in designing the search engine is with designing effective ranking function.
In other words, we need to define what is the value of this function f on the query and document pair.
Now how to design such a function is a main topic in the following lectures.
There are two suggested additional readings.
The first is the classic paper on probability ranking principle.
The second, is a must read for anyone doing research information retrieval.
It's classical IR book, which has excellent coverage of the main research results in early days, up to the time when the book was written.
Chapter six of this book has an in depth discussion of the probability of the ranking principal, and the probabilistic retrieval models, in general.
This lecture is about the web search.
In this lecture we are going to talk about one of the most important applications of text retrieval, web search engines.
So let's first look at some general challenges and opportunities in web search.
Now, many information retrieval algorithms had been developed at the, before the web was born.
So, when the web was born, it created the best opportunity to apply those algorithms to major application problem that everyone would care about.
So naturally, there had to be some further extensions of the classical search algorithms to address some new challenges encountered in web search.
So here are some general challenges.
Firstly, this is a scalability challenge.
How we handle the size of the web, and ensure completeness of coverage of all the information.
How to serve many users quickly, and by answering all their queries.
All right, so, that's one major challenge.
And before the web was born, the scale of search was relatively small.
The second problem is that there is low quality information.
And there are often spams.
The third challenge is dynamics of the web.
The new pages are constantly created and some pages may be updated, eve-, very quickly.
So it makes it harder to, keep the index fresh.
So these are some of the challenges that the, we have to solve in order to, build a high quality web search engine.
On the other hand, there are also some interesting opportunities that we can leverage to improve search results.
There are many additional heuristics.
For example you know using links that we can leverage to improve scoring.
Now the errors that we talked about such as the vector space model are general algorithms.
And they can be applied to any search applications, so that's, the advantage.
On the other hand, they also don't take advantage of special characteristics of pages, or documents, in the specific applications such as web search.
Web pages are linked with each other so obviously the linking is something that we can also leverage.
So because of these challenges and opportunities there are new techniques that have been developed for web search, or due to the need of a web search.
One is parallel indexing and searching, and this is to address the issue of scalability, in particular Google's imaging of MapReduce is very inferential, and has been very helpful in that aspect.
Second, there are techniques that are developed for, addressing the problem of spams.
So, spam detection.
We'll have to prevent those, spam pages from being ranked high.
And there are also techniques to achieve robust ranking.
And we're going to use a lot of signals to rank pages so that it's not easy to spam the search engine with particular tricks.
And the third line of techniques is link analysis.
And these are techniques that can allow us to to improve search results by leveraging extra information.
And in general in web search we're going to use multiple features for ranking.
Not just link analysis but also exploiting all kinds of crawls like the layout of web pages or anchor text that describes a link to another page.
So here's a picture showing the basic search engine technologies.
Basically, this is the web on the left and then user on the right side.
And we're going to help these, this user get access to the web information.
And the first component is the crawler that with the crawl pages and the second component is indexer.
That will take these pages create an invert index.
The third component that is a retrieval, not with the using, but the index to answer user's query, by talking to the user's browser.
And then, the search results would be, given to the user.
And, and then the browser will show those results and, to allow the user to interact with the web.
So we're going to talk about each of these component.
First we're going to talk about the crawler also called a spider or a software robot that would do something like a crawling pages on the web.
To build a toy crawler is relatively easy because you just need to start with a set of seed pages and then fetch pages from the web and parse these pages new links.
And then add them to the priority of q and then just explore those additional links, right.
But to build a real crawler actually is tricky and there are some complicated issues that we have do deal with.
For example robustness, what if the server doesn't respond.
What if there's a trap that generates dynamically generated webpages that might, attract your crawler to keep crawling the same site and to fetch dynamically generated pages.
The results of this issue of crawling and you don't want to overload one particular server with many crawling requests.
And you have to respect the, the robot exclusion protocol.
You also need to handle different types of files.
There are images, PDF files, all kinds of formats on the web.
And you have to also consider URL extensions.
So, sometimes those are cgi scripts, and, you know, internal references, etc., and sometimes, you have JavaScripts on the page that, they also create challenges.
And you ideally should also recognize [INAUDIBLE] the pages because you don't have to duplicate to the, those pages.
And finally, you may be interesting to discover hidden URLs.
Those are URLs that may not be linked, to any page.
But if you truncate the URL to, shorter pass, you might be able to get some additional pages.
So, what are the major crawling strategies?
In general, Breadth-First, is most common, because it naturally balance, balances server load.
You would not, keep probing a particular server [INAUDIBLE].
Also parallel crawling is very natural, because this task is very easy to parallelise and there are some variations of the crawling task.
One interesting variation is called focused crawling.
In this kind we're going to crawl just some pages about a particular topic.
For example, all pages about automobiles.
And, and, this is typically going to start with a query, and then you can use the query to get some results.
From the major search engine.
And then you can start it with those results and gradually crawl more.
So one challenge in crawling is to find the new pages that people have created, and people probably are creating new pages all the time, and this is very challenging if the new pages have not been actually linked to any old page.
If they are, then you can probably refine them by recrawling the older page.
So these are also some um,interesting challenges that have to be solved.
And finally we might face the scenario of incremental crawling or repeated crawling.
Right?
So your first, let's say if you want to be able to web search engine.
And you were the first to crawl a lot of data from the web.
And then, but then once you have collected all the data and in future we just need to crawl the, the update pages.
You, you, in general you don't have to re-crawl everything, right?
Or it's not necessary.
So, in this case you, you go as you minimize a resource overhead by using minimum resource to, to just still crawl updated pages.
So this is after a very interesting research question here.
And [INAUDIBLE] research question is that there aren't many standard algorithms [INAUDIBLE] for doing this, this task.
Right?
But in general, you can imagine, you can learn from the past experience.
Right.
So the two major factors that you have to consider are first, will this page be updated frequently?
And do I have to crawl this page again?
If the page is a static page that hasn't been changed for months you probably don't have to re-crawl it everyday, right?
Because it's unlikely that it will be changed frequently.
On the other hand if it's you know, sports score page that gets updated very frequently and you may need to re-crawl it maybe even multiple times, on the same day.
The other factor to consider is, is this page frequently accessed by users?
If it, if it is, that means it's a high utility page, and then thus it's more important to ensure such a page to be fresh.
Compare it with another page that has never been fetched by any users for a year.
Than, even though that page has been changed a lot, then, it's probably not necessary to crawl that page or at least it's not as urgent as, to maintain the freshness of frequently accessed page by users.
So to summarize, web search is one of the most important applications of text retrieval.
And there are some new challenges particularly scalability, efficiency, quality information.
There are also new opportunities particularly, rich link information and layout, et cetera.
Crawler is an essential component of web search applications.
And, in general, we can classify two scenarios.
Once is initial crawling and here we want to have complete crawling of the web if you are doing a general search engine or focus crawling if you want to just target it at a certain type of pages.
And then there is another scenario that's incremental updating of the crawl data or incremental crawling.
In this case you need to optimize the resource.
For to use minimum resource we get the [INAUDIBLE] .
This lecture is about query likelihood and probabilistic retrieval model.
In this lecture, we continue the discussion of probabilistic retrieval model.
In particular, we're going to talk about the query likelihood of the retrieval function.
In the query of likelihood retrieval model our idea is a model.
How a likely a user, who likes a document would pose a particular query.
So in this case, you can imagine, if a user likes this particular document about the presidential campaign news.
Then we can assume, the user would use this working as a basis to oppose a query to try and retrieve this doc.
So you can imagine the user, could use a process that works as follows, where we assume that the query is generated by sampling words from the document.
So for example, a user might pick a word like presidential from this document, and then use this as a query word.
And then the user would pick another word, like campaign and that would be the second query word.
Now this, of course, is assumption that we have made about, how a user would post a query.
Whether a user actually followed this process.
Maybe a different question.
But this assumption, has allowed us to formally characterize this conditional probability.
And this allows to also not rely on the big table that I showed you earlier to use imperative data to estimate this probability.
And this is why we can use this idea to then further derive retrieval function that we can implement with the languages.
So, as you see, the assumption that we've made here is, each query word, is independent in this sample, and also, each word is basically obtained from the document.
So now let's see how this works exactly.
Well, since we are computing a query likelihood, then the probability here is just the probability of this particular query, which is a sequence of words.
And we make the assumption that each word is generated independently.
So, as a result, the probability of the query is just a product of the probability of each query word.
Now, how do we compute the probability of each query word?
Well, based on the assumption, that a word is picked from the document, that the user has in mind.
Now we know the probability of each word is just the, the relative frequency of the word in the document.
So, for example the probability of presidential given the document, would be just the count of presidential in the document, divided by the total number of words in the document or document length.
So with this these assumptions, we now have actual simple formula for retrieval, right?
We can use this to rank our document.
So does this model work?
Let's take a look, here are some example documents that you have seen before.
Suppose now the query is presidential campaign.
And we see the formula here on the top.
So how do we score these documents?
Well it's very simple, right, we just count how many times we have seen presidential, how many times we have seen campaign etc.
And see here 44 and we've seen president Jou Tai, so that's two over the lands of document the four.
Multiply by 1 over lands of document of 4 for the probability of campaign and seeming we can probabilities for the other two documents.
Now if you'll look at this, these numbers or these, this, these formulas for scoring all these documents, it seems to make sense because, if we assume d3 and d4 have about the same length, then it looks like we will rank d4 above d3 and which is above d2, right?
And as we would expect, looks like it did capture the tf heuristic.
And so this seems to work well.
However, if we try a different query like this one, presidential campaign update, then we might see a problem.
But what problem?
Well, think about update, now none of these documents has mentioned update.
So according to our assumption that a user would pick a order from a document to generate a query, then the probability of obtaining a word like update would be what.
Would be zero, right?
So that cause a problem, because it would cause all these documents to have zero probability of generating this query.
Now, while it's fine to have a zero probability for d2 which is not relevant.
It's not okay to have zero for d3 and d4, because now we no longer can distinguish them.
What's worse, we can't even distinguish them from d 2.
All right, so that's obviously not desirable.
Now when one has such result, we should think about what has caused this problem.
So we have to examine what assumptions have been made, as we derive this ranking function.
Now if you examine those assumptions carefully you would realize.
What has caused this problem, right?
So, take a moment to think about, what do you think is the reason why update has zero probability, and how do we fix it?
Right?
So, if you think about this for the moment that you realize that.
That's because we have made an assumption that every query word must be drawn from the document in the user's mind.
So, in order to fix this, we have to assume that, the user could have drawn a word, not necessarily from the document.
So let's see improved model.
An improvement here is to say that, well, instead of drawing a word from the document, let's imagine that the user would actually draw a word from a document model and so I show a model here.
Here we assume that this document is generated, by using this unigram image model.
Now, this model, doesn't necessarily assign zero probability for update.
In fact we assume this model does not assign zero probability for any word.
Now if we're thinking this way then the generation process is a little bit different.
Now the user has this model in mind, instead of this particular document.
Although the model has to be estimated based on the document.
So the user can again generate the query using a similar process.
They may pick a word, for example presidential and another word campaign.
Now the difference is that, this time we can also pick a word like update, even though update it doesn't occur in the document to potentially generate a query word like update.
So that, a query was updated we want to have zero probabilities.
So this would fix our problem and it's also reasonable, because we're now thinking of what the user is looking for in a more general way, that is unique language model instead of a fixed document.
So how do we compute this query, like if we make this sum where it involves two steps, right?
The first is the computer's model, and we call it talking the language model here.
For example, I have shown two possible energy models here.
This has been based on two documents.
And then given a query and I get a mining algorithms.
The second step, is just to compute the likelihood of this query.
And by making independence assumptions, we could then have this probability as a product of the probability of each query word, all right?
But we do this for both documents.
And then we're going to score these two documents and then rank them.
So that's the basic idea of this query likelihood retrieval function.
So more generally than this ranking function would look like the following and here as, we assume that query has end words W1 through WN.
And then the scoring function, the ranking function is the probability that we observe this query, given that the user is thinking of this document.
And this assumed to be product of probabilities of all individual words and this is based on the independence assumption.
Now we actually often score the, document for this query by using log of the query likelihood, as shown on the sigma line.
Now we do this to avoid having a lot of small probabilities.
M, multiplied together.
And this could cause underflow and we might lose precision by transforming the value as a logarithm function.
We maintain the order of these documents, yet we can avoid the end of flow problem.
So if we take longer than transformation of coarse the product that would become a sum, as you stake in the line here.
So it's a sum of all of the query words, and inside the sum that is log of the probability of this word given by the document.
And then we can further rewrite the sum, into a different form.
So in the first of the sum here, in this sum, we have it over all the query words n query words.
And in this sum, we have a sum of all the possible words but we put a counter here of each word in the query.
Essentially we are only considering the words in the query, because if a word is not in the query, it can would be zero.
So we're still considering only these end words.
But we're using a different form as if we were going to a sum of all the words, in the vocabulary.
And of course a word might occur multiple times in the query.
That's wh, why we have a count here.
And then this part is log of the probability of the word given by the document MG model.
So you can see, in this material function, we actually know the count of the word in the query.
So, the only thing that we don't know is this document language model.
Therefore, we can convert through the retrieval problem into the problem of estimating this document language model.
So that we can compute, the probability of each query we're given by this document.
At different estimation methods here, would lead to different ranking functions.
And this is just like a different a ways to place a doc in the vector, in the vector space.
Would lead it to a different ranking function in the vector space model.
Here are different ways to estimate this stuff in the language model, will lead you to a different ranking function for query likelihood.
So now let's talk about the problem a little bit more, and specifically let's talk about the two different ways of estimating the parameters.
One is called the Maximum Likelihood estimate that I already just mentioned.
The other is Bayesian estimation.
So in maximum likelihood estimation, we define best as meaning the data likelihood has reached the maximum.
So formally it's given by this expression here, where we define the estimate as a arg max of the probability of x given theta.
So, arg max here just means its actually a function that will turn.
The argument that gives the function maximum value, adds the value.
So the value of arg max is not the value of this function.
But rather, the argument that has made it the function reaches maximum.
So in this case the value of arg max is theta.
It's the theta that makes the probability of X, given theta, reach it's maximum.
So this estimate that in due it also makes sense and it's often very useful, and it seeks the premise that best explains the data.
But it has a problem, when the data is too small because when the data points are too small, there are very few data points.
The sample is small, then if we trust data in entirely and try to fit the data and then we'll be biased.
So in the case of text data, let's say, all observed 100 words did not contain another word related to text mining.
Now, our maximum likelihood estimator will give that word a zero probability.
Because giving the non-zero probability would take away probability mass from some observer word.
Which obviously is not optimal in terms of maximizing the likelihood of the observer data.
But this zero probability for all the unseen words may not be reasonable sometimes.
Especially, if we want the distribution to characterize the topic of text mining.
So one way to address this problem is actually to use Bayesian estimation, where we actually would look at the both the data, and our prior knowledge about the parameters.
We assume that we have some prior belief about the parameters.
Now in this case of course, so we are not going to look at just the data, but also look at the prior.
So the prior here is defined by P of theta, and this means, we will impose some preference on certain theta's of others.
And by using Bayes Rule, that I have shown here, we can then combine the likelihood function.
With the prior to give us this posterior probability of the parameter.
Now, a full explanation of Bayes rule, and some of these things related to Bayesian reasoning, would be outside the scope of this course.
But I just gave a brief introduction because this is general knowledge that might be useful to you.
The Bayes Rule is basically defined here, and allows us to write down one conditional probability of X given Y in terms of the conditional probability of Y given X.
And you can see the two probabilities are different in the order of the two variables.
But often the rule is used for making inferences of the variable, so let's take a look at it again.
We can assume that p(X) Encodes our prior belief about X.
That means before we observe any other data, that's our belief about X, what we believe some X values have higher probability than others.
And this probability of X given Y is a conditional probability, and this is our posterior belief about X.
Because this is our belief about X values after we have observed the Y.
Given that we have observed the Y, now what do we believe about X?
Now, do we believe some values have higher probabilities than others?
Now the two probabilities are related through this one, this can be regarded as the probability of the observed evidence Y, given a particular X.
So you can think about X as our hypothesis, and we have some prior belief about which hypothesis to choose.
And after we have observed Y, we will update our belief and this updating formula is based on the combination of our prior.
And the likelihood of observing this Y if X is indeed true, so much for detour about Bayes Rule.
In our case, what we are interested in is inferring the theta values.
So, we have a prior here that includes our prior knowledge about the parameters.
And then we have the data likelihood here, that would tell us which parameter value can explain the data well.
The posterior probability combines both of them, so it represents a compromise of the the two preferences.
And in such a case, we can maximize this posterior probability.
To find this theta that would maximize this posterior probability, and this estimator is called a Maximum a Posteriori, or MAP estimate.
And this estimator is a more general estimator than the maximum likelihood estimator.
Because if we define our prior as a noninformative prior, meaning that it's uniform over all the theta values.
No preference, then we basically would go back to the maximum likelihood estimated.
Because in such a case, it's mainly going to be determined by this likelihood value, the same as here.
But if we have some not informative prior, some bias towards the different values then map estimator can allow us to incorporate that.
But the problem here of course, is how to define the prior.
There is no free lunch and if you want to solve the problem with more knowledge, we have to have that knowledge.
And that knowledge, ideally, should be reliable.
Otherwise, your estimate may not necessarily be more accurate than that maximum likelihood estimate.
So, now let's look at the Bayesian estimation in more detail.
So, I show the theta values as just a one dimension value and that's a simplification of course.
And so, we're interested in which variable of theta is optimal.
So now, first we have the Prior.
The Prior tells us that some of the variables are more likely the others would believe.
For example, these values are more likely than the values over here, or here, or other places.
So this is our Prior, and then we have our theta likelihood.
And in this case, the theta also tells us which values of theta are more likely.
And that just means loose syllables can best expand our theta.
And then when we combine the two we get the posterior distribution, and that's just a compromise of the two.
It would say that it's somewhere in-between.
So, we can now look at some interesting point that is made of.
This point represents the mode of prior, that means the most likely parameter value according to our prior, before we observe any data.
This point is the maximum likelihood estimator, it represents the theta that gives the theta of maximum probability.
Now this point is interesting, it's the posterior mode.
It's the most likely value of the theta given by the posterior of this.
And it represents a good compromise of the prior mode and the maximum likelihood estimate.
Now in general in Bayesian inference, we are interested in the distribution of all these parameter additives as you see here.
If there's a distribution over see how values that you can see.
Here, P of theta given X.
So the problem of Bayesian inference is to infer this posterior, this regime, and also to infer other interesting quantities that might depend on theta.
So, I show f of theta here as an interesting variable that we want to compute.
But in order to compute this value, we need to know the value of theta.
In Bayesian inference, we treat theta as an uncertain variable.
So we think about all the possible variables of theta.
Therefore, we can estimate the value of this function f as extracted value of f, according to the posterior distribution of theta, given the observed evidence X.
As a special case, we can assume f of theta is just equal to theta.
In this case, we get the expected value of the theta, that's basically the posterior mean.
That gives us also one point of theta, and it's sometimes the same as posterior mode, but it's not always the same.
So, it gives us another way to estimate the parameter.
So, this is a general illustration of Bayesian estimation and its an influence.
And later, you will see this can be useful for topic mining where we want to inject the sum prior knowledge about the topics.
So to summarize, we've used the language model which is basically probability distribution over text.
It's also called a generative model for text data.
The simplest language model is Unigram Language Model, it's basically a word distribution.
We introduced the concept of likelihood function, which is the probability of the a data given some model.
And this function is very important, given a particular set of parameter values this function can tell us which X, which data point has a higher likelihood, higher probability.
Given a data sample X, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum livelihood estimate.
We also talk about the Bayesian estimation or inference.
In this case we, must define a prior on the parameters p of theta.
And then we're interested in computing the posterior distribution of the parameters, which is proportional to the prior and the likelihood.
And this distribution would allow us then to infer any derive that is from theta.
This lecture is about the probabilistic retrieval model.
In this lecture, we're going to continue the discussion of text retrieval methods.
We're going to look at another kind of very different way to design ranking functions, then the Vector Space Model that we discussed before.
In probabilistic models we define the ranking function based on the probability that this document is random to this query.
In other words, we are, we introduced a binary random variable here.
This is the variable R here.
And we also assume that the query and the documents are all observations from random variables.
Note that in the vector space model, we assume they are vectors.
But here we assumed we assumed they are the data observed from random variables.
And so the problem, model retrieval becomes to estimate the probability of relevance.
In this category of models, there are different variants.
The classic probabilistic model has led to the BM25 retrieval function, which we discussed in the vector space model, because it's form is actually similar to a vector space model.
In this lecture, we're going to discuss another subclass in this big class called a language modeling approaches to retrieval.
In particular, we're going to discuss the Query Likelihood retrieval model, which is one of the most effective models in probabilistic models.
There is also another line called a divergence-from-randomness model, which has latitude the PL2 function.
It's also one of the most effective state of the art attribute functions.
In query likelihood, our assumption is that this probability readiness can be approximated by the probability of query given a document and readiness.
So, intuitively, this probability just captures the following probability.
And that is if a user likes document d, how likely would the user enter query q in order to retrieve document d. So we'll assume that the user likes d, because we have a relevance value here.
And the we ask the question about how likely we will see this particular query from this user?
So this is the basic idea.
Now to understand this idea, let's take a look at the general idea or the basic idea of probabilistic retrieval models.
So here, I listed some imagined relevance status values or relevance judgments of queries and documents.
For example, in this slide, it shows that query one is a query that the user typed in and d1 is a document the user has seen and one means the user thinks d1 is relevant to to q1.
So this R here can be also approximated by the clicks little data that the search engine can collect it by watching how you interact with the search results.
So, in this case, let's say, the user clicked on this document, so there's a one here.
Similarly, the user clicked on d2 also, so there's a one here.
In other words, d2 is assumed to relevant at two, q1.
On the other hand, d3 is non relevant, there's a zero here.
And d4 is non-relevant and then d 5 is again relevant and so on and so forth.
And this part of maybe, they are collected from a different user.
Right.
So this user typed in q1 and then found that d1 is actually not useful, so d1 is actually non-relevant.
In contrast here we see it's relevant and, or this could be the same query typing by the same user at different times, but d2 is also relevant, et cetera.
And then here, we can see more data that about other queries.
Now we can imagine, we have a lot of search data.
Now we can ask the question, how can we then estimated the probability of relevance?
Right.
So how can we compute this probability of relevance?
Well, intuitively, that just means if we look at the, all the entries where we see this particular d and this particular q, how likely will we see a one on the third column?
Basically, that just means we can correct the counts.
We can first count how many times where we see q and d as a pair in this table and then count how many times we actually have also seen one in the third column and then we just compute the ratio.
So let's take a look at some specific examples.
Suppose we are trying to computed this probability for d1, d2 and d3 for q1.
What is the estimated probability?
Now think about that.
You can pause the video if needed.
Try to take a look at the table and try to give your estimate of the probability.
Have you seen that if we are interested in q1 and d1, we've been looking at the, these two pairs and in both cases or actually in one of the cases, the user has said that this is one, this is relevant.
So R is equal to 1 in only one of the two cases.
In the other case, this is zero.
So that's one out of two.
What about the d1 and the d2?
Well, they're are here, you want d2, d1, d2.
In both cases, in this case R is equal to 1.
So, it's two out of two and so and so forth.
So you can see with this approach, we captured it score these documents for the query.
Right?
We now have a score for d1, d2 and d3 for this query.
We can simply ranked them based on these probabilities and so that's the basic idea of probabilistic retrieval model.
And you can see, it makes a lot of sense.
In this case, it's going to rank d2 above all the other documents.
Because in all the cases, when you have seen q1 and d2, R is equal to 1.
The user clicked on this document.
So this also showed showed that with a lot of click through data, a search engine can learn a lot from the data to improve the search engine.
This is a simple example that shows that with even a small number of entries here, we can already estimate some probabilities.
These probabilities would give us some sense about which document might be more read or more useful to a user for typing this query.
Now, of course, the problem is that we don't observe all the queries and all of the documents and all the relevance values.
Right?
There will be a lot of unseen documents.
In general, we can only collect data from the document's that we have shown to the users.
There are even more unseen queries, because you cannot predict what queries will be typed in by users.
So, obviously, this approach won't work if we apply it to unseen queries or unseen documents.
Nevertheless, this shows the basic idea of the probabilistic retrieval model and it makes sense intuitively.
So what do we do in such a case when we have a lot of unseen documents and, and unseen queries?
Well, the solutions that we have to approximate in some way.
Right.
So, in this particular case called the Query LIkelihood Retrieval Model, we just approximate this by another conditional probability, p q | d, R is equal to 1.
So, in the condition part, we assume that the user likes the document, because we have seen that the user clicked on this document.
And this part, shows that we're interested in how likely the user would actually enter this query.
How likely we will see this query in the same row.
So note that here, we have made an interesting assumption here.
Basically, we, we're going to assume that whether the user types in this query has something to do with whether user likes the document.
In other words, we actually make the foreign assumption and that is a user formula to query based on an imaginary relevant document.
Well, if you just look at this as a conditional probability, it's not obvious we are making this assumption.
So what I really meant is that to use this new conditional probability to help us score then this new condition of probability.
We have to somehow be able to estimate this conditional probability without relying on this big table.
Otherwise, it would be having similar problems as before.
And by making this assumption, we have some way to bypass this big table and try to just mortar how to use a formula to the query.
Okay.
So this is how you can simplify the, the general model so that we can give either specific function later.
So let's look at how this model works for our example.
And basically, what we are going to do in this case is to ask the following question.
Which of these documents is most likely the imaginary relevant document in the user's mind when the user formulates this query?
And so we ask this question and we quantify the probability and this probability is a conditional probability of observing this query if a particular document is in fact the imaginary relevant document in the user's mind.
Here you can see we compute all these query likelihood probabilities, the likelihood of queries given each document.
Once we have these values, we can then rank these documents based on these values.
So to summarize, the general idea of modern relevance in the probability risk model is to assume that we introduce a binary random variable, R here.
And then let the scoring function be defined based on this conditional probability.
We also talked about a proximate in this by using the query likelihood.
And in this case, we have a ranking function that's basically based on a probability of a query given the document.
And this probability should be interpreted as the probability that a user who likes document d would pose query q.
Now the question, of course is how do we compute this additional probability?
At this in general has to do with how to compute the probability of text, because q is a text.
And this has to do with a model called a Language Model.
And this kind of models are proposed to model text.
So most specifically, we will be very interested in the following conditional probability as I show you, you this here.
If the user like this document, how likely the user would approve this query?
And in the next lecture, we're going to give introduction to Language Model, so that we can see how we can model text with a probability risk model in general.
So this is indeed a general idea of the Expectation-Maximization, or EM, Algorithm.
So in all the EM algorithms we introduce a hidden variable to help us solve the problem more easily.
In our case the hidden variable is a binary variable for each occurrence of a word.
And this binary variable would indicate whether the word has been generated from 0 sub d or 0 sub p. And here we show some possible values of these variables.
For example, for the it's from background, the z value is one.
And text on the other hand.
Is from the topic then it's zero for z, etc.
Now, of course, we don't observe these z values, we just imagine they're all such.
Values of z attaching to other words.
And that's why we call these hidden variables.
Now, the idea that we talked about before for predicting the word distribution that has been used when we generate the word is it a predictor, the value of this hidden variable?
And, so, the EM algorithm then, would work as follows.
First, we'll initialize all the parameters with random values.
In our case, the parameters are mainly the probability.
of a word, given by theta sub d. So this is an initial addition stage.
These initialized values would allow us to use base roll to take a guess of these z values, so we'd guess these values.
We can't say for sure whether textt is from background or not.
But we can have our guess.
This is given by this formula.
It's called an E-step.
And so the algorithm would then try to use the E-step to guess these z values.
After that, it would then invoke another that's called M-step.
In this step we simply take advantage of the inferred z values and then just group words that are in the same distribution like these from that ground including this as well.
We can then normalize the count to estimate the probabilities or to revise our estimate of the parameters.
So let me also illustrate that we can group the words that are believed to have come from zero sub d, and that's text, mining algorithm, for example, and clustering.
And we group them together to help us re-estimate the parameters that we're interested in.
So these will help us estimate these parameters.
Note that before we just set these parameter values randomly.
But with this guess, we will have somewhat improved estimate of this.
Of course, we don't know exactly whether it's zero or one.
So we're not going to really do the split in a hard way.
But rather we're going to do a softer split.
And this is what happened here.
So we're going to adjust the count by the probability that would believe this word has been generated by using the theta sub d. And you can see this, where does this come from?
Well, this has come from here, right?
From the E-step.
So the EM Algorithm would iteratively improve uur initial estimate of parameters by using E-step first and then M-step.
The E-step is to augment the data with additional information, like z.
And the M-step is to take advantage of the additional information to separate the data.
To split the data accounts and then collect the right data accounts to re-estimate our parameter.
And then once we have a new generation of parameter, we're going to repeat this.
We are going the E-step again.
To improve our estimate of the hidden variables.
And then that would lead to another generation of re-estimated parameters.
For the word distribution that we are interested in.
Okay, so, as I said, the bridge between the two is really the variable z, hidden variable, which indicates how likely this water is from the top water distribution, theta sub p. So, this slide has a lot of content and you may need to.
Pause the reader to digest it.
But this basically captures the essence of EM Algorithm.
Start with initial values that are often random themself.
And then we invoke E-step followed by M-step to get an improved setting of parameters.
And then we repeated this, so this a Hill-Climbing algorithm that would gradually improve the estimate of parameters.
As I will explain later there is some guarantee for reaching a local maximum of the log-likelihood function.
So lets take a look at the computation for a specific case, so these formulas are the EM.
Formulas that you see before, and you can also see there are superscripts, here, like here, n, to indicate the generation of parameters.
Like here for example we have n plus one.
That means we have improved.
From here to here we have an improvement.
So in this setting we have assumed the two numerals have equal probabilities and the background model is null.
So what are the relevance of the statistics?
Well these are the word counts.
So assume we have just four words, and their counts are like this.
And this is our background model that assigns high probabilities to common words like the.
And in the first iteration, you can picture what will happen.
Well first we initialize all the values.
So here, this probability that we're interested in is normalized into a uniform distribution of all the words.
And then the E-step would give us a guess of the distribution that has been used.
That will generate each word.
We can see we have different probabilities for different words.
Why?
Well, that's because these words have different probabilities in the background.
So even though the two distributions are equally likely.
And then our initial audition say uniform distribution because of the difference in the background of the distribution, we have different guess the probability.
So these words are believed to be more likely from the topic.
These on the other hand are less likely.
Probably from background.
So once we have these z values, we know in the M-step these probabilities will be used to adjust the counts.
So four must be multiplied by this 0.33 in order to get the allocated accounts toward the topic.
And this is done by this multiplication.
Note that if our guess says this is 100% If this is one point zero, then we just get the full count of this word for this topic.
In general it's not going to be one point zero.
So we're just going to get some percentage of this counts toward this topic.
Then we simply normalize these counts to have a new generation of parameters estimate.
So you can see, compare this with the older one, which is here.
So compare this with this one and we'll see the probability is different.
Not only that, we also see some words that are believed to have come from the topic will have a higher probability.
Like this one, text.
And of course, this new generation of parameters would allow us to further adjust the inferred latent variable or hidden variable values.
So we have a new generation of values, because of the E-step based on the new generation of parameters.
And these new inferred values of Zs will give us then another generation of the estimate of probabilities of the word.
And so on and so forth so this is what would actually happen when we compute these probabilities using the EM Algorithm.
As you can see in the last row where we show the log-likelihood, and the likelihood is increasing as we do the iteration.
And note that these log-likelihood is negative because the probability is between 0 and 1 when you take a logarithm, it becomes a negative value.
Now what's also interesting is, you'll note the last column.
And these are the inverted word split.
And these are the probabilities that a word is believed to have come from one distribution, in this case the topical distribution, all right.
And you might wonder whether this would be also useful.
Because our main goal is to estimate these word distributions.
So this is our primary goal.
We hope to have a more discriminative order of distribution.
But the last column is also bi-product.
This also can actually be very useful.
You can think about that.
We want to use, is to for example is to estimate to what extent this document has covered background words.
And this, when we add this up or take the average we will kind of know to what extent it has covered background versus content was that are not explained well by the background.
This lecture is a continuing discussion of Generative Probabilistic Models for text clustering.
In this lecture, we are going to continue talking about the text clustering, particularly, the Generative Probabilistic Models.
So this is a slide that you have seen earlier where we have written down the likelihood function for a document with two distributions, being a two component mixed model for document clustering.
Now in this lecture, we're going to generalize this to include the k clusters.
Now if you look at the formula and think about the question, how to generalize it, you'll realize that all we need is to add more terms, like what you have seen here.
So you can just add more thetas and the probabilities of thetas and the probabilities of generating d from those thetas.
So this is precisely what we are going to use and this is the general presentation of the mixture model for document clustering.
So as more cases would follow these steps in using a generating model first, think about our data.
And so in this case our data is a collection of documents, end documents denoted by d sub i, and then we talk about the other models, think of other modelling.
In this case, we design a mixture of k unigram language models.
It's a little bit different from the topic model, but we have similar parameters.
We have a set of theta i's that denote that our distributions corresponding to the k unigram language models.
We have p of each theta i as a probability of selecting each of the k distributions we generate the document.
Now note that although our goal is to find the clusters and we actually have used a more general notion of a probability of each cluster and this as you will see later, will allow us to assign a document to the cluster that has the highest probability of being able to generate the document.
So as a result, we can also recover some other interesting properties, as you will see later.
So the model basically would make the following assumption about the generation of a document.
We first choose a theta i according to probability of theta i, and then generate all the words in the document using this distribution.
Note that it's important that we use this distribution all the words in the document.
This is very different from topic model.
So the likelihood function would be like what you are seeing here.
So you can take a look at the formula here, we have used the different notation here in the second line of this equation.
You are going to see now the notation has been changed to use unique word in the vocabulary, in the product instead of particular position in the document.
So from X subject to W, is a change of notation and this change allows us to show the estimation formulas more easily.
And you have seen this change also in the topic model presentation, but it's basically still just a product of the probabilities of all the words.
And so with the likelihood function, now we can talk about how to do parameter estimation.
Here we can simply use the maximum likelihood estimator.
So that's just a standard way of doing things.
So all should be familiar to you now.
It's just a different model.
So after we have estimated parameters, how can we then allocate clusters to the documents?
Well, let's take a look at the this situation more closely.
So we just repeated the parameters here for this mixture model.
Now if you think about what we can get by estimating such a model, we can actually get more information than what we need for doing clustering, right?
So theta i for example, represents the content of cluster i, this is actually a by-product, it can help us summarize what the cluster is about.
If you look at the top terms in this cluster or in this word distribution and they will tell us what the cluster is about.
p of theta i can be interpreted as indicating the size of cluster because it tells us how likely the cluster would be used to generate the document.
The more likely a cluster is used to generate a document, we can assume the larger the cluster size is.
Note that unlike in PLSA and this probability of theta i is not dependent on d. Now you may recall that the topic you chose at each document actually depends on d. That means each document can have a potentially different choice of topics, but here we have a generic choice probability for all the documents.
But of course, even a particular document that we still have to infer which topic is more likely to generate the document.
So in that sense, we can still have a document dependent probability of clusters.
So now let's look at the key problem of assigning documents to clusters or assigning clusters to documents.
So that's the computer c sub d here and this will take one of the values in the range of one through k to indicate which cluster should be assigned to d. Now first you might think about a way to use likelihood on it and that is to assign d to the cluster corresponding to the topic of theta i, that most likely has been used to generate d. So that means we're going to choose one of those distributions that gives d the highest probability.
In other words, we see which distribution has the content that matches our d at the [INAUDIBLE].
Intuitively that makes sense, however, this approach does not consider the size of clusters, which is also a available to us and so a better way is to use the likelihood together with the prior, in this case the prior is p of theta i.
And together, that is, we're going to use the base formula to compute the posterior probability of theta, given d. And if we choose theta .based on this posterior probability, we would have the following formula that you see here on the bottom of this slide.
And in this case, we're going to choose the theta that has a large P of theta i, that means a large cluster and also a high probability of generating d. So we're going to favor a cluster that's large and also consistent with the document.
And that intuitively makes sense because the chance of a document being a large cluster is generally higher than in a small cluster.
So this means once we can estimate the parameters of the model, then we can easily solve the problem of document clustering.
So next, we'll have to discuss how to actually compute the estimate of the model.
This lecture is about a specific technique for Contextual Text Mining called Contextual Probabilistic Latent Semantic Analysis.
In this lecture, we're going to continue discussing Contextual Text Mining.
And we're going to introduce Contextual Probablitistic Latent Semantic Analysis as exchanging of POS for doing contextual text mining.
Recall that in contextual text mining we hope to analyze topics in text, in consideration of the context so that we can associate the topics with a property of the context were interesting.
So in this approach, contextual probabilistic latent semantic analysis, or CPLSA, the main idea is to express to the add interesting context variables into a generating model.
Recall that before when we generate the text we generally assume we'll start wIth some topics, and then assemble words from some topics.
But here, we're going to add context variables, so that the coverage of topics, and also the content of topics would be tied in context.
Or in other words, we're going to let the context Influence both coverage and the content of a topic.
The consequences that this will enable us to discover contextualized topics.
Make the topics more interesting, more meaningful.
Because we can then have topics that can be interpreted as specifically to a particular context that we are interested in.
For example, a particular time period.
As an extension of PLSA model, CPLSA does the following changes.
Firstly it would model the conditional likelihood of text given context.
That clearly suggests that the generation of text would then depend on context, and that allows us to bring context into the generative model.
Secondly, it makes two specific assumptions about the dependency of topics on context.
One is to assume that depending on the context, depending on different time periods or different locations, we assume that there are different views of a topic or different versions of word descriptions that characterize a topic.
And this assumption allows us to discover different variations of the same topic in different contexts.
The other is that we assume the topic coverage also depends on the context.
That means depending on the time or location, we might cover topics differently.
Again, this dependency would then allow us to capture the association of topics with specific contexts.
We can still use the EM algorithm to solve the problem of parameter estimation.
And in this case, the estimated parameters would naturally contain context variables.
And in particular, a lot of conditional probabilities of topics given certain context.
And this is what allows you to do contextual text mining.
So this is the basic idea.
Now, we don't have time to introduce this model in detail, but there are references here that you can look into to know more detail.
Here I just want to explain the high level ideas in more detail.
Particularly I want to explain the generation process.
Of text data that has context associated in such a model.
So as you see here, we can assume there are still multiple topics.
For example, some topics might represent a themes like a government response, donation Or the city of New Orleans.
Now this example is in the context of Hurricane Katrina and that hit New Orleans.
Now as you can see we assume there are different views associated with each of the topics.
And these are shown as View 1, View 2, View 3.
Each view is a different version of word distributions.
And these views are tied to some context variables.
For example, tied to the location Texas, or the time July 2005, or the occupation of the author being a sociologist.
Now, on the right side, now we assume the document has context information.
So the time is known to be July 2005.
The location is Texas, etc.
And such context information is what we hope to model as well.
So we're not going to just model the text.
And so one idea here is to model the variations of top content and various content.
And this gives us different views of the water distributions.
Now on the bottom you will see the theme coverage of top Coverage might also vary according to these context because in the case of a location like Texas, people might want to cover the red topics more.
That's New Orleans.
That's visualized here.
But in a certain time period, maybe Particular topic and will be covered more.
So this variation is also considered in CPLSA.
So to generate the searcher document With context, with first also choose a view.
And this view of course now could be from any of these contexts.
Let's say, we have taken this view that depends on the time.
In the middle.
So now, we will have a specific version of word distributions.
Now, you can see some probabilities of words for each topic.
Now, once we have chosen a view, now the situation will be very similar to what happened in standard ((PRSA)) We assume we have got word distribution associated with each topic, right?
And then next, we will also choose a coverage from the bottom, so we're going to choose a particular coverage, and that coverage, before is fixed in PLSA, and assigned to a particular document.
Each document has just one coverage distribution.
Now here, because we consider context, so the distribution of topics or the coverage of Topics can vary depending on the context that has influenced the coverage.
So, for example, we might pick a particular coverage.
Let's say in this case we picked a document specific coverage.
Now with the coverage and these word distributions we can generate a document in exactly the same way as in PLSA.
So what it means, we're going to use the coverage to choose a topic, to choose one of these three topics.
Let's say we have picked the yellow topic.
Then we'll draw a word from this particular topic on the top.
Okay, so we might get a word like government.
And then next time we might choose a different topic, and we'll get donate, etc.
Until we generate all the words.
And this is basically the same process as in PLSA.
So the main difference is when we obtain the coverage.
And the word distribution, we let the context influence our choice So in other words we have extra switches that are tied to these contacts that will control the choices of different views of topics and the choices of coverage.
And naturally the model we have more parameters to estimate.
But once we can estimate those parameters that involve the context, then we will be able to understand the context specific views of topics, or context specific coverages of topics.
And this is precisely what we want in contextual text mining.
So here are some simple results.
From using such a model.
Not necessary exactly the same model, but similar models.
So on this slide you see some sample results of comparing news articles about Iraq War and Afghanistan War.
Now we have about 30 articles on Iraq wa,r and 26 articles on Afghanistan war.
And in this case, the goal is to review the common topic.
It's covered in both sets of articles and the differences of variations of the topic in each of the two collections.
So in this case the context is explicitly specified by the topic or collection.
And we see the results here show that there is a common theme that's corresponding to Cluster 1 here in this column.
And there is a common theme indicting that United Nations is involved in both Wars.
It's a common topic covered in both sets of articles.
And that's indicated by the high probability words shown here, united and nations.
Now if you know the background, of course this is not surprising and this topic is indeed very relevant to both wars.
If you look at the column further and then what's interesting's that the next two cells of word distributions actually tell us collection specific variations of the topic of United Nations.
So it indicates that the Iraq War, United Nations was more involved in weapons factions, whereas in the Afghanistan War it was more involved in maybe aid to Northern Alliance.
It's a different variation of the topic of United Nations.
So this shows that by bringing the context.
In this case different the walls or different the collection of texts.
We can have topical variations tied to these contexts, to review the differences of coverage of the United Nations in the two wars.
Now similarly if you look at the second cluster Class two, it has to do with the killing of people, and, again, it's not surprising if you know the background about wars.
All the wars involve killing of people, but imagine if you are not familiar with the text collections.
We have a lot of text articles, and such a technique can reveal the common topics covered in both sets of articles.
It can be used to review common topics in multiple sets of articles as well.
If you look at of course in that column of cluster two, you see variations of killing of people and that corresponds to different contexts And here is another example of results obtained from blog articles about Hurricane Katrina.
In this case, what you see here is visualization of the trends of topics over time.
And the top one shows just the temporal trends of two topics.
One is oil price, and one is about the flooding of the city of New Orleans.
Now these topics are obtained from blog articles about Hurricane Katrina.
And people talk about these topics.
And end up teaching to some other topics.
But the visualisation shows that with this technique, we can have conditional distribution of time.
Given a topic.
So this allows us to plot this conditional probability the curve is like what you're seeing here.
We see that, initially, the two curves tracked each other very well.
But later we see the topic of New Orleans was mentioned again but oil price was not.
And this turns out to be the time period when another hurricane, hurricane Rita hit the region.
And that apparently triggered more discussion about the flooding of the city.
The bottom curve shows the coverage of this topic about flooding of the city by block articles in different locations.
And it also shows some shift of coverage that might be related to people's migrating from the state of Louisiana to Texas for example.
So in this case we can see the time can be used as context to review trends of topics.
These are some additional results on spacial patterns.
In this case it was about the topic of government response.
And there was some criticism about the slow response of government in the case of Hurricane Katrina.
And the discussion now is covered in different locations.
And these visualizations show the coverage in different weeks of the event.
And initially it's covered mostly in the victim states, in the South, but then gradually spread into other locations.
But in week four, which is shown on the bottom left, we see a pattern that's very similar to the first week on the top left.
And that's when again Hurricane Rita hit in the region.
So such a technique would allow us to use location as context to examine their issues of topics.
And of course the moral is completely general so you can apply this to any other connections of text.
To review spatial temporal patterns.
His view found another application of this kind of model, where we look at the use of the model for event impact analysis.
So here we're looking at the research articles information retrieval.
IR, particularly SIGIR papers.
And the topic we are focusing on is about the retrieval models.
And you can see the top words with high probability about this model on the left.
And then we hope to examine the impact of two events.
One is a start of TREC, for Text and Retrieval Conference.
This is a major evaluation sponsored by U.S. government, and was launched in 1992 or around that time.
And that is known to have made a impact on the topics of research information retrieval.
The other is the publication of a seminal paper, by Croft and Porte.
This is about a language model approach to information retrieval.
It's also known to have made a high impact on information retrieval research.
So we hope to use this kind of model to understand impact.
The idea here is simply to use the time as context.
And use these events to divide the time periods into a period before.
For the event and another after this event.
And then we can compare the differences of the topics.
The and the variations, etc.
So in this case, the results show before track the study of retrieval models was mostly a vector space model, Boolean model etc.
But the after Trec, apparently the study of retrieval models have involved a lot of other words.
That seems to suggest some different retrieval tasks, so for example, email was used in the enterprise search tasks and subtopical retrieval was another task later introduced by Trec.
On the bottom, we see the variations that are correlated with the propagation of the language model paper.
Before, we have those classic probability risk model, logic model, Boolean etc., but after 1998, we see clear dominance of language model as probabilistic models.
And we see words like language model, estimation of parameters, etc.
So this technique here can use events as context to understand the impact of event.
Again the technique is generals so you can use this to analyze the impact of any event.
Here are some suggested readings.
The first is paper about simple staging of psi to label cross-collection comparison.
It's to perform comparative text mining to allow us to extract common topics shared by multiple collections.
And there are variations in each collection.
The second one is the main paper about the CPLSA model.
Was a discussion of a lot of applications.
The third one has a lot of details about the special temporal patterns for the Hurricane Katrina example.
.
This lecture is about a statistical language model.
In this lecture, we're go, we're going to get an introduction to the probabilistic model.
This has to do with how many models have to go into these models.
So, it's ready to how we model theory based on a document.
We're going to talk about, what is a language model and, then, we're going to talk about the simplest language model called a unigram language model.
Which also happens to be the most useful model for text retrieval.
And finally we'll discuss possible uses of an m model.
What is a language model?
Well, it's just a probability distribution over word sequences.
So, here I show one.
This model gives the sequence today's Wednesday a probability of 0.001 it gives today Wednesday is a very very small probability, because it's algorithmatical.
You can see the probabilities given to these sentences or sequences of words can vary a lot depending on the model.
Therefore, it's clearly context-dependent.
In ordinary conversation, probably today is Wednesday is most popular among these sentences.
But imagine in the context of discussing a private math, maybe the higher values positive would have a higher probability.
This means it can be used to represent as a topic of a test.
The model can also be regarded as a probabilistic mechanism for generating text, and this is why it is often called a generating model.
So, what does that mean?
We can image this is a mechanism that's visualized here as a [INAUDIBLE] system that can generate a sequences of words.
So we can ask for a sequence and it's to sample a sequence from the device if you want.
And it might generate, for example, today is Wednesday, but it could have generated many other sequences.
So for example, there are many possibilities, right?
So this, in this sense, we can view our data as basically a sample observed from such a generated model.
So why is such a model useful?
Well, it's mainly because it can quantify the uncertainties in natural language.
Where do uncertainties come from?
Well, one source is simply the ambiguity in natural language that we discussed earlier in the lecture.
Another source is because we don't have complete understanding.
We lack all the knowledge to understand language.
In that case there will be uncertainties as well.
So let me show some examples of questions that we can answer with an average model that would have an interesting application in different ways.
Given that we see John and feels.
How likely will we see happy as opposed to habit as the next word in a sequence of words?
Obviously this would be very useful speech recognition because happy and habit would have similar acoustical sound.
Acoustic signals.
But if we look at the language model we know that John feels happy would be far more likely than John feels habit.
Another example, given that we observe baseball three times and gained once in the news article how likely is it about the sports?
This obviously is related to text categorization and information.
Also, given that a user is interested in sports news, how likely would the user use baseball in a query?
Now this is clearly related to the query that we discussed in the previous lecture.
So now let's look at the simplest language model.
Called a lan, unigram language model.
In such a case, we assume that we generate the text by generating each word independently.
So this means the probability of a sequence of words would be then the product of the probability of each word.
Now normally they are not independent, right?
So if you have seen a word like language.
Now, we'll make it far more likely to observe model than if you haven't seen language.
So this assumption is not necessary sure but we'll make this assumption to simplify the model.
So now, the model has precisely n parameters, where n is vocabulary size.
We have one probability for each word, and all these probabilities must sum to 1.
So strictly speaking, we actually have N minus 1 parameters.
As I said, text can be then be assumed to be a sample drawn from this word distribution.
So for example, now we can ask the device, or the model, to stochastically generate the words for us instead of in sequences.
So instead of giving a whole sequence like today is Wednesday, it now gives us just one word.
And we can get all kinds of words.
And we can assemble these words in a sequence.
So, that would still allows you to compute the probability of today is Wed as the product of the three probabilities.
As you can see even though we have not asked the model to generate the, the sequences it actually allows us to compute the probability for all the sequences.
But this model now only needs N parameters to characterize.
That means if we specify all the probabilities for all the words then the model's behavior is completely specified.
Whereas if you, we don't make this assumption we would have to specify.
Find probabilities for all kinds of combinations of words in sequences.
So by making this assumption, it makes it much easier to estimate these parameters.
So let's see a specific example here.
Here I show two unigram lambda models with some probabilities and these are high probability words that are shown on top.
The first one clearly suggests the topic of text mining because the high probability words are all related to this topic.
The second one is more related to health.
Now, we can then ask the question how likely we'll observe a particular text from each of these three models.
Now suppose with sample words to form the document, let's say we take the first distribution which are the sample words.
What words do you think it would be generated or maybe text?
Or maybe mining maybe another word?
Even food, which has a very small probability, might still be able to show up.
But in general, high probability words will likely show up more often.
So we can imagine a generated text that looks like text mining.
A factor with a small probability, you might be able to actually generate the actual text mining paper that would actually be meaningful, although the probability would be very, very small.
In the extreme case, you might imagine we might be able to generate a, a text paper, text mining paper that would be accepted by a major conference.
And in that case the probability would be even smaller.
For instance nonzero probability, if we assume none of the words will have a nonzero probability.
Similarly from the second topic, we can imagine we can generate a food and nutrition paper.
That doesn't mean we cannot generate this paper from text mining distribution.
We can, but the probability would be very, very small, maybe smaller than even generating a paper that can be accepted by a major conference on text mining.
So the point of here is that given a distribution, we can talk about the probability of observing a certain kind of text.
Some text would have higher probabilities than others.
Now, let's look at the problem in a different way.
Supposedly, we now have available a particular document.
In this case, maybe the abstract or the text mining paper, and we see these word accounts here.
The total number of words is 100.
Now the question you ask here is a estimation question.
We can ask the question, which model, which word distribution has been used to, to generate this text.
Assuming the text has been generated by assembling words from the distribution.
So what would be your guess?
What have to decide what probabilities test, mining, et cetera would have.
So pause a view for a second and try to think about your best guess.
If you're like a lot of people you would have guessed that well, my best guess is text has a probability of 10 out of 100 because I have seen text ten times and there are a total of 100 words.
So we simply noticed, normalize these counts.
And that's in fact [INAUDIBLE] justified.
And your intuition is consistent with mathematical derivation.
And this is called a maximum likelihood [INAUDIBLE].
In this estimator, we'll assume that the parameter settings,.
Are those that would give our observer the maximum probability.
That means if we change these probabilities, then the probability of observing the particular text would be somewhat smaller.
So we can see this has a very simple formula.
Basically, we just need to look at the count of a word in the document and then divide it by the total number of words in the document.
About the length.
Normalize the frequency.
Well a consequence of this, is of course, we're going to assign If we have an observed word, there will be no incentive to assign a non-0 probability using this approach.
Why?
Because that would take away probability mass for this observed words.
And that obviously wouldn't maximize the probability of this particular observed [INAUDIBLE] data.
But one can still question whether this is our best estimator.
Well, the answer depends on what kind of model you want to find, right?
This is made if it's a best model based on this particular layer.
But if you're interested in a model that can explain the content of the four paper of, for this abstract, then you might have a second thought, right?
So for one thing there should be other things in the body of that article.
So they should not have, zero probabilities, even though they are not observing the abstract.
We're going to cover this later, in, discussing the query model.
So, let's take a look at some possible uses of these language models.
One use is simply to use it to represent the topics.
So here it shows some general English background that text.
We can use this text to estimate a language model.
And the model might look like this.
Right?
So on the top we'll have those all common words, is we, is, and then we'll see some common words like these, and then some very, very real words in the bottom.
This is the background image model.
It represents the frequency on words, in English in general, right?
This is the background model.
Now, let's look at another text.
Maybe this time, we'll look at Computer Science research papers.
So we have a correction of computer science research papers, we do again, we can just use the maximum where we simply normalize the frequencies.
Now, in this case, we look at the distribution, that looks like this.
On the top, it looks similar, because these words occur everywhere, they are very common.
But as we go down we'll see words that are more related to computer science.
Computer, or software, or text et cetera.
So, although here, we might also see these words, for example, computer.
But, we can imagine the probability here is much smaller than the probability here.
And we will see many other words here that, that would be more common in general in English.
So, you can see this distribution characterizes a topic of the corresponding text.
We can look at the, even the smaller text.
So, in this case let's look at the text mining paper.
Now if we do the same we have another.
Distribution again the can be expected to occur on the top.
Soon we will see text, mining, association, clustering, these words have relatively high probabilities in contrast in this distribution has relatively small probability.
So this means, again based on different text data that we can have a different model.
And model captures the topic.
So we call this document an LM model and we call this collection LM model.
And later, we'll see how they're used in a retrieval function.
But now, let's look at the, another use of this model.
Can we statistically find what words are semantically related to computer?
Now how do we find such words?
Well our first thought is well let's take a look at the text that match.
Computer.
So we can take a look at all the documents that contain the word computer.
Let's build a language model.
Okay, see what words we see there.
Well, not surprisingly, we see these common words on top as we always do.
So in this case, this language model gives us the.
Conditional probability of seeing a word in the context of computer.
And these common words will naturally have high probabilities.
Other words will see computer itself, and software will have relatively high probabilities.
But we, if we just use this model we cannot.
I just say all these words are semantically related to computer.
So intuitively what we'd like to get rid of these these common words.
How can we do that?
It turns out that it's possible to use language model to do that.
Now I suggest you think about that.
So how can we know what words are very common so that we want to kind of get rid of them.
What model will tell us that?
Well, maybe you can think about that.
So the background language model precisely tells us this information.
It tells us what words are common in general.
So if we use this background model, we would know that these words are common words in general.
So it's not surprising to observe them in the context of computer.
Whereas computer has a very small probability in general.
So it's very surprising that we have seen computer in, with this probability.
And the same is true for software.
So then we can use these two models to somehow figure out.
The words that are related to computer.
For example we can simply take the ratio of these two probabilities and normalize the top of the model by the probability of the word in the background model.
So if we do that, we take the ratio, we'll see that then on the top, computer, is ramped, and then followed by software, program, all these words related to computer.
Because they occur very frequently in the context of computer, but not frequently in whole connection.
Where as these common words will not have a high probability.
In fact, they have a ratio of about one down there.
Because they are not really related to computer.
By taking the same ball of text that contains the computer we don't really see more occurrences of that in general.
So this shows that even with this simple LM models, we can do some limited analysis of semantics.
So in this lecture, we talked about, language model, which is basically a probability distribution over the text.
We talked about the simplistic language model called unigram language model.
Which is also just a word distribution.
We talked about the two uses of a language model.
One is to represent the, the topic in a document, in a classing or in general.
The other is discover word associations.
In the next lecture we're going to talk about the how language model can be used to design a retrieval function.
Here are two additional readings.
The first is a textbook on statistical and natural language processing.
The second is a article that has a survey of statistical language models with other pointers to research work.
This lecture is about mixture model estimation.
In this lecture we're going to continue discussing probabilistic topic models.
In particular, we're going to talk about how to estimate the parameters of a mixture model.
So let's first look at our motivation for using a mixture model.
And we hope to factor out the background words.
From the top-words equation.
The idea is to assume that the text data actually contained two kinds of words.
One kind is from the background here.
So, the is, we, etc.
And the other kind is from our pop board distribution that we are interested in.
So in order to solve this problem of factoring out background words, we can set up our mixture model as false.
We're going to assume that we already know the parameters of all the values for all the parameters in the mixture model, except for the water distribution of which is our target.
So this is a case of customizing a probabilist model so that we embedded a known variable that we are interested in.
But we're going to simplify other things.
We're going to assume we have knowledge above others.
And this is a powerful way of customizing a model.
For a particular need.
Now you can imagine, we could have assumed that we also don't know the background words.
But in this case, our goal is to factor out precisely those high probability background words.
So we assume the background model is already fixed.
And one problem here is how can we adjust theta sub d in order to maximize the probability of the observed document here and we assume all the other perimeters are now.
Now although we designed the model holistically.
To try to factor out these background words.
It's unclear whether, if we use maximum write or estimator.
We will actually end up having a whole distribution where the Common words like the would indeed have smaller probabilities than before.
Now in this case it turns out the answer is yes.
And when we set up the probability in this way, when we use maximum likelihood or we will end up having a word distribution where the use common words would be factored out.
By the use of the background rule of distribution.
So to understand why this is so, it's useful to examine the behavior of a mixture model.
So we're going to look at a very very simple case.
In order to understand some interesting behaviors of a mixture model.
The observed pattern here actually are generalizable to mixture model in general.
But it's much easier to understand this behavior when we use A very simple case like what we are seeing here.
So specifically in this case, let's assume that the probability choosing each of the two models is exactly the same.
So we're going to flip a fair coin to decide which model to use.
Furthermore, we're going to assume there are.
Precisely two words, the and text.
Obviously this is a very naive oversimplification of the actual text, but again, it's useful to examine the behavior in such a special case.
So we further assume that the background model gives probability of 0.9 towards the end text 0.1.
Now, lets also assume that our data is extremely simple the document has just two words text and the so now lets right down the likeable function in such a case.
First, what's the probability of text, and what's the probably of the.
I hope by this point you'll be able to write it down.
So the probability of text is basically the sum over two cases, where each case corresponds with to each of the order distribution and it accounts for the two ways of generating text.
And inside each case, we have the probability of choosing the model, which is 0.5 multiplied by the probability of observing text from that model.
Similarly, the, would have a probability of the same form, just what is different is the exact probabilities.
So naturally our lateral function is just a product of the two.
So It's very easy to see that, once you understand what's the probability of each word.
Which is also why it's so important to understand what's exactly the probability of observing each word from such a mixture model.
Now, the interesting question now is, how can we then optimize this likelihood?
Well, you will note that there are only two variables.
They are precisely the two probabilities of the two words.
Text [INAUDIBLE] given by theta sub d. And this is because we have assumed that all the other parameters are known.
So, now the question is a very simple algebra question.
So, we have a simple expression with two variables and we hope to choose the values of these two variables to maximize this function.
And the exercises that we have seen some simple algebra problems.
Note that the two probabilities must sum to one, so there's some constraint.
If there were no constraint of course, we would set both probabilities to their maximum value which would be one, to maximize, But we can't do that because text then the must sum to one.
We can't give both a probability of one.
So, now the question is how should we allocate the probability and the math between the two words.
What do you think?
Now, it would be useful to look at this formula For a moment, and to see what, intuitively, what we do in order to do set these probabilities to maximize the value of this function.
Okay, if we look into this further, then we see some interesting behavior of The two component models in that they will be collaborating to maximize the probability of the observed data.
Which is dictated by the maximum likelihood estimator.
But they are also competing in some way, and in particular, they would be competing on the words.
And they would tend to back high probabilities on different words to avoid this competition in some sense or to gain advantages in this competition.
So again, looking at this objective function and we have a constraint on the two probabilities.
Now, if you look at the formula intuitively, you might feel that you want to set the probability of text to be somewhat larger.
And this inducing can be work supported by mathematical fact, which is when the sum of two variables is a constant then the product of them which is maximum when they are equal, and this is a fact we know from algebra.
Now if we plug that [INAUDIBLE] It would mean that we have to make the two probabilities equal.
And when we make them equal and then if we consider the constraint it will be easy to solve this problem, and the solution is the probability of tax will be .09 and probability is .01.
The probability of text is now much larger than probability of the, and this is not the case when have just one distribution.
And this is clearly because of the use of the background model, which assigned the very high probability to the and low probability to text.
And if you look at the equation you will see obviously some interaction of the two distributions here.
In particular, you will see in order to make them equal.
And then the probability assigned by theta sub d must be higher for a word that has a smaller probability given by the background.
This is obvious from examining this equation.
Because the background part is weak for text.
It's small.
So in order to compensate for that, we must make the probability for text given by theta sub D somewhat larger, so that the two sides can be balanced.
So this is in fact a very general behavior of this model.
And that is, if one distribution assigns a high probability to one word than another, then the other distribution would tend to do the opposite.
Basically it would discourage other distributions to do the same And this is to balance them out so we can account for all kinds of words.
And this also means that by using a background model that is fixed into assigned high probabilities through background words.
We can indeed encourages the unknown topical one of this to assign smaller probabilities for such common words.
Instead put more probability than this on the content words, that cannot be explained well by the background model.
Meaning that they have a very small probability from the background motor like text here.
This lecture is about the Latent Aspect Rating Analysis for Opinion Mining and Sentiment Analysis.
In this lecture, we're going to continue discussing Opinion Mining and Sentiment Analysis.
In particular, we're going to introduce Latent Aspect Rating Analysis which allows us to perform detailed analysis of reviews with overall ratings.
So, first is motivation.
Here are two reviews that you often see in the net about the hotel.
And you see some overall ratings.
In this case, both reviewers have given five stars.
And, of course, there are also reviews that are in text.
Now, if you just look at these reviews, it's not very clear whether the hotel is good for its location or for its service.
It's also unclear why a reviewer liked this hotel.
What we want to do is to decompose this overall rating into ratings on different aspects such as value, rooms, location, and service.
So, if we can decompose the overall ratings, the ratings on these different aspects, then, we can obtain a more detailed understanding of the reviewer's opinionsabout the hotel.
And this would also allow us to rank hotels along different dimensions such as value or rooms.
But, in general, such detailed understanding will reveal more information about the user's preferences, reviewer's preferences.
And also, we can understand better how the reviewers view this hotel from different perspectives.
Now, not only do we want to infer these aspect ratings, we also want to infer the aspect weights.
So, some reviewers may care more about values as opposed to the service.
And that would be a case.
like what's shown on the left for the weight distribution, where you can see a lot of weight is places on value.
But others care more for service.
And therefore, they might place more weight on service than value.
The reason why this is also important is because, do you think about a five star on value, it might still be very expensive if the reviewer cares a lot about service, right?
For this kind of service, this price is good, so the reviewer might give it a five star.
But if a reviewer really cares about the value of the hotel, then the five star, most likely, would mean really cheap prices.
So, in order to interpret the ratings on different aspects accurately, we also need to know these aspect weights.
When they're combined together, we can have a more detailed understanding of the opinion.
So the task here is to get these reviews and their overall ratings as input, and then, generate both the aspect ratings, the compose aspect ratings, and the aspect rates as output.
And this is a problem called Latent Aspect Rating Analysis.
So the task, in general, is given a set of review articles about the topic with overall ratings, and we hope to generate three things.
One is the major aspects commented on in the reviews.
Second is ratings on each aspect, such as value and room service.
And third is the relative weights placed on different aspects by the reviewers.
And this task has a lot of applications, and if you can do this, and it will enable a lot of applications.
I just listed some here.
And later, I will show you some results.
And, for example, we can do opinion based entity ranking.
We can generate an aspect-level opinion summary.
We can also analyze reviewers preferences, compare them or compare their preferences on different hotels.
And we can do personalized recommendations of products.
So, of course, the question is how can we solve this problem?
Now, as in other cases of these advanced topics, we wont have time to really cover the technique in detail.
But Im going to give you a brisk, basic introduction to the technique development for this problem.
So, first step, were going to talk about how to solve the problem in two stages.
Later, were going to also mention that we can do this in the unified model.
Now, take this review with the overall rating as input.
What we want to do is, first, we're going to segment the aspects.
So we're going to pick out what words are talking about location, and what words are talking about room condition, etc.
So with this, we would be able to obtain aspect segments.
In particular, we're going to obtain the counts of all the words in each segment, and this is denoted by C sub I of W and D. Now this can be done by using seed words like location and room or price to retrieve the [INAUDIBLE] in the segments.
And then, from those segments, we can further mine correlated words with these seed words and that would allow us to segmented the text into segments, discussing different aspects.
But, of course, later, as we will see, we can also use [INAUDIBLE] models to do the segmentation.
But anyway, that's the first stage, where the obtain the council of words in each segment.
In the second stage, which is called Latent Rating Regression, we're going to use these words and their frequencies in different aspects to predict the overall rate.
And this predicting happens in two stages.
In the first stage, we're going to use the [INAUDIBLE] and the weights of these words in each aspect to predict the aspect rating.
So, for example, if in your discussion of location, you see a word like, amazing, mentioned many times, and it has a high weight.
For example, here, 3.9.
Then, it will increase the Aspect Rating for location.
But, another word like, far, which is an acted weight, if it's mentioned many times, and it will decrease the rating.
So the aspect ratings, assume that it will be a weighted combination of these word frequencies where the weights are the sentiment weights of the words.
Of course, these sentimental weights might be different for different aspects.
So we have, for each aspect, a set of term sentiment weights as shown here.
And that's in order by beta sub I and W. In the second stage or second step, we're going to assume that the overall rating is simply a weighted combination of these aspect ratings.
So we're going to assume we have aspect weights to the [INAUDIBLE] sub i of d, and this will be used to take a weighted average of the aspect ratings, which are denoted by r sub i of d. And we're going to assume the overall rating is simply a weighted average of these aspect ratings.
So this set up allows us to predict the overall rating based on the observable frequencies.
So on the left side, you will see all these observed information, the r sub d and the count.
But on the right side, you see all the information in that range is actually latent.
So, we hope to discover that.
Now, this is a typical case of a generating model where would embed the interesting variables in the generated model.
And then, we're going to set up a generation probability for the overall rating given the observed words.
And then, of course, we can adjust these parameter values including betas Rs and alpha Is in order to maximize the probability of the data.
In this case, the conditional probability of the observed rating given the document.
So we have seen such cases before in, for example, PISA, where we predict a text data.
But here, we're predicting the rating, and the parameters, of course, are very different.
But we can see, if we can uncover these parameters, it would be nice, because r sub i of d is precise as the ratings that we want to get.
And these are the composer ratings on different aspects.
[INAUDIBLE] sub I D is precisely the aspect weights that we hope to get as a byproduct, that we also get the beta factor, and these are the [INAUDIBLE] factor, the sentiment weights of words.
So more formally, the data we are modeling here is a set of review documents with overall ratings.
And each review document denote by a d, and the overall ratings denote by r sub d. And d pre-segments turn into k aspect segments.
And we're going to use ci(w,d) to denote the count of word w in aspect segment i.
Of course, it's zero if the word doesn't occur in the segment.
Now, the model is going to predict the rating based on d. So, we're interested in the provisional problem of r sub-d given d. And this model is set up as follows.
So r sub-d is assumed the two follow a normal distribution doesn't mean that denotes actually await the average of the aspect of ratings r Sub I of d as shown here.
This normal distribution is a variance of data squared.
Now, of course, this is just our assumption.
The actual rating is not necessarily anything thing this way.
But as always, when we make this assumption, we have a formal way to model the problem and that allows us to compute the interest in quantities.
In this case, the aspect ratings and the aspect weights.
Now, the aspect rating as you see on the [INAUDIBLE] is assuming that will be a weight of sum of these weights.
Where the weight is just the [INAUDIBLE] of the weight.
So as I said, the overall rating is assumed to be a weighted average of aspect ratings.
Now, these other values, r for sub I of D, or denoted together by other vector that depends on D is that the token of specific weights.
And were going to assume that this vector itself is drawn from another Multivariate Gaussian distribution, with mean denoted by a Mu factor, and covariance metrics sigma here.
Now, so this means, when we generate our overall rating, we're going to first draw a set of other values from this Multivariate Gaussian Prior distribution.
And once we get these other values, we're going to use then the weighted average of aspect ratings as the mean here to use the normal distribution to generate the overall rating.
Now, the aspect rating, as I just said, is the sum of the sentiment weights of words in aspect, note that here the sentiment weights are specific to aspect.
So, beta is indexed by i, and that's for aspect.
And that gives us a way to model different segment of a word.
This is neither because the same word might have positive sentiment for another aspect.
It's also used for see what parameters we have here beta sub i and w gives us the aspect-specific sentiment of w. So, obviously, that's one of the important parameters.
But, in general, we can see we have these parameters, beta values, the delta, and the Mu, and sigma.
So, next, the question is, how can we estimate these parameters and, so we collectively denote all the parameters by lambda here.
Now, we can, as usual, use the maximum likelihood estimate, and this will give us the settings of these parameters, that with a maximized observed ratings condition of their respective reviews.
And of, course, this would then give us all the useful variables that we are interested in computing.
So, more specifically, we can now, once we estimate the parameters, we can easily compute the aspect rating, for aspect the i or sub i of d. And that's simply to take all of the words that occurred in the segment, i, and then take their counts and then multiply that by the center of the weight of each word and take a sum.
So, of course, this time would be zero for words that are not occurring in and that's why were going to take the sum of all the words in the vocabulary.
Now what about the s factor weights?
Alpha sub i of d, well, it's not part of our parameter.
Right?
So we have to use that to compute it.
And in this case, we can use the Maximum a Posteriori to compute this alpha value.
Basically, we're going to maximize the product of the prior of alpha according to our assumed Multivariate Gaussian Distribution and the likelihood.
In this case, the likelihood rate is the probability of generating this observed overall rating given this particular alpha value and some other parameters, as you see here.
So for more details about this model, you can read this paper cited here.
.
In this lecture, we're going to talk about how to instantiate a vector space model, so that we can get a very specific ranking function.
So this is the, to continue the discussion of the vector space model.
Which is one particular approach to design ranking function.
And we are going to talk about how we use the general framework of the the vector space model.
As a guidance to instantiate the framework to derive a specific ranking function.
And we're going to cover the simplest instantiation of the framework.
So as we discussed in the previous lecture.
The vector space model is really a framework.
It isn't, didn't say.
As we discussed in the previous lecture, vector space model is really a framework.
It doesn't, say many things.
So for example here it shows that it did not say how we should define the dimension.
It also did not say how we place a documented vector in this space.
It did not say how we place a query vector in this vector space.
And finally, it did not say how we should match a similarity between the query vector and the document vector.
So, you can imagine, in order to implement this model.
We have to see specifically, how we are computing these vectors.
What is exactly xi and what is exactly yi?
This will determine where we place the document vector.
Where we place a query vector.
And of course, we also need to say exactly what will be the similarity function.
So if we can provide a definition of the concepts that would define the dimensions and these xi's, or yi's.
And then, the waits of terms for query and document.
Then we will be able to place document vectors and query vector in this well defined space.
And then, if we also specify similarity function, then we'll have well defined ranking function.
So let's see how we can do that.
And think about the the simpliciter instantiation.
Actually, I would suggest you to pause the lecture at this point spend a couple of minute to think about.
Suppose you are asked to implement this idea.
You've come up with the idea of vector space model.
But you still haven't figured out how to compute this vector exactly, how to define this similarity function.
What would you do?
So think for a couple of minutes and then, proceed.
So let's think about some simplest ways of instantiating this vector space model.
First, how do we define a dimension.
Well the obvious choice is we use each word in our vocabulary to define a dimension.
And a whole issue that there are n words in our vocabulary, therefore there are n dimensions.
Each word defines one dimension.
And this is basically the Bag of Words Instantiation.
Now let's look at how we place vectors in this space.
Again here, the simplest of strategy is to use a bit vector to represent both a query and a document.
And that means each element xi and yi would be taking a value of either zero or one.
When it's one, it means the corresponding word is present in the document or in the query.
When it's zero, it's going to mean that it's absent.
So you can imagine if the user types in a few word in your query.
Then the query vector, we only have a few ones, many, many zeros.
The document vector in general we have more ones of course, but we also have many zeros.
So it seems the vocabulary is generally very large.
Many words don't really occur in a document.
Many words will only occasionally occur in the document.
A lot of words will be absent in a particular document.
So, now we have placed the documents and the query in the vector space.
Let's look at how we match up the similarity.
So, a commonly used similarity measure here is Dot Product.
The dot product of two vectors is simply defined as the sum of the products of the corresponding elements of the two vectors.
So here we see that it's the product of x1 and the y1.
So here.
And then, x2 multiplied by y2.
And then finally xn multiplied by yn.
And then we take a sum here.
So that's the dot product.
Now we can represent this in a more general way, using a sum here.
So this only one of the many different ways of matching the similarity.
So now we see that we have defined the, the dimensions.
We have defined the, the vectors.
And we have also defined the similarity function.
So now we finally have the Simplest Vector Space Model.
Which is based on the bit vector representation, dot product similarity, and bag of words instantiation.
And the formula looks like this.
So this is our formula.
And that's actually a particular retrieval function, a ranking function all right?
Now, we can finally implement this function using a program language and then rank documents for query.
Now at this point you should again pause the lecture to think about how we can interpret this score.
So we have gone through the process of modeling the retrieval problem using a vector space model.
And then, we make assumptions.
About how we place vectors in the vector space and how we define the similarity.
So in the end we've got a specific retrieval function shown here.
Now the next step is to think about what of this individual function actually makes sense?
I, can we expect this function to actually perform well?
Where we use it to ramp it up, for use in query.
So, it's worth thinking about, what is this value that we are calculating?
So in the end, we've got a number, but what does this number mean?
Is it meaningful?
So spend a couple minutes to think about that.
And of course, the general question here is do you believe this is a good ranking function?
Would it actually work well?
So again, think about how to interpret this value.
Is it actually meaningful?
Does it mean something?
So related to how well that document matches the query.
So in order to assess whether this simplest vector space model actually works well, let's look at the example.
So here I show some sample documents and a simple query.
The query is news about the presidential campaign.
And we have five documents here.
They cover different, terms in the query.
And if you look at the, these documents for a moment.
You may realize that some documents are probably relevant in some cases or probably not relevant.
Now if I ask you to rank these documents, how would you rank them?
This is basically our ideal ranking.
Right.
When humans can examine the documents and then try to rank them.
Now, so think for a moment and take a look at this slide.
And perhaps by pausing the lecture.
So I think most of you would agree that d4, and d3, are probably better than others.
Because they really cover the query well.
They match news, presidential, and campaign.
So, it looks like that these two documents are probably better than the others.
They should be ranked on top.
And the other three, d1, d2, and d5, are really non-relavant.
So we can also say d4 and d3 are relevent documents, and d1, d2, and d5 are non-relevant.
So, now lets see if our vector space model could do the same or could do something closer.
So let's first think about how we actually use this model to score documents.
Right here I show two documents, d1 and d3, and we have the query also here.
In the vector space model, of course we want to first compute the vectors for these documents and the query.
Now I issue with the vocabulary here as well, so these are the n dimensions that we'll be thinking about.
So what do you think is the vector representation for the query?
Note that we are assuming that we only use zero and one to indicate whether a term is absent or present in the query or in the document.
So these are zero, one bit vectors.
So what do you think is the query vector?
Well the query has four words here.
So for these four words, there would be a one and for the rest, there will be zeros.
Now what about the documents?
It's the same.
So d1 has two rows, news and about.
So there are two ones here and the rest are zeros.
Similarly, so now that we have the two vectors, let's compute the similarity.
And we're going to use dot product.
So you can see when we use dot product we just, multiply the corresponding elements.
Right.
So these two would be, form a, be forming a product.
And these two will generate another product.
And these two would generate yet another product.
And so on and so forth.
Now you can, you need to see if we do that.
We actually don't have to care about these zeroes because if whenever we have a zero, the product will be zero.
So, when we take a sum over all these pairs, then the zero entries will be gone.
As long as you have one zero, then the product would be zero.
So in the fact, we're just counting how many pairs of one and one, right?
In this case, we have seen two.
So the result will be two.
So, what does that mean?
Well that means, this number or the value of this scoring function.
Is simply the count of how many unique query terms are matched in the document.
Because if a document, if a term is matched in the document, then there will be two ones.
If it's not, then there will be zero on the document side.
Similarly, if the document has a term,.
But the terms not in the query there will be zero in the query vector.
So those don't count.
So as a result this scoring function basically meshes how many unique query terms are matched in a document.
This is how we interpret this score.
Now we can also take a look at the d3.
In this case, you can see the result is three.
Because d3 matched the three distinctive query words, news, presidential, campaign.
Whereas d1 only matched two.
Now in this case, it seems reasonable to rank d3 on top of d1.
And this simplest vector space model indeed does that.
So that looks pretty good.
However, if we examine this model in detail, we likely will find some problems.
So here I'm going to show all the scores for these five documents.
And you can even verify they are correct.
Because we're basically counting the number of unique query terms matched in each document.
Now note that this method actually makes sense.
Right?
It basically means if a document matches more unique query terms, then the document will be assuming to be more relevant.
And that seems to make sense.
The only problem is here, we can note set there are three documents, d2, d3, and d4.
And they tied with a three, as a score.
So that's a problem, because if you look at them carefully it seems that d4 should be right above d3.
Because d3 only mentioned the presidential once.
But d4 mentioned it much more times.
In case of d3, presidential could be extended mentioned.
But d4 is clearly above presidential campaign.
Another problem is that d2 and d3 also have the same soul.
But, if you look at the, the three words that are matched.
In the case of d2, it matched the news, about, and the campaign.
But in the case of d3, it match the news, presidential, and campaign.
So intuitively, d3 is better.
Because matching presidential is more important though than matching about.
Even though about and the presidential are both in the query.
So intuitively, we would like d3 to be ranked above d2.
But this model, doesn't do that.
So that means this is still not good enough, we have to solve these problems.
To summarize, in this lecture we talked about how to instantiate a vector space model.
We may need to do three things.
One is to define the dimension.
The second is to decide how to place documents as vectors in the vector space.
And to also place a query in the vector space as a vector.
And third is to define the similarity between two vectors, particularly the query vector and the document vector.
We also talked about a very simple way to instantiate the vector space model.
Indeed, that's probably the simplest vector space model that we can derive.
In this case, we use each word to define a dimension.
We use a zero one bit vector to represent a document or a query.
In this case, we basically only care about word presence or absence.
We ignore the frequency.
And we use the dot product as the similarity function.
And with such a, a, in situation.
And we showed that the scoring function is basically to score a document based on the number of distinct query words matched in the document.
We also show that such a single vector space model still doesn't work well, and we need to improve it.
And this is the topic that we're going to cover in the next lecture.
>> This lecture is about the Overview of Statistical Language Models, which cover proper models as special cases.
In this lecture we're going to give a overview of Statical Language Models.
These models are general models that cover probabilistic topic models as a special cases.
So first off, what is a Statistical Language Model?
A Statistical Language Model is basically a probability distribution over word sequences.
So, for example, we might have a distribution that gives, today is Wednesday a probability of .001.
It might give today Wednesday is, which is a non-grammatical sentence, a very, very small probability as shown here.
And similarly another sentence, the eigenvalue is positive might get the probability of .00001.
So as you can see such a distribution clearly is Context Dependent.
It depends on the Context of Discussion.
Some Word Sequences might have higher probabilities than others but the same Sequence of Words might have different probability in different context.
And so this suggests that such a distribution can actually categorize topic such a model can also be regarded as Probabilistic Mechanism for generating text.
And that just means we can view text data as data observed from such a model.
For this reason, we call such a model as Generating Model.
So, now given a model we can then assemble sequences of words.
So, for example, based on the distribution that I have shown here on this slide, when matter it say assemble a sequence like today is Wednesday because it has a relative high probability.
We might often get such a sequence.
We might also get the item value as positive sometimes with a smaller probability and very, very occasionally we might get today is Wednesday because it's probability is so small.
So in general, in order to categorize such a distribution we must specify probability values for all these different sequences of words.
Obviously, it's impossible to specify that because it's impossible to enumerate all of the possible sequences of words.
So in practice, we will have to simplify the model in some way.
So, the simplest language model is called the Unigram Language Model.
In such a case, it was simply a the text is generated by generating each word independently.
But in general, the words may not be generated independently.
But after we make this assumption, we can significantly simplify the language more.
Basically, now the probability of a sequence of words, w1 through wn, will be just the product of the probability of each word.
So for such a model, we have as many parameters as the number of words in our vocabulary.
So here we assume we have n words, so we have n probabilities.
One for each word.
And then some to 1.
So, now we assume that our text is a sample drawn according to this word distribution.
That just means, we're going to draw a word each time and then eventually we'll get a text.
So for example, now again, we can try to assemble words according to a distribution.
We might get Wednesday often or today often.
And some other words like eigenvalue might have a small probability, etcetera.
But with this, we actually can also compute the probability of every sequence, even though our model only specify the probabilities of words.
And this is because of the independence.
So specifically, we can compute the probability of today is Wednesday.
Because it's just a product of the probability of today, the probability of is, and probability of Wednesday.
For example, I show some fake numbers here and when you multiply these numbers together you get the probability that today's Wednesday.
So as you can see, with N probabilities, one for each word, we actually can characterize the probability situation over all kinds of sequences of words.
And so, this is a very simple model.
Ignore the word order.
So it may not be, in fact, in some problems, such as for speech recognition, where you may care about the order of words.
But it turns out to be quite sufficient for many tasks that involve topic analysis.
And that's also what we're interested in here.
So when we have a model, we generally have two problems that we can think about.
One is, given a model, how likely are we to observe a certain kind of data points?
That is, we are interested in the Sampling Process.
The other is the Estimation Process.
And that, is to think of the parameters of a model given, some observe the data and we're going to talk about that in a moment.
Let's first talk about the sampling.
So, here I show two examples of Water Distributions or Unigram Language Models.
The first one has higher probabilities for words like a text mining association, it's separate.
Now this signals a topic about text mining because when we assemble words from such a distribution, we tend to see words that often occur in text mining contest.
So in this case, if we ask the question about what is the probability of generating a particular document.
Then, we likely will see text that looks like a text mining paper.
Of course, the text that we generate by drawing words.
This distribution is unlikely coherent.
Although, the probability of generating attacks mine [INAUDIBLE] publishing in the top conference is non-zero assuming that no word has a zero probability in the distribution.
And that just means, we can essentially generate all kinds of text documents including very meaningful text documents.
Now, the second distribution show, on the bottom, has different than what was high probabilities.
So food [INAUDIBLE] healthy [INAUDIBLE], etcetera.
So this clearly indicates a different topic.
In this case it's probably about health.
So if we sample a word from such a distribution, then the probability of observing a text mining paper would be very, very small.
On the other hand, the probability of observing a text that looks like a food nutrition paper would be high, relatively higher.
So that just means, given a particular distribution, different than the text.
Now let's look at the estimation problem now.
In this case, we're going to assume that we have observed the data.
I will know exactly what the text data looks like.
In this case, let's assume we have a text mining paper.
In fact, it's abstract of the paper, so the total number of words is 100.
And I've shown some counts of individual words here.
Now, if we ask the question, what is the most likely Language Model that has been used to generate this text data?
Assuming that the text is observed from some Language Model, what's our best guess of this Language Model?
Okay, so the problem now is just to estimate the probabilities of these words.
As I've shown here.
So what do you think?
What would be your guess?
Would you guess text has a very small probability, or a relatively large probability?
What about query?
Well, your guess probably would be dependent on how many times we have observed this word in the text data, right?
And if you think about it for a moment.
And if you are like many others, you would have guessed that, well, text has a probability of 10 out of 100 because I've observed the text 10 times in the text that has a total of 100 words.
And similarly, mining has 5 out of 100.
And query has a relatively small probability, just observed for once.
So it's 1 out of 100.
Right, so that, intuitively, is a reasonable guess.
But the question is, is this our best guess or best estimate of the parameters?
Of course, in order to answer this question, we have to define what do we mean by best, in this case, it turns out that our guesses are indeed the best.
In some sense and this is called Maximum Likelihood Estimate.
And it's the best thing that, it will give the observer data our maximum probability.
Meaning that, if you change the estimate somehow, even slightly, then the probability of the observed text data will be somewhat smaller.
And this is called a Maximum Likelihood Estimate.
This lecture is about using a time series as context to potentially discover causal topics in text.
In this lecture, we're going to continue discussing Contextual Text Mining.
In particular, we're going to look at the time series as a context for analyzing text, to potentially discover causal topics.
As usual, it started with the motivation.
In this case, we hope to use text mining to understand a time series.
Here, what you are seeing is Dow Jones Industrial Average stock price curves.
And you'll see a sudden drop here.
Right.
So one would be interested knowing what might have caused the stock market to crash.
Well, if you know the background, and you might be able to figure it out if you look at the time stamp, or there are other data that can help us think about.
But the question here is can we get some clues about this from the companion news stream?
And we have a lot of news data that generated during that period.
So if you do that we might actually discover the crash.
After it happened, at the time of the September 11 attack.
And that's the time when there is a sudden rise of the topic about September 11 happened in news articles.
Here's another scenario where we want to analyze the Presidential Election.
And this is the time series that are from the Presidential Prediction Market.
For example, I write a trunk of market would have stocks for each candidate.
And if you believe one candidate that will win then you tend to buy the stock for that candidate, causing the price of that candidate to increase.
So, that's a nice way to actual do survey of people's opinions about these candidates.
Now, suppose you see something drop of price for one candidate.
And you might also want to know what might have caused the sudden drop.
Or in a social science study, you might be interested in knowing what method in this election, what issues really matter to people.
Now again in this case, we can look at the companion news stream and ask for the question.
Are there any clues in the news stream that might provide insight about this?
So for example, we might discover the mention of tax cut has been increasing since that point.
So maybe, that's related to the drop of the price.
So all these cases are special cases of a general problem of joint analysis of text and a time series data to discover causal topics.
The input in this case is time series plus text data that are produced in the same time period, the companion text stream.
And this is different from the standard topic models, where we have just to text collection.
That's why we see time series here, it serves as context.
Now, the output that we want to generate is the topics whose coverage in the text stream has strong correlations with the time series.
For example, whenever the topic is managing the price tends to go down, etc.
Now we call these topics Causal Topics.
Of course, they're not, strictly speaking, causal topics.
We are never going to be able to verify whether they are causal, or there's a true causal relationship here.
That's why we put causal in quotation marks.
But at least they are correlating topics that might potentially explain the cause and humans can certainly further analyze such topics to understand the issue better.
And the output would contain topics just like in topic modeling.
But we hope that these topics are not just the regular topics with.
These topics certainly don't have to explain the data of the best in text, but rather they have to explain the data in the text.
Meaning that they have to reprehend the meaningful topics in text.
Cement but also more importantly, they should be correlated with external hand series that's given as a context.
So to understand how we solve this problem, let's first adjust to solve the problem with reactive topic model, for example PRSA.
And we can apply this to text stream and with some extension like a CPRSA or Contextual PRSA.
Then we can discover these topics in the correlation and also discover their coverage over time.
So, one simple solution is, to choose the topics from this set that have the strongest correlation with the external time series.
But this approach is not going to be very good.
Why?
Because awareness pictured to the topics is that they will discover by PRSA or LDA.
And that means the choice of topics will be very limited.
And we know these models try to maximize the likelihood of the text data.
So those topics tend to be the major topics that explain the text data well.
aAnd they are not necessarily correlated with time series.
Even if we get the best one, the most correlated topics might still not be so interesting from causal perspective.
So here in this work site here, a better approach was proposed.
And this approach is called Iterative Causal Topic Modeling.
The idea is to do an iterative adjustment of topic, discovered by topic models using time series to induce a product.
So here's an illustration on how this work, how this works.
Take the text stream as input and then apply regular topic modeling to generate a number of topics.
Let's say four topics.
Shown here.
And then we're going to use external time series to assess which topic is more causally related or correlated with the external time series.
So we have something that rank them.
And we might think that topic one and topic four are more correlated and topic two and topic three are not.
Now we could have stopped here and that would be just like what the simple approached that I talked about earlier then we can get to these topics and call them causal topics.
But as I also explained that these topics are unlikely very good because they are general topics that explain the whole text connection.
They are not necessary.
The best topics are correlated with our time series.
So what we can do in this approach is to first zoom into word level and we can look into each word and the top ranked word listed for each topic.
Let's say we take Topic 1 as the target examined.
We know Topic 1 is correlated with the time series.
Or is at least the best that we could get from this set of topics so far.
And we're going to look at the words in this topic, the top words.
And if the topic is correlated with the Time Series, there must be some words that are highly correlated with the Time Series.
So here, for example, we might discover W1 and W3 are positively correlated with Time Series, but W2 and W4 are negatively correlated.
So, as a topic, and it's not good to mix these words with different correlations.
So we can then for the separate of these words.
We are going to get all the red words that indicate positive correlations.
W1 and W3.
And we're going to also get another sub topic.
If you want.
That represents a negatively correlated words, W2 and W4.
Now, these subtopics, or these variations of topics, based on the correlation analysis, are topics that are still quite related to the original topic, Topic 1.
But they are already deviating, because of the use of time series information for bias selection of words.
So then in some sense, well we should expect so, some sense more correlated with the time series than the original Topic 1.
Because the Topic 1 has mixed words, here we separate them.
So each of these two subtopics can be expected to be better coherent in this time series.
However, they may not be so coherent as it mention.
So the idea here is to go back to topic model by using these each as a prior to further guide the topic modeling.
And that's to say we ask our topic models now discover topics that are very similar to each of these two subtopics.
And this will cause a bias toward more correlate to the topics was a time series.
Of course then we can apply topic models to get another generation of topics.
And that can be further ran to the base of the time series to set after the highly correlated topics.
And then we can further analyze the components at work in the topic and then try to analyze.word level correlation.
And then get the even more correlated subtopics that can be further fed into the process as prior to drive the topic of model discovery.
So this whole process is just a heuristic way of optimizing causality and coherence, and that's our ultimate goal.
Right?
So here you see the pure topic models will be very good at maximizing topic coherence, the topics will be all meaningful.
If we only use causality test, or correlation measure, then we might get a set words that are strongly correlate with time series, but they may not necessarily mean anything.
It might not be cementric connected.
So, that would be at the other extreme, on the top.
Now, the ideal is to get the causal topic that's scored high, both in topic coherence and also causal relation.
In this approach, it can be regarded as an alternate way to maximize both sine engines.
So when we apply the topic models we're maximizing the coherence.
But when we decompose the topic model words into sets of words that are very strong correlated with the time series.
We select the most strongly correlated words with the time series.
We are pushing the model back to the causal dimension to make it better in causal scoring.
And then, when we apply the selected words as a prior to guide a topic modeling, we again go back to optimize the coherence.
Because topic models, we ensure the next generation of topics to be coherent and we can iterate when they're optimized in this way as shown on this picture.
So the only I think a component that you haven't seen such a framework is how to measure the causality.
Because the rest is just talking more on.
So let's have a little bit of discussion of that.
So here we show that.
And let's say we have a topic about government response here.
And then we just talking more of we can get coverage of the topic over time.
So, we have a time series, X sub t. Now, we also have, are give a time series that represents external information.
It's a non text time series, Y sub t. It's the stock prices.
Now the the question here is does Xt cause Yt?
Well in other words, we want to match the causality relation between the two.
Or maybe just measure the correlation of the two.
There are many measures that we can use in this framework.
For example, pairs in correlation is a common use measure.
And we got to consider time lag here so that we can try to capture causal relation.
Using somewhat past data and using the data in the past to try to correlate with the data on points of y that represents the future, for example.
And by introducing such lag, we can hopefully capture some causal relation by even using correlation measures like person correlation.
But a common use, the measure for causality here is Granger Causality Test.
And the idea of this test is actually quite simple.
Basically you're going to have all the regressive model to use the history information of Y to predict itself.
And this is the best we could without any other information.
So we're going to build such a model.
And then we're going to add some history information of X into such model.
To see if we can improve the prediction of Y.
If we can do that with a statistically significant difference.
Then we just say X has some causal inference on Y, or otherwise it wouldn't have causal improvement of prediction of Y.
If, on the other hand, the difference is insignificant and that would mean X does not really have a cause or relation why.
So that's the basic idea.
Now, we don't have time to explain this in detail so you could read, but you would read at this cited reference here to know more about this measure.
It's a very convenient used measure.
Has many applications.
So next, let's look at some simple results generated by this approach.
And here the data is the New York Times and in the time period of June And here the time series we used is stock prices of two companies.
American Airlines and Apple and the goal is to see if we inject the sum time series contest, whether we can actually get topics that are wise for the time series.
Imagine if we don't use any input, we don't use any context.
Then the topics from New York times discovered by PRSA would be just general topics that people talk about in news.
All right.
Those major topics in the news event.
But here you see these topics are indeed biased toward each time series.
And particularly if you look at the underlined words here in the American Airlines result, and you see airlines, airport, air, united trade, or terrorism, etc.
So it clearly has topics that are more correlated with the external time series.
On the right side, you see that some of the topics are clearly related to Apple, right.
So you can see computer, technology, software, internet, com, web, etc.
So that just means the time series has effectively served as a context to bias the discovery of topics.
From another perspective, these results help us on what people have talked about in each case.
So not just the people, what people have talked about, but what are some topics that might be correlated with their stock prices.
And so these topics can serve as a starting point for people to further look into issues and you'll find the true causal relations.
Here are some other results from analyzing Presidential Election time series.
The time series data here is from Iowa Electronic market.
And that's a prediction market.
And the data is the same.
New York Times from May That's for Now, what you see here are the top three words in significant topics from New York Times.
And if you look at these topics, and they are indeed quite related to the campaign.
Actually the issues are very much related to the important issues of this presidential election.
Now here I should mention that the text data has been filtered by using only the articles that mention these candidate names.
It's a subset of these news articles.
Very different from the previous experiment.
But the results here clearly show that the approach can uncover some important issues in that presidential election.
So tax cut, oil energy, abortion and gun control are all known to be important issues in that presidential election.
And that was supported by some literature in political science.
And also I was discussing Wikipedia, right.
So basically the results show that the approach can effectively discover possibly causal topics based on the time series data.
So there are two suggested readings here.
One is the paper about this iterative topic modeling with time series feedback.
Where you can find more details about how this approach works.
And the second one is reading about Granger Casuality text.
So in the end, let's summarize the discussion of Text-based Prediction.
Now, Text-based prediction is generally very useful for big data applications that involve text.
Because they can help us inform new knowledge about the world.
And the knowledge can go beyond what's discussed in the text.
As a result can also support optimizing of our decision making.
And this has a wider spread application.
Text data is often combined with non-text data for prediction.
because, for this purpose, the prediction purpose, we generally would like to combine non-text data and text data together, as much cruel as possible for prediction.
And so as a result during the analysis of text and non-text is very necessary and it's also very useful.
Now when we analyze text data together with non-text data, we can see they can help each other.
So non-text data, provide a context for mining text data, and we discussed a number of techniques for contextual text mining.
And on the other hand, a text data can also help interpret patterns discovered from non-text data, and this is called a pattern annotation.
In general, this is a very active research topic, and there are new papers being published.
And there are also many open challenges that have to be solved.
.
This lecture is about how to do fast research by using inverted index.
In this lecture, we are going to continue the discussion of the system implementation.
In particular, we're going to talk about, to how to support a faster search by using inverted index.
So, let's think about what a general scoring function might look like.
Now, of curse the vector space model is a special case of this.
But we can imagine many other retrieval functions of the same form.
So, the form of this function is as follows.
We see this scoring function of document d, and query q is defined as first, a function of f a that's adjustment in the function.
That what consider two factors that are shown here at the end, f sub d of d, and f sub q of q.
These are adjustment factors of a document and query, so they're at the level of document, and query.
So, and then inside of this function we also see there's a another function called edge.
So, this is the main part of the scoring function, and these as I just said of the scoring factors at the level of the whole document, and the query.
For example, document and this aggregate function would then combine all these.
Now, inside this h function, there are functions that would compute the weights of the contribution of a matched query term t i.
So, this this g, the function g gives us the weight of a matched query term t i in document d. And this h function with that aggregate all these weights, so it were, for example, take a sum, but it of all the matched query in that terms.
But it can also be a product, or could be another way of aggregate them.
And then finally, this adjustment function would then consider the document level, or query level factors through further adjuster score, for example, document lens [INAUDIBLE].
So, this general form would cover many state of original functions.
Let's look at how we can score such score documents with such a function using inverted index.
So here's the general algorithm that works as follows.
First these these Query level and document level factors can be pre-computed in the indexing term.
Of course, for the query, we have to compute it as a query term.
But for document, for example, document can be pre-computed.
And then we maintain a score accumulator for each document d to compute the h. And h is aggregation function of all the matching query terms.
So how do we do that?
Well, for each query term, we going to do fetch inverted list, from the inverted index.
This will give us all the documents that match this query term, and that includes d1, f1, and so, d and fn.
So each pair is document id and the frequency of the term in the document.
Then for each entry d sub j and f sub j, a particular match of the term in this particular document d sub j, we're going to computer the function g. That would give us something like a t of i, ef weights of this term.
So, we're computing the weight contribution of matching this query term in this document.
And then we're going to update the score accumulator for this document.
And this would allow us to add this to our accumulator, that would incrementally compute function h. So this is basically a general way to allow sort of computer all functions of this form, by using inverted index.
Note that we don't have to attach any document that that didn't match any query term, but this is why it's fast.
We only need to process the documents that tap, that match at least one query term.
In the end, then we're going to adjust the score to compute a, this function f of a and then we can sort.
So let's take a look at the specific example.
In this, case let's assume the scoring function's a very simple one.
It just takes us sum of tf, the rule of tf, the count of, of term in the document.
Now this simple equation with the help showing the algorithm clearly.
It's very easy to extend the, the computation to include other weights like the transformation of TF or document or IDF weighting.
So let's take a look at specific example with the query's information security, and shows some entries of the inverted index on the right side.
Information occurring before documents and the frequencies is also there, security is coding three documents.
So, let's see how the algorithm works, all right?
So, first we iterate all the query terms, and we fetch the first query then.
What is that?
That's information.
Right?
So, and imagine we have all these score accumulators to score, score the, score the scores for these documents.
We can imagine there will be allocated, but then they will only be allocated as needed.
So before we do any weighting of terms we don't even need a score accumulators.
But conceptual we have these score accumulators eventually allocated, right?
So let's fetch the, the entries from the inverted list for information first, that's the first one.
So these score accumulators obviously would be initialized as zeros.
So the first entry is d1 and 3, information in this document.
Since our scoring function assume that the score is just a sum of these raw counts.
We just need to add a 3 to the score accumulator to account for the increase of score, due to matching this term information, a document d1.
And now we go to the next entry.
That's d2 and 4 and then we'll add a 4 to the score accumulator of d2.
Of course, at this point we will allocate the score accumulator as needed.
And so, at this point, we have located d1 and d2, and the next one is d3.
And we add 1, or we locate another score coming in the spot d3 and add 1 to it.
And finally, the d4 gets a 5 because the information the term information occurred ti in five times in this document.
Okay, so this completes the processing of all the entries in the, inverted index for information.
It's processed all the contributions of matching information in this four documents.
So now our arrows will go to the next query term, that's security.
So, we're going to factor all the inverted index entries for security.
So in this case, there were three entries.
And we're going to go through each of them.
The first is d2 and 3.
And that means security occurred three times in d2, and what do we do?
Well, we do exactly the same as what we did for information.
So this time we're going to do change the score, accumulating d2 sees it's already allocate.
And what we do is we'll add 3 to the existing value which is a 4, so we now get the 7 for d2.
D2 sc, score is increased because of the match both information and the security.
Go to the next step entry, that's d4 and 1, so we've updated the score for d4,and again we add 1 to d4, so d4 goes from 5 to 6.
Finally we process d5 and 3.
SInce we have not yet equated a score accumulator d4 to d5, at this point, we allocate one, So, those scores on the last row are the final scores for these documents.
If our scoring function is just a, a simple sum of tf values.
Now what if we actually would like to, to do lands normalization.
Well we can do the normalization at this point for each document.
So to summarize this, all right so you can see we first processed the information determine query term information, and we process all the entries in the inverted index for this term.
Then we process the security, all right, let's think about the what should be the order of processing here when we consider query terms?
It might make difference, especially if we don't want to keep to keep all the score accumulators.
Let's say we only want to keep the most promising score accumulators.
What do you think it would be a good order to go through?
Would you go would you process a common term first or would you process a rare term first?
The answer is we should go through we should process the rare term first.
A rare term will match fewer documents and then the score confusion will be higher, because the IDF value will be higher and, and then it allows us to attach the most diplomacy documents first.
So it helps pruning some non promising ones, if we don't need so many documents to be returned to the user.
And so those are heuristics for further improving the accuracy.
Here can also see how we can incorporate the idea of weighting.
All right.
So they can [INAUDIBLE] when we incorporated a one way process each query term.
When we fetch in word index we can fetch the document frequency, and then we can compute the IDF.
Or maybe perhapsIDF value has already been pre-computed when we index the document.
At that time we already computed the IDF value that we can just fetch it.
So all these can be down at this time.
So that will mean one will process all the entries for information these these weights would be adjusted by the same IDF, which is IDF for information.
So this is the basic idea of using inverted index for faster search, and works well for all kinds of formulas that are of the general form and this generally cov, the general form covers actually most state of the art retrieval functions.
So there are some tricks to further improve the efficiency ,some general mac tech, techniques include caching.
This is just a to store some results of popular query's, so that next time when you see the same query you simply return the stored results.
Similarly, you can also score the missed of inverted index in the memory for popular term.
And if the query comes popular you will assume it will fetch the inverted index for the same term again.
So keeping that in the memory would help.
And these are general techniques for improving efficiency.
We can also only keep the most promising accumulators because a user generally doesn't want to examine so many documents.
We only want to return high quality subset of documents that likely ranked on the top, in,in for that purpose we can then prune the accumulators.
We don't have to store all the accumulators.
At some point we just keep the highest value accumulators.
Another technique is to do parallel processing, and that's needed for really processing such a large data set, like the web data set.
And to scale up to the Web-scale we need to special to have the special techniques to do parallel processing and to distribute the storage of files on multiple machines.
So here as a, here is a list of some text retrieval toolkits.
It's, it's not a complete list.
You can find the more information at this URL on the bottom.
Here I listed four here, lucene is one of the most popular toolkit that can support a lot of applications.
And it has very nice support for applications.
You can use it to build a search engine very quickly, the downside is that it's not that easy to extend it, and the algorithms incremented there are not the most advanced algorithms.
Lemur or Indri is another toolkit that that does not have such a nice support application as Lucene.
But it has many advanced search algorithms.
And it's also easy to extend.
Terrier is yet another toolkit that also has good support for quotation capability and some advanced algorithms.
So that's maybe in between Lemur, or Lucene or maybe rather combining the strands of both, so that's also useful toolkit.
MeTA is the toolkit that we'll use for the programming assignment, and this is a new toolkit that has a combination of both text retrieval algorithms and text mining algorithms.
And so, toolkit models are implement, they are, there are a number of text analysis algorithms, implemented in the toolkit, as well as basic research algorithms.
So, to summarize all the discussion about the system implementation, here are the major take away points.
Inverted index is the primary data structure for supporting a search engine.
That's the key to enable faster response to a user's query.
And the basic idea is process that, pre-process the data as much as we can, and we want to do compression when appropriate.
So that we can save disk space and can speed up IO and processing of the inverted index in general.
We'll talk about how we will construct the inverted index when the data can fit into the memory.
And then we talk about faster search using inverted index, basically to exploit the inverted index to accumulate scores for documents matching a query term.
And we exploit Zipf's law avoid touching many documents that don't match any query term.
And this algorithm can, can support a wide range of ranking algorithms.
So these basic techniques have mm, have great potential for further scanning output using distribution to withstand parallel processing and the caching.
Here are two additional readings that you can take a look at if you have time, and are interested in learning more about this.
The first one is a classic textbook on the scare the efficiency of inverted index and the compression techniques, and how to in general, build a efficient search engine in terms of the space overhead and speed.
The second one is a newer textbook that has a nice discussion of implementing and evaluating search engines.
This lecture is a continued discussion of probabilistic topic models.
In this lecture, we're going to continue discussing probabilistic models.
We're going to talk about a very simple case where we are interested in just mining one topic from one document.
So in this simple setup, we are interested in analyzing one document and trying to discover just one topic.
So this is the simplest case of topic model.
The input now no longer has k, which is the number of topics because we know there is only one topic and the collection has only one document, also.
In the output, we also no longer have coverage because we assumed that the document covers this topic 100%.
So the main goal is just to discover the world of probabilities for this single topic, as shown here.
As always, when we think about using a generating model to solve such a problem, we start with thinking about what kind of data we are going to model or from what perspective we're going to model the data or data representation.
And then we're going to design a specific model for the generating of the data, from our perspective.
Where our perspective just means we want to take a particular angle of looking at the data, so that the model will have the right parameters for discovering the knowledge that we want.
And then we'll be thinking about the microfunction or write down the microfunction to capture more formally how likely a data point will be obtained from this model.
And the likelihood function will have some parameters in the function.
And then we argue our interest in estimating those parameters for example, by maximizing the likelihood which will lead to maximum likelihood estimated.
These estimator parameters will then become the output of the mining hours, which means we'll take the estimating parameters as the knowledge that we discover from the text.
So let's look at these steps for this very simple case.
Later we'll look at this procedure for some more complicated cases.
So our data, in this case is, just a document which is a sequence of words.
Each word here is denoted by x sub i.
Our model is a Unigram language model.
A word distribution that we hope to denote a topic and that's our goal.
So we will have as many parameters as many words in our vocabulary, in this case M. And for convenience we're going to use theta sub i to denote the probability of word w sub i.
And obviously these theta sub i's will sum to 1.
Now what does a likelihood function look like?
Well, this is just the probability of generating this whole document, that given such a model.
Because we assume the independence in generating each word so the probability of the document will be just a product of the probability of each word.
And since some word might have repeated occurrences.
So we can also rewrite this product in a different form.
So in this line, we have rewritten the formula into a product over all the unique words in the vocabulary, w sub 1 through w sub M. Now this is different from the previous line.
Well, the product is over different positions of words in the document.
Now when we do this transformation, we then would need to introduce a counter function here.
This denotes the count of word one in document and similarly this is the count of words of n in the document because these words might have repeated occurrences.
You can also see if a word did not occur in the document.
It will have a zero count, therefore that corresponding term will disappear.
So this is a very useful form of writing down the likelihood function that we will often use later.
So I want you to pay attention to this, just get familiar with this notation.
It's just to change the product over all the different words in the vocabulary.
So in the end, of course, we'll use theta sub i to express this likelihood function and it would look like this.
Next, we're going to find the theta values or probabilities of these words that would maximize this likelihood function.
So now lets take a look at the maximum likelihood estimate problem more closely.
This line is copied from the previous slide.
It's just our likelihood function.
So our goal is to maximize this likelihood function.
We will find it often easy to maximize the local likelihood instead of the original likelihood.
And this is purely for mathematical convenience because after the logarithm transformation our function will becomes a sum instead of product.
And we also have constraints over these these probabilities.
The sum makes it easier to take derivative, which is often needed for finding the optimal solution of this function.
So please take a look at this sum again, here.
And this is a form of a function that you will often see later also, the more general topic models.
So it's a sum over all the words in the vocabulary.
And inside the sum there is a count of a word in the document.
And this is macroed by the logarithm of a probability.
So let's see how we can solve this problem.
Now at this point the problem is purely a mathematical problem because we are going to just the find the optimal solution of a constrained maximization problem.
The objective function is the likelihood function and the constraint is that all these probabilities must sum to one.
So, one way to solve the problem is to use Lagrange multiplier approace.
Now this command is beyond the scope of this course but since Lagrange multiplier is a very useful approach, I also would like to just give a brief introduction to this, for those of you who are interested.
So in this approach we will construct a Lagrange function, here.
And this function will combine our objective function with another term that encodes our constraint and we introduce Lagrange multiplier here, lambda, so it's an additional parameter.
Now, the idea of this approach is just to turn the constraint optimization into, in some sense, an unconstrained optimizing problem.
Now we are just interested in optimizing this Lagrange function.
As you may recall from calculus, an optimal point would be achieved when the derivative is set to zero.
This is a necessary condition.
It's not sufficient, though.
So if we do that you will see the partial derivative, with respect to theta i here ,is equal to this.
And this part comes from the derivative of the logarithm function and this lambda is simply taken from here.
And when we set it to zero we can easily see theta sub i is related to lambda in this way.
Since we know all the theta i's must a sum to one we can plug this into this constraint, here.
And this will allow us to solve for lambda.
And this is just a net sum of all the counts.
And this further allows us to then solve the optimization problem, eventually, to find the optimal setting for theta sub i.
And if you look at this formula it turns out that it's actually very intuitive because this is just the normalized count of these words by the document ns, which is also a sum of all the counts of words in the document.
So, after all this mess, after all, we have just obtained something that's very intuitive and this will be just our intuition where we want to maximize the data by assigning as much probability mass as possible to all the observed the words here.
And you might also notice that this is the general result of maximum likelihood raised estimator.
In general, the estimator would be to normalize counts and it's just sometimes the counts have to be done in a particular way, as you will also see later.
So this is basically an analytical solution to our optimization problem.
In general though, when the likelihood function is very complicated, we're not going to be able to solve the optimization problem by having a closed form formula.
Instead we have to use some numerical algorithms and we're going to see such cases later, also.
So if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document d here?
Let's imagine this document is a text mining paper.
Now, what you might see is something that looks like this.
On the top, you will see the high probability words tend to be those very common words, often functional words in English.
And this will be followed by some content words that really characterize the topic well like text, mining, etc.
And then in the end, you also see there is more probability of words that are not really related to the topic but they might be extraneously mentioned in the document.
As a topic representation, you will see this is not ideal, right?
That because the high probability words are functional words, they are not really characterizing the topic.
So my question is how can we get rid of such common words?
Now this is the topic of the next module.
We're going to talk about how to use probabilistic models to somehow get rid of these common words.
This lecture is about the feedback in the vector space model.
In this lecture, we continue talking about the feedback and text retrieval.
Particularly we're going to talk about feedback in the vector space model.
As we have discussed before in the case of feedback the task of a text retrieval system is relearned from examples to improve retrieval accuracy.
We will have positive examples, those are the documents that are assumed that will be random or judged with being random and all the documents that are viewed by users.
We also have negative examples, those are documents known to be non-relevant.
They can also be the documents that are escaped by users.
The general method in the vector space model for feedback is to modify our query vector.
Now we want to place the query vector in a better position to make that accurate and what does that mean exactly?
Well, if you think about the query vector that would mean you would have to do something to vector elements.
And in general that would mean we might add new terms.
We might adjust weights of old terms or assign weights to new terms.
And as a result in general the query will have more terms so we often call this query expansion.
The most effective method in the vector space model of feedback is called Rocchio feedback which was actually proposed several decades ago.
So, the idea is quite simple we illustrate this idea by using a two-dimensional display of all the documents in the collection and also the query vector.
So, now we can see the query vector is here in the center and these are all of the documents.
So when we use a query vector and use a similarity function to find the most similar documents.
We are basically drawing a circle here and then these documents would be basically the top-ranked documents.
And this process of relevant documents, right?
And these are random documents for example that's relevant, etc.
And then these minuses are negative documents like this.
So our goal here is trying to move this query vector to some position to improve the retrieval accuracy.
By looking at this diagram what do you think where should we move the query vector so that we can improve the retrieval accuracy.
Intuitively, where do you want to move the query back to?
If you want to think more you can pause the video.
Now if you think about this picture you can realize that in order to work well in this case you want the query vector to be as close to the positive vectors as possible.
That means, ideally you want to place the query vector somewhere here or we want to move the query vector closer to this point.
Now, so what exactly at this point?
Well, if you want these relevant documents to be ranked on the top you want this to be in the center of all of these relevant documents, right?
Because then if you draw a circle around this one you get all these relevant documents.
So that means we can move the query back toward the centroid of all the relevant document vectors.
And this is basically the idea of Rocchio, of course you then can see that the centroid of negative documents.
And one move away from the negative documents.
Now geometrically we're talking about a moving vector closer to some other vector and away from other vectors.
Algebraically it just means we have this formula.
Here you can see this is original query vector and this average basically is the centroid vector of relevant documents.
When we take the average over these vectors then we're computing the centroid of these vectors.
And similarly this is the average in that non-relevant document of vectors so it's essentially of now random, documents.
And we have these three parameters here, alpha, beta and gamma.
They're controlling the amount of movement.
When we add these two vectors together we're moving the query at the closer to the centroid, alright, so when we add them, together.
When we subtracted this part we kind of move the query vector away from that centroid so this is the main idea of Rocchio Feedback.
And after we have done this we will get a new query vector which can use it to store documents.
This new New query vector will then reflect the move of this Original query vector toward this Relevant centroid vector and away from the Non-relevant centroid vector, okay?
So let's take a look at example, right?
This is the example that we have seen earlier only that I in the, the display of the actual documents I only showed the vector representation of these documents.
We have five documents here and we have true red in the documents here, right?
They are displayed in red and these are the term vectors.
Now, I just assumed an idea of weights, a lot of times we have zero weights of course.
These are negative documents, there are two here, there is another one here.
Now in this Rocchio method we first compute the centroid of each category and so let's see.
Look at the centroid of the positive document but we simply just so it's very easy to see.
We just add this with this one the corresponding element and that's down here and take the average.
And then we're going to add the corresponding elements and then just take the average, right?
So we do this for all these.
In the end, what we have is this one.
This is the average vector of these two so it's a centroid of these two, right?
Let's also look at the centroid of the nested documents.
This is basically the same we're going to take the average of three elements.
And these are the corresponding elements in these three vectors and so on and so forth.
So in the end, that we have this one.
Now, in the Rocchio feedback method we're going to combine all these with original query vector, which is this.
So now let's see how we combine them together.
Well, that's basically this, right?
So we have a parameter outlier controlling the original query term weight that's 1.
And now I've beta to control the inference of the positive centroid Vector weight that's 1.5 that comes from here, right?
So this goes here and we also have this negative wait here.
Conduit by a gamma here and this weight has come from of course the nective centroid here.
And we do exactly the same for other terms each is for one term.
And this is our new vector.
And we're going to use this new query vector, this one to run the documents.
You can imagine what would happen, right?
Because of the movement that this one or the match of these red documents much better.
Because we move this vector closer to them and it's going to penalize these black documents, these non-relevant documents.
So this is precisely what we want from feedback.
Now of course, if we apply this method in practice we will see one potential problem and that is the original query has only four times that are not zero.
But after we do queries, imagine you can imagine we'll have many terms that would have a number of weights.
So the calculation would have to involve more terms.
In practice, we often truncate this vector and only retain the terms which is the highest weight.
So let's talk about how we use this method in practice.
I just mentioned that we often truncate the vector consider only a small number of words that have highest weights in the centroid vector.
This is for efficiency concern.
I also say that here that a negative examples or non-relevant examples tend not to be very useful especially compared with positive examples.
Now you can think about the, why.
One reason is because negative documents tend to distract the query in all directions so when you take the average it doesn't really tell you where exactly it should be moving to.
Whereas, positive documents tend to be clustered together and they respond to you to consistent the direction.
So that also means that sometimesw we don't have those negative examples but note that in, in some cases in difficult queries where most top random results are negative.
Negative feedback afterwards is very useful.
Another thing is to avoid over-fitting that means we have to keep relatively high weight on the original query terms.
Why?
Because the sample that we see in feedback is a relatively small sample.
We don't want to overly trust the small sample and the original query terms are still very important.
Those terms are typed in by the user and the user has decided that those terms are most important.
So in order to prevent the us from over-fitting or drifting.
A type of drift prevent type of drifting due to the bias toward the, the feedback examples.
We generally would have to keep a pretty high weight on the original terms so it is safe to do that.
And this is especially, true for pseudo awareness feedback.
Now this method can be used for both relevance feedback and pseudo relevance feedback.
In the case of pseudo feedback, the parameter beta should be set to a, a smaller value because the random examples are assumed to be random there not as reliable as your relevance feedback, right?
In the case of relevance feedback, we obviously could use a larger value.
So, those parameters still have to be set and.
And the ro, Rocchio method is usually robust and effective.
It's, it's still a very popular method for feedback.
.
There are some interesting challenges in threshold.
Would have known in the filtering problem.
So here I show the, sort of the data that you can collect in, in the filtering system.
So you can see the scores and the status of relevance.
So the first one has a score 36.5, and it's relevant.
The second one is not relevant.
Of course, we have a lot of documents for which we don't know the status, because we will have to the user.
So as you can see here, we only see the judgements of documents delivered to the user.
So this is not a random sample.
So it's a censored data.
It's kind of biased, so that creates some difficulty for learning.
And secondly, there are in general very little labeled data and very few relevant data, so it's, it's also challenging for machine learning approaches.
Typically they require require more training data.
And in the extreme case at the beginning, we don't even have any, label there as well.
The system still has to make a decision, so that's a very difficult problem at the beginning.
Finally, the results of this issue of exploration versus exploitation tradeoff.
Now this means we also want to explore the document space a little bit, and to, to see if the user might be interested in the documents that we have not yet labeled.
So, in other words, we're going to explore the space of user interests by testing whether the user might be interested in some other documents that currently are not matching the user's interest.
This so well.
So how do we do that?
Well we could lower the threshold a little bit and do we just deliver some near misses to the user to see what the user would respond so see how the user will, would respond to this extra document.
And, and this is a trade off, because on the one hand, you want to explore, but on the other hand, you don't want to really explore too much, because then you would over-deliver non-relevant information.
So exploitation means you would, exploit what you learn about the user.
And let's say you know the user is interested in this particular topic, so you don't want to deviate that much.
And, but if you don't deviate at all, then you don't explore at all.
That's also not good.
You might miss opportunity to learn another interest of the user.
So this is a dilemma.
And that's also a difficult problem to solve.
Now how do we solve these problems?
In general, I think why can't I used the empirical utility optimization strategy?
And this strategy is basically to optimize the threshold based on, historical data, just as you have seen on the previous slide, right?
So you can just compute the utility on the training data for each candidate score threshold.
Pretend that [INAUDIBLE] cut at this point.
What if I cut out the [INAUDIBLE] threshold, what would happen?
What's utility?
Compute the utility, right?
We know the status, what's it based on approximation of click-throughs, right?
So then we can just choose this threshold that gives the maximum utility on the training data.
Now but this of course doesn't account for exploration that we just talked about.
And there is also the difficulty of bias.
Training sample, as we mentioned.
So in general, we can only get an upper bound or, for the true optimal threshold because the, the al, the threshold might be actually lower than this.
So it's possible that the discarded item might be actually interesting to the user.
So how do we solve this problem?
Well we generally as I said we can lower the threshold to explore a little bit.
So here's one particular approach called the beta-gamma threshold learning.
So the, the idea is foreign.
So, here I show a ranked list of all the training documents that we have seen so far.
And they are ranked by their positions.
And on the Y-axis, we show the Utility.
Of course, this function depends on how you specify the coefficients in the Utility function.
But we can not imagine depending on the cut off position we will have a utility.
That means suppose I cut at this position and that will be the utility.
So we can for example I then find some cut off point.
The optimal point theta optimal is the point when we would achieve the maximum utility if we had chosen this threshold.
And there is also 0 threshold, As you can see at this cut off.
The utility is 0.
Now, what does that mean?
That means if I lower the threshold, and then get the, and now I'm I reach this threshold, the utility would be lower, but it's still positive.
Still non-elective, at least.
So it's not as high as the optimal utility, but it gives us a a safe point to explore the threshold.
As I just explained, it's desirable to explore the interest space.
So it's desirable to lower the threshold based on your training data.
So that means, in general, we want to set the threshold somewhere in this range.
It's the when user off fault to control the the deviation from the optimal utility point.
So you can see the formula of the threshold will be just the incorporation of the zero utility threshold and the optimal between the threshold.
Now the question is how, how should we set r form, you know and when should we deviate more from the optimal utility point.
Well this can depend on multiple factors and the one way to solve the problem is to encourage this threshold mechanism to explore up the 0 point, and that's a safe point, but we're not going to necessarily reach all the way to the 0 point.
But rather we're going to use other parameters to further define alpha.
And this specifically is as follows.
So there will be a beta parameter to control.
The deviation from the optimal threshold.
And this can be based on for example can be accounting for the over throughout the training data let's say.
And so this can be just the adjustment factor.
But what's more interesting is this gamma parameter here, and you can see in this formula gamma is controlling the the influence of the number of examples in training data set.
So you can see in this formula as N which denotes the number of training examples.
Becomes bigger than it would actually encourage less exploration.
In other words, when N is very small, it will try to explore more.
And that just means if we have seen few examples, we're not sure whether we have exhausted the space of interests.
So [INAUDIBLE].
But as we have seen many examples from the user, many data points, then we feel that we probably dont' have to explore more.
So this gives us a dynamic of strategy for exploration, right?
The more examples we have seen, the less exploration we are going to do.
So, the threshold will be closer to the optimal threshold.
So, that's the basic idea of this approach.
Now, this approach actually, has been working well in some evaluation studies.
And, particularly effective.
And, also can welcome arbitrary utility with a appropriate lower bound.
And explicitly addresses exploration-exploration tradeoff.
And it kind of uses a zero in this threshold point as a, a safeguard.
For exploration and exploiting tradeoff.
We're not, never going to explore further than the zero utility point.
So, if you take the analogy of gambling, and you, you don't want to risk losing money.
You know, so it's a safe strategy, a conservative strategy for exploration.
And the problem is, of course, this approach is purely heuristic.
And the zero utility lower bound is also often too conservative.
And there are, of course, calls are more advanced than machine learning projects that have been proposed for solving these problems.
And this is a very active research area.
So to summarize there are two strategies for recommending systems or filtering systems.
One is content based, which is looking at the item similarity.
And the other is collaborative filtering, which is looking at the user similarity.
In this lecture we have covered content-based filtering approach.
In the next lecture, we're going to talk about collaborative filtering.
The content-based filtering system we generally have to solve several problems related to filtering decision and learning, etc.
And such a system can actually be based on a search engine system by adding a threshold mechanism and adding adaptive learning algorithm to allow the system to learn from long term feedback from the user.
So to summarize our discussion of recommender systems in some sense the filtering task of recommended is easy and in some other sense and the task is actually difficult.
So its easy because the user dexpectations, though in this case, the system takes initiative to push the information to the user.
So the user doesn't really make an effort.
So any recommendation is better than nothing, right?
So unless you recommend that all the you know, noisy items or useless documents, if you can recommend that some useful information uses general, would appreciate it, all right.
So that's in that sense, that's easy.
However, filtering is actually a much harder task.
Because you have to make a binary decision, and you can't afford waiting for a lot of items and then you will whether one item is better than others.
You have to make a decision when you see this item.
Let's think about news filtering as well as you see the news.
And you have to decide whether the news would be interesting to a user.
If you wait for a few days, well, even if you can make accurate recommendation of the most relevant news, only two days wouldn't be significantly decreased.
Another reason why it's hard, it's because of data sparseness.
If you think of this as a learning problem in collaborative filtering, for example, it's purely based on learning from the past ratings.
So if you don't have many ratings, there's really not much you can do, right?
And may I just mention this problem.
This is actually a very serious problem.
But of course there are strategies that have been proposed to solve the problem.
And there are, there are different strategies that we will use to alleviate the problem.
We can use, for example, more user information to assess their similarity instead of using the preferences.
Of these users on these items the immediate additional information or better for about the user etcetera and, and we also talked about the two strategies for filtering task.
One is content based where we look at items in clarity you know there's a clarity of filtering where we look at the user similarity.
And they obviously can be combined.
In a practical system, you can imagine, they generally would have to be combined.
So that will give us a hybrid strategy for filtering.
A, and, we also could recall that we talked about push versus pull as two strategies for getting access to the text data.
And recommend the system is it will help, users in the push mode.
And search engines are, certain users in the pull mode.
Of using the tool should be combined, and they can be combined into have a system that can support user with multiple mode and formation access.
So in the future, we could anticipate for such a system to be more usable to a user.
And also this is a active research area so there are a lot of new algorithms being, being proposed over time.
In particular, those new algorithms tend to use a lot of context information.
Now the context here could be the context of the user, you know, it could also be context of documents or items.
The items are not isolated.
They are connected in many ways.
The users might form social network as well, so there's a rich context there that we can leverage in order to really solve the problem well, and then that's a active research area where also machine learning algorithms have been applied.
Here are some additional readings in the handbook called Recommender Systems.
And has a collection of a lot of good articles that can give you an overview of a number of specific approaches to recommender systems.
This lecture is about the word association mining and analysis.
In this lecture, we're going to talk about how to mine associations of words from text.
Now this is an example of knowledge about the natural language that we can mine from text data.
Here's the outline.
We're going to first talk about what is word association and then explain why discovering such relations is useful and finally we're going to talk about some general ideas about how to mine word associations.
In general there are two word relations and these are quite basic.
One is called a paradigmatic relation.
The other is syntagmatic relation.
A and B have paradigmatic relation if they can be substituted for each other.
That means the two words that have paradigmatic relation would be in the same semantic class, or syntactic class.
And we can in general replace one by the other without affecting the understanding of the sentence.
That means we would still have a valid sentence.
For example, cat and dog, these two words have a paradigmatic relation because they are in the same class of animal.
And in general, if you replace cat with dog in a sentence, the sentence would still be a valid sentence that you can make sense of.
Similarly Monday and Tuesday have paradigmatical relation.
The second kind of relation is called syntagmatical relation.
In this case, the two words that have this relation, can be combined with each other.
So A and B have syntagmatic relation if they can be combined with each other in a sentence, that means these two words are semantically related.
So for example, cat and sit are related because a cat can sit somewhere.
Similarly, car and drive are related semantically and they can be combined with each other to convey meaning.
However, in general, we can not replace cat with sit in a sentence or car with drive in the sentence to still get a valid sentence, meaning that if we do that, the sentence will become somewhat meaningless.
So this is different from paradigmatic relation.
And these two relations are in fact so fundamental that they can be generalized to capture basic relations between units in arbitrary sequences.
And definitely they can be generalized to describe relations of any items in a language.
So, A and B don't have to be words and they can be phrases, for example.
And they can even be more complex phrases than just a non-phrase.
If you think about the general problem of the sequence mining then we can think about the units being and the sequence data.
Then we think of paradigmatic relation as relations that are applied to units that tend to occur in a singular locations in a sentence, or in a sequence of data elements in general.
So they occur in similar locations relative to the neighbors in the sequence.
Syntagmatical relation on the other hand is related to co-occurrent elements that tend to show up in the same sequence.
So these two are complimentary and are basic relations of words.
And we're interested in discovering them automatically from text data.
Discovering such worded relations has many applications.
First, such relations can be directly useful for improving accuracy of many NLP tasks, and this is because this is part of our knowledge about a language.
So if you know these two words are synonyms, for example, and then you can help a lot of tasks.
And grammar learning can be also done by using such techniques.
Because if we can learn paradigmatic relations, then we form classes of words, syntactic classes for example.
And if we learn syntagmatic relations, then we would be able to know the rules for putting together a larger expression based on component expressions.
So we learn the structure and what can go with what else.
Word relations can be also very useful for many applications in text retrieval and mining.
For example, in search and text retrieval, we can use word associations to modify a query, and this can be used to introduce additional related words into a query and make the query more effective.
It's often called a query expansion.
Or you can use related words to suggest related queries to the user to explore the information space.
Another application is to use word associations to automatically construct the top of the map for browsing.
We can have words as nodes and associations as edges.
A user could navigate from one word to another to find information in the information space.
Finally, such word associations can also be used to compare and summarize opinions.
For example, we might be interested in understanding positive and negative opinions about the iPhone 6.
In order to do that, we can look at what words are most strongly associated with a feature word like battery in positive versus negative reviews.
Such a syntagmatical relations would help us show the detailed opinions about the product.
So, how can we discover such associations automatically?
Now, here are some intuitions about how to do that.
Now let's first look at the paradigmatic relation.
Here we essentially can take advantage of similar context.
So here you see some simple sentences about cat and dog.
You can see they generally occur in similar context, and that after all is the definition of paradigmatic relation.
On the right side you can kind of see I extracted expressly the context of cat and dog from this small sample of text data.
I've taken away cat and dog from these sentences, so that you can see just the context.
Now, of course we can have different perspectives to look at the context.
For example, we can look at what words occur in the left part of this context.
So we can call this left context.
What words occur before we see cat or dog?
So, you can see in this case, clearly dog and cat have similar left context.
You generally say his cat or my cat and you say also, my dog and his dog.
So that makes them similar in the left context.
Similarly, if you look at the words that occur after cat and dog, which we can call right context, they are also very similar in this case.
Of course, it's an extreme case, where you only see eats.
And in general, you'll see many other words, of course, that can't follow cat and dog.
You can also even look at the general context.
And that might include all the words in the sentence or in sentences around this word.
And even in the general context, you also see similarity between the two words.
So this was just a suggestion that we can discover paradigmatic relation by looking at the similarity of context of words.
So, for example, if we think about the following questions.
How similar are context of cat and context of dog?
In contrast how similar are context of cat and context of computer?
Now, intuitively, we're to imagine the context of cat and the context of dog would be more similar than the context of cat and context of the computer.
That means, in the first case the similarity value would be high, between the context of cat and dog, where as in the second, the similarity between context of cat and computer would be low because they all not having a paradigmatic relationship and imagine what words occur after computer in general.
It would be very different from what words occur after cat.
So this is the basic idea of what this covering, paradigmatic relation.
What about the syntagmatic relation?
Well, here we're going to explore the correlated occurrences, again based on the definition of syntagmatic relation.
Here you see the same sample of text.
But here we're interested in knowing what other words are correlated with the verb eats and what words can go with eats.
And if you look at the right side of this slide and you see, I've taken away the two words around eats.
I've taken away the word to its left and also the word to its right in each sentence.
And then we ask the question, what words tend to occur to the left of eats?
And what words tend to occur to the right of eats?
Now thinking about this question would help us discover syntagmatic relations because syntagmatic relations essentially captures such correlations.
So the important question to ask for syntagmatical relation is, whenever eats occurs, what other words also tend to occur?
So the question here has to do with whether there are some other words that tend to co-occur together with each.
Meaning that whenever you see eats you tend to see the other words.
And if you don't see eats, probably, you don't see other words often either.
So this intuition can help discover syntagmatic relations.
Now again, consider example.
How helpful is occurrence of eats for predicting occurrence of meat?
Right.
All right, so knowing whether eats occurs in a sentence would generally help us predict whether meat also occurs indeed.
And if we see eats occur in the sentence, and that should increase the chance that meat would also occur.
In contrast, if you look at the question in the bottom, how helpful is the occurrence of eats for predicting of occurrence of text?
Because eats and text are not really related, so knowing whether eats occurred in the sentence doesn't really help us predict the weather, text also occurs in the sentence.
So this is in contrast to the question about eats and meat.
This also helps explain that intuition behind the methods of what discovering syntagmatic relations.
Mainly we need to capture the correlation between the occurrences of two words.
So to summarize the general ideas for discovering word associations are the following.
For paradigmatic relation, we present each word by its context.
And then compute its context similarity.
We're going to assume the words that have high context similarity to have paradigmatic relation.
For syntagmatic relation, we will count how many times two words occur together in a context, which can be a sentence, a paragraph, or a document even.
And we're going to compare their co-occurrences with their individual occurrences.
We're going to assume words with high co-occurrences but relatively low individual occurrences to have syntagmatic relations because they attempt to occur together and they don't usually occur alone.
Note that the paradigmatic relation and the syntagmatic relation are actually closely related in that paradigmatically related words tend to have syntagmatic relation with the same word.
They tend to be associated with the same word, and that suggests that we can also do join the discovery of the two relations.
So these general ideas can be implemented in many different ways.
And the course won't cover all of them, but we will cover at least some of the methods that are effective for discovering these relations.
This lecture is about Collaborative Filtering.
In this lecture, we're going to continue the discussion of Recommender Systems.
In particular, we're going to look at the approach of collaborative filtering.
You have seen this slide before when we talked about the two strategies to answer the basic question will user U like item X.
In the previous lecture, we looked at the item similarity, that's content-based filtering.
In this lecture, we're going to look at the user similarity.
This is a different strategy called collaborative filtering.
So first of all, what is collaborative filtering?
It is to make filtering decisions for individual user based on the judgement of other users and that is to say, we will infer individual's interest or preferences from that, of other similar users.
So the general idea is the following.
Given a user u, we are going to first find the similar users, u1 through and then we're going to predict the used preferences based on the preferences of these similar users, u1 through.
Now the users similarity here can be judged based on their similarity.
The preference is on a common set of items.
Now here you'll see that the exact content of item doesn't really matter.
We're going to look at the only, the relationship between the users and the items.
So this means this approach is very general if it can be applied to any items not just with text objects.
So this approach, it would work well under the following assumptions.
First users with the same interests will have similar preferences.
Second, the users with similar preferences probably share the same interests.
So for example, if the interest of the user is in information retrieval, then we can infer the user probably favor SIGIR papers.
And so those who are interested in information retrieval researches probably all favor SIGIR papers, that's something that we make.
And if this assumption is true, then it would help collaborative filtering to work well.
We can also assume that if we see people favor SIGIR papers, then we can infer the interest is probably information retrieval.
So these simple examples, it seems what makes sense.
And in many cases such as assumption actually does make sense.
So, another assumption you have to make is that there are a sufficiently large number of user preferences available to us.
So for example, if you see a lot of ratings of users for movies and those indicate their preferences in movies.
And if you have a lot of such data, then collaborative filtering can be very effective.
If not, there will be a problem and that's often called a cold start problem.
That means you don't have many preferences available, so the system could not fully take advantage of collaborative filtering yet.
So let's look at the collaborative filtering problem in a more formal way.
And so this picture shows that we are in general considering a lot of users and showing we're showing m users here.
So, u1 through and we're also considering a number of objects.
Let's say, n objects denoted as o1 through on and then we will assume that the users will be able to judge those objects and the user could for example, give ratings to those items.
For example, those items could be movies, could be products and then the users would give ratings one through five, let's say.
So what you see here is that we have assumed some ratings available for some combinations.
So some users have watched movies, they have rated those movies.
They obviously won't be able to watch all the movies and some users may actually only watch a few movies.
So this is in general a response matrix, right?
So many item many entries have unknown values and what's interesting here is we could potentially infer the value of a element in this matrix based on other values and that's actually the central question in collaborative filtering.
And that is, we assume an unknown function here f, that would map a pair of user and object to a rating.
And we have observed there are some values of this function and we want to infer the value of this function for other pairs that we, that don't have values available here.
So this is ve, very similar to other machine learning problems, where we would know the values of the function on some training there that and we hope to predict the the values of this function on some test there.
All right.
So this is the function approximation.
And how can we pick out the function based on the observed ratings?
So this is the, the setup.
Now there are many approaches to solving this problem.
And in fact, this is a very active research area.
A reason that there are special conferences dedicated to the problem is a major conference devoted to the problem.
This lecture is about natural language content analysis.
As you see from this picture, this is really the first step to process any text data.
Text data are in natural languages.
So, computers have to understand natural languages to some extent in order to make use of the data, so that's the topic of this lecture.
We're going to cover three things.
First, what is natural language processing, which is a main technique for processing natural language to obtain understanding?
The second is the State of the Art in NLP, which stands for natural language processing.
Finally, we're going to cover the relation between natural language processing and text retrieval.
First, what is NLP?
Well, the best way to explain it is to think about, if you see a text in a foreign language that you can't understand.
Now, what you have to do in order to understand that text?
This is basically what computers are facing.
Right?
So, looking at the simple sentence like, a dog is chasing a boy on the playground.
We don't have any problems understanding this sentence, but imagine what the computer would have to do in order to understand it.
For in general, it would have to do the following.
First, it would have to know dog is a noun, chasing's a verb, et cetera.
So, this is a code lexile analysis or part of speech tagging.
And, we need to pick out the, the syntaxing categories of those words.
So, that's a first step.
After that, we're going to figure out the structure of the sentence.
So for example, here it shows that a and dog would go together to form a noun phrase.
And, we won't have dog and is to go first, right.
And, there are some structures that are not just right.
But, this structure shows what we might get if we look at the sentence and try to interpret the sentence.
Some words would go together first, and then they will go together with other words.
So here, we show we have noun phrases as intermediate components and then verb phrases.
Finally, we have a sentence.
And, you get this structure, we need to do something called a syntactic analysis, or parsing.
And, we may have a parser, a computer program that would automatically create this structure.
At this point, you would know the structure of this sentence, but still you don't know the meaning of the sentence.
So, we have to go further through semantic analysis.
In our mind, we usually can map such a sentence to what we already know in our knowledge base.
And for example, you might imagine a dog that looks like that, there's a boy and there's some activity here.
But for computer, will have to use symbols to denote that.
All right.
So, we would use the symbol d1 to denote a dog.
And, b1 to denote a boy, and then p1 to denote the playground, playground.
Now, there is also a chasing activity that's happening here, so we have the relation chasing here, that connects all these symbols.
So, this is how a computer would obtain some understanding of this sentence.
Now from this representation, we could also further infer some other things, and we might indeed, naturally think of something else when we read text.
And, this is call inference.
So for example, if you believe that if someone's being chased and this person might be scared.
All right.
With this rule, you can see computers could also infer that this boy may be scared.
So, this is some extra knowledge that you would infer based on some understanding of the text.
You can even go further to understand the, why the person said this sentence.
So, this has to do with the use of language.
All right.
This is called pragmatic analysis.
In order to understand the speech actor of a sentence, all right, we say something to basically achieve some goal.
There's some purpose there and this has to do with the use of language.
In this case, the person who said the sentence might be reminding another person to bring back the dog.
That could be one possible intent.
To reach this level of understanding, we would require all these steps.
And, a computer would have to go through all these steps in order to completely understand this sentence.
Yet, we humans have no trouble with understand that.
We instantly, will get everything, and there is a reason for that.
That's because we have a large knowledge base in our brain, and we use common sense knowledge to help interpret the sentence.
Computers, unfortunately, are hard to obtain such understanding.
They don't have such a knowledge base.
They are still incapable of doing reasoning and uncertainties.
So, that makes natural language processing difficult for computers.
But, the fundamental reason why the natural language processing is difficult for computers is simple because natural language has not been designed for computers.
They, they, natural languages are designed for us to communicate.
There are other languages designed for computers.
For example, program languages.
Those are harder for us, right.
So, natural languages is designed to make our communication efficient.
As a result, we omit a lot of common sense knowledge because we assume everyone knows about that.
We also keep a lot of ambiguities because we assume the receiver, or the hearer could know how to discern an ambiguous word, based on the knowledge or the context.
There's no need to invent a different word for different meanings.
We could overload the same word with different meanings without the problem.
Because of these reasons, this makes every step in natural language of processing difficult for computers.
Ambiguity's the main difficulty, and common sense reasoning is often required, that's also hard.
So, let me give you some examples of challenges here.
Conceded the word-level ambiguities.
The same word can have different syntactical categories.
For example, design can be a noun or a verb.
The word root may have multiple meanings.
So, square root in math sense, or the root of a plant.
You might be able to think of other meanings.
There are also syntactical ambiguities.
For example, the main topic of this lecture, natural language processing, can actually be interpreted in two ways, in terms of the structure.
Think for a moment and see if you can figure that out.
We usually think of this as processing of natural languages, but you could also think of this as you say, language process is natural.
Right.
So, this is example of syntatic ambiguity.
Where we have different structures that can be applied to the same sequence of words.
Another example of ambiguous sentence is the following, a man saw a boy with a telescope.
Now, in this case, the question is, who had the telescope?
All right, this is called a prepositional phrase attachment ambiguity, or PP attachment ambiguity.
Now, we generally don't have a problem with these ambiguities because we have a lot of background knowledge to help us disintegrate the ambiguity.
Another example of difficulty is anaphora resolution.
So, think about the sentence like John persuaded Bill to buy a TV for himself.
The question here is, does himself refer to John or Bill?
So again, this is something that you have to use some background or the context to figure out.
Finally, presupposition is another problem.
Consider the sentence, he has quit smoking.
Now this obviously implies he smoked before.
So, imagine a computer wants to understand all the subtle differences and meanings.
They would have to use a lot of knowledge to figure that out.
It also would have to maintain a large knowl, knowledge base of odd meanings of words and how they are connected to our common sense knowledge of the word.
So this is why it's very difficult.
So as a result we are still not perfect.
In fact, far from perfect in understanding natural languages using computers.
So this slide sort of gives a simplified view of state of the art technologies.
We can do part of speech tagging pretty well.
So, I showed minus 7% accuracy here.
Now this number is obviously based on a certain data set, so don't take this literally.
All right, this just shows that we could do it pretty well.
But it's still not perfect.
In terms of parsing, we can do partial parsing pretty well.
That means we can get noun phrase structures or verb phrase structure, or some segment of the sentence understood correctly in terms of the structure.
And, in some evaluation results we have seen about 90% accuracy in terms of partial parsing of sentences.
Again, I have to say, these numbers are relative to the data set.
In some other data sets, the numbers might be lower.
Most of existing work has been evaluated using news data set.
And so, a lot of these numbers are more or less biased towards news data.
Think about social media data.
The accuracy likely is lower.
In terms of semantic analysis, we are far from being able to do a complete understanding of a sentence.
But we have some techniques that would allow us to do partial understanding of the sentence.
So, I could mention some of them.
For example, we have techniques that can allow us to extract the entities and relations mentioned in text or articles.
For example, recognizing the mentions of people, locations, organizations, et cetera in text.
Right?
So this is called entity extraction.
We may be able to recognize the relations.
For example, this person visited that per, that place.
Or, this person met that person, or this company acquired another company.
Such relations can be extracted by using the current and natural languaging processing techniques.
They are not perfect, but they can do well for some entities.
Some entities are harder than others.
We can also do word sentence disintegration to some extent.
We have to figure out whether this word in this sentence would have certain meaning, and in another context, the computer could figure out that it has a different meaning.
Again, it's not perfect but you can do something in that direction.
We can also do sentiment analysis meaning to figure out whether sentence is positive or negative.
This is a special use for, for review analysis for example.
So these examples of semantic analysis.
And they help us to obtain partial understanding of the sentences.
Right?
It's not giving us a complete understanding as I showed before for the sentence, but it will still help us gain understanding of the content and these can be useful.
In terms of inference, we are not yet there, probably because of the general difficulty of inference and uncertainties.
This is a general challenge in artificial intelligence.
That's probably also because we don't have complete semantic reimplementation for natural language text.
So this is hard.
Yet in some domains, perhaps in limited domains when you have a lot of restrictions on the world of users, you may be to may be able to perform inference to some extent, but in general we cannot really do that reliably.
Speech act analysis is also far from being done, and we can only do that analysis for very special cases.
So, this roughly gives you some idea about the state of the art.
And let me also talk a little bit about what we can't do.
And, and so we can't even do 100% part of speech tagging.
This looks like a simple task, but think about the example here, the two uses of off may have different syntactic categories if you try to make a fine grain distinctions.
It's not that easy to figure out such differences.
It's also hard to do general complete the parsing.
And, again this same sentence that you saw before is example.
This, this ambiguity can be very hard to disambiguate.
And you can imagine example where you have to use a lot of knowledge i, in the context of the sentence or from the background in order to figure out the, who actually had the telescope.
So is, i, although sentence looks very simple, it actually is pretty hard.
And in cases when the sentence is very long, imagine it has four or five prepositional phrases, then there are even more possibilities to figure out.
It's also harder to precise deep semantic analysis.
So here's example.
In this sentence, John owns a restaurant, how do we define owns exactly?
The word, own, you know, is something that we can understand but it's very hard to precisely describe the meaning of own for computers.
So as a result we have robust and general natural language processing techniques that can process a lot of text data in a shallow way, meaning we only do superficial analysis.
For example, part of s, of speech tagging, or partial parsing, or recognizing sentiment.
And those are not deep understanding because we're not really understanding the exact meaning of the sentence.
On the other hand, the deep understanding techniques tend not to scale up well, meaning that they would fail on some unrestricted text.
And if you don't restrict the text domain or the use of words, then these techniques tend not to work well.
They may work well, based on machine learning techniques on the data that are similar to the training data that the program has been trained on.
But they generally wouldn't work well on the data that are very different from the training data.
So this pretty much summarizes the state of the art of natural language processing.
Of course, within such a short amount of time, we can't really give you a, a complete view of any of it, which is a big field, and either expect that to have, to see multiple courses on natural language processing topic itself.
But, because of it's relevance to the topic that we talked about it's useful for you to know the background in case you haven't been exposed to that.
So, what does that mean for text retrieval?
Well, in text retrieval we are dealing with all kinds of text.
It's very hard to restrict the text to a certain domain.
And we also are often dealing with a lot of text data, so that means.
The NLP techniques must be general, robust, and efficient and that just implies today we can only use fairly shallow NLP techniques for text retrieval.
In fact, most search engines today use something called a bag of words representation.
Now this is probably the simplest representation you can probably think of.
That is to turn text data into simply a bag of words.
Meaning we will keep the individual words but we'll ignore all the orders of words.
And we'll keep duplicated occurrences of words.
So this is called a bag of words representation.
When you represent the text in this way, you ignore a lot about the information, and that just makes it harder to understand the exact meaning of a sentence because we've lost the order.
But yet, this representation tends to actually work pretty well for most search tasks.
And this is partly because the search task is not all that difficult.
If you see matching of some of the query words in a text document, chances are that that document is about the topic, although there are exceptions, right?
So in comparison some other tasks, for example machine translation, would require you to understand the language accurately, otherwise the translation would be wrong.
So in comparison, search tasks are solved relatively easy such a representation is often sufficient.
And that's also the representation that the major search engines today, like Google or Bing are using.
Of course I put in in parentheses but not all.
Of course there are many queries that are not answered well by the current search engines, and they do require a representation that would go beyond bag of words representation.
That would require more natural language processing, to be done.
There is another reason why we have not used the sophisticated NLP techniques in modern search engines, and that's because some retrieval techniques actually naturally solve the problem of NLP.
So, one example, is word sense disambiguation.
Think about a word like java.
It could mean coffee or it could mean program language.
If you look at the word alone it would be ambiguous.
But when the user uses the water in the query, usually there are other words.
For example I'm looking for usage of Java applet.
When I have applet there that implies Java means program language.
And that context can help us naturally prefer documents where Java is referring to program language, because those documents would probably match applet as well.
If java occurs in the document in a way that means coffee, then you would never match applet, or with very small probability.
Right.
So this is a case when some retrieval techniques naturally achieve the goal of word sense disambiguation.
Another example is some technique called feedback which we will talk about later in some of the lectures.
This tech, technique would allow us to add additional words to the query.
And those additional words could be related to the query words.
And these words can help match documents where the original query words have not occurred.
So this achieves, to some extent, semantic matching of terms.
So those techniques also helped us bypass some of the difficulties in natural language processing.
However, in the long run, we still need deeper natural language processing techniques in order to improve the accuracy of the current search engines.
And it's particularly needed for complex search tasks, or for question answering.
Google has recently launched a knowledge graph.
And this is one step toward that goal, because knowledge graph would contain entities and their relations.
And this goes beyond the simple bag of words representation.
And such technique should help us improve the search engine utility significantly, although this is a still open topic for research and exploration.
In sum, in this lecture we'll talk about what is NLP and we've talked about the state of the art techniques, what we can do, what we cannot do.
And finally, we also explained why bag of words representation remains the dominant representation used in modern search engines even though deeper NLP would be needed for future search engines.
If you want to know more you can take a look at some additional readings.
I only cited one here.
And that's a good starting point though.
Thanks.
This lecture is a summary of this course.
This map shows the major topics we have covered in this course.
And here are some key high-level take-away messages.
First we talk about natural language content analysis.
Here the main take-away message is natural language processing is the foundation for textual retrieval, but current NLP isn't robust enough.
So the back of words replenishing is generally the main method used in modern search engines and it's often sufficient for most of the search tasks.
But obviously, for more compass search tasks, then we need a deeper measurement processing techniques.
And we then talked about a high-level strategies for text access and we talked about push versus pull in plural.
We talked about a query, which is browsing.
Now, in general in future search engines, we should integrate all these techniques to provide a multiple information access and then we talked about a number of issues related to search engines.
We talked about the search problem and we framed that as a ranking problem and we talked about the a number of retrieval methods.
We start with an overview of the vector space model and probabilistic model and then we talked about the vector space model in that.
We also later talked about leverageable learning approach and that's probabilistic model.
And here, the main take-away message is that model retrieval functions tend to look similar and they generally use various heuristics.
Most important ones are TF-IDF waiting document length normalization and that TF is often transformed through a sub-linear transformation function and then we talked about how to implement a retrieval system.
And here the main technique that we talked about how to construct an inverted index.
So that we can prepare the system to answer a query quickly and we talked about how to, to fast research by using the inverted index and we then talked about how to evaluate the text retrieval system mainly introduced the Cranfield evaluation methodology.
This was a very important the various methodology of that can be applied to many tasks.
We talked about the major evaluation measures.
So the most important measures for a search engine are MAP mean average precision and nDCG.
Normalized discounted accumulative gain and also precision and record the two basic measures.
And we then talked about feedback techniques.
And we talked about the rock you in the vector space model and the mixture model in the language modeling approach.
Feedback is very important technique especially considering the opportunity of learning from a lot of pixels on the web.
We then talked about the web search.
And here, we talk about the how to use parallel indexing to resolve the scalability issue in indexing, we introduce a MapReduce and then we talked about the how to using information interacting pull search.
We talked about page random hits as the major algorithms to analyze links on the web.
We then talked about learning to rank.
This is a use of machine learning to combine multiple features for improving scoring.
Not only the effectiveness can be improved using this approach but we can also improve the robustness of the ranking function, so that it's not easy to spam a search engine with just a, a some features to promote a page.
And finally, we talked about the future of web search.
We talked about some major interactions that we might assume in the future in improving the current generation of search engines.
And then finally, we talked about the Recommender System and these are systems to implement the push mode and we'll talk about the two approaches.
One is content based, one is collaborative filtering and they can be combined together.
Now an obvious missing piece in this picture is the user, you can see.
So user interface is also a important component in any search engine, even though the current search interface is relatively simple.
There actually have been a lot of studies of user interfaces related to visualization for example and this is topic to that, you can learn more by reading this book.
It's a excellent book about all kind of studies of search user interface.
If you want to know more about the, the topics that we talked about, you can also read some additional readings that are listed here.
In this short course, we are only managing to cover some basic topics in text retrieval in search engines.
And these resources provide additional information about more advanced topics and they give more thorough treatment of some of the topics that we talked about.
And a main source is synthesis digital library where you can see a lot of short textbook or textbooks or long tutorials.
They tend to provide us with a lot of information to explain a topic and there are multiple series that are related to this course.
One is information concepts, retrieval and services.
Another is human Language technology and yet, another is artificial intelligence and machine learning.
There are also some major journals and conferences listed over here that tend to have a lot of research papers related to the topic of this course.
And finally for more information about resources including readings and tool kits, etc.
You can check out this URL.
So, if you have not taken the text mining course in this in this data mining specialization series, then naturally, the next step is to take that calls.
As this picture shows to mine the text data, we generally need two kinds of techniques.
One is text retrieval, which is covered in this course.
And these techniques will help us convert raw big text data into small, relevant text data, which are actually needed in the specific application.
And human plays important role in mining any text data, because text data is written for humans to consume.
So, involving humans in the process of data mining is very important.
And in this course, we have covered various strategies to help users get access to the most relevant data.
These techniques are also essential in any text mining system to help provide providence and to help users interpret the inner patterns that the user would find through text data mining.
So, in general, the user would have to go back to the original data to better understand the patterns.
So the text mining course or rather text mining and ana, analytics course will be deal, dealing with what to do once the user has found the information.
So this is a in this picture where we would convert the text data into action or knowledge.
And this has to do with helping users to go further digest with a found information or to find the patterns and to reveal knowledge buried in text and such knowledge can be used in application system to help decision-making or to help user finish a task.
So, if you have not taken that course the natural step and the natural next step would be to take that course.
Thank you for taking this course.
I hope you have found this course to be useful to you and I look forward to interacting with you at a future activity.
Text data is very special.
In contrast to the data captured by machines such as sensors, text data is produced by humans.
And they also are meant to be consumed by humans.
And this has some interesting consequences.
Because it is produced by humans, it tends to have a lot of useful knowledge about people's' preferences, people's' opinions about everything.
And that makes it possible to mine text data to discover those latent prefaces of people, which could be very useful to build an intelligent system to help people.
You can think about scientific literature or so and it's a way to encode our knowledge about the world.
So it's very high quality content, yet we have difficulty digesting all the content.
Now as a result of the fact that text is consumed by we humans, we also need intelligent software tools to help people digest the content, or otherwise we'd miss a lot of useful content.
This slide shows that the human really plays important role in test data mining.
We have to consider human in the loop, and we have to consider the fact that the text is generated by human.
So, here are some examples of useful text information systems.
This is by no means a complete list of all applications.
I categorize them into different categories.
But you can probably imagine other kinds of applications.
So let's take a look at some of them.
Search for example, we all know search engines is special.
Web search engines, iPad, all of you are using Google, or Bing, or another web search engine all the time.
And we also have live research assistants.
And in fact, wherever you have a lot of text data, you would have a search engine.
So for example, you might have a search box on your laptop.
All right, to search content on your computer.
So that's one kind of application systems, but we also have filtering systems or recommended systems.
Those systems can push information to users.
They can recommend useful information to users.
So again, use filters, spam filters.
Literature the movie recommenders.
Now not of them are necessary recommending the information to you.
For example email filter, spam email filter, this is actually to filter out the spams from your inbox, all right.
But in nature these are similar systems in that they have to make a binary decision regarding whether to retain a particular document or discard it.
Another kind of systems are categorization systems.
So for example, in handling emails, you might prefer automatic, sorter that would automatically sort incoming emails into a proper folders that you created.
Or we might want to categorize product reviews into positive or negative.
News agencies might be interested in categorizing news articles into all kinds of subject categories.
Those are all categorization systems.
Finally there are also systems that might do more analysis.
And oh, you can say mine text data.
And these can be text mining systems or information extraction systems, and they can be used to analyze text data in more detail to discover potentially useful knowledge.
For example companies might be interested in discovering major complaints from their customers based on the email messages that the, they have received from the customers.
All right, so having a system to support that would really help improve their productivity and the customer relations.
Also in business, intelligence companies are often interested in analyzing product reviews to understand the relative strengths of their own products in comparison with competitors.
And, and so these are all examples of these test mining systems.
[INAUDIBLE] we have a lot of data in particular literature data.
So, there's also great opportunity of using computer systems to analyze the data to automatically read literature, and to gain knowledge, and to help biologists make discoveries.
And you can imagine many others.
So the point is that with so much text data, we can build very useful systems to help people in many different ways.
Now how do we build this systems?
Well these actually are the main technologies that we'll be talking about in this course and the other course that I'm teaching for this specialization.
The main techniques for building these systems and also for harnessing the text data are text retrieval and text data mining.
So I use this picture to show the relation between these two some of the different techniques.
We started with big text data, right?
But for any applications, we don't necessarily need to use all the data.
Often we only need the small subset of the most relevant data, and that's shown here.
So text retrieval is to convert big, raw text data into that small subset of most relevant data that are most useful for a particular application.
And this is usually done by search engines.
And so this will be covered in this course.
After we have got a small amount of relevant data, we also need to further analyze the data to help people digest the data, or to turn the data into actionable knowledge.
And this step is called text mining, where we use a number of techniques to mine the data to get useful knowledge or pairings.
And the knowledge can then be used in many different applications.
And this part, text mining, will be covered in the other course that I'm teaching called Text Mining and Analytics.
The emphasis of this course is on basic concepts and practical techniques in text retrieval.
More specifically we will cover how search engines work.
How to implement a search engine.
How to evaluate a search engine, so that you know one search engine is better than another or one method is better than another.
How to improve and optimize a search engine system.
And how to build a recommender system.
We also hope to provide a hands on experience on multiple aspects.
One is to create a test collection for evaluating search engines.
This is very important for knowing which technique actually worked well.
And whether your search engine system is really good for your application.
The other aspect is to experiment with search engine algorithms.
In practice, you will have to face choices of different algorithms.
So it's important to know how to compare them and to figure out how they work or maybe potentially, how to improve them.
And finally, we'll provide a platform for you to do search engine competition.
Where you can compare your different ideas to see which idea works better on some data set.
The prerequisites for this course are minimum.
Basically we hope you have some basic concepts of computer science, for example data structures.
And we hope you will be comfortable with programming, especially in C++.
because that's the language that we'll use for some of the programming assignments.
The format is lectures plus quizzes, as often happens in MOOCs.
And we also will provide a program assignments for those of you that have the resources to do that.
We don't really have any required readings for this course.
That just means if you follow all the lecture videos carefully, and you're suppose to know all the basic concepts and the basic techniques.
But it's always useful to read more, so here we provide a list of some useful reference books.
And this in time order, and that also includes a book that and I are co-authoring now, and we make some draft chapters available on this website.
And we can find more readings and reference books on this website.
Finally, and this is the course schedule.
That's just the top of the map for the rest of the course, and it shows the topics that we will cover in the remaining lectures.
This picture also shows basic flow of information in a text information system.
So starting from the big text data, the first step is to do some natural language content analysis, because text data is in the form of natural language text.
So we need to understand the text to some extent in order to do something useful for the users.
So this is the first topic that we will cover.
And then on top of that as you can see there are two boxes here.
Those are two types of systems that can be used to help people get access to the most relevant data.
Or in other words, those are the two kinds of systems that will convert big text data into small relevant text data.
Search engines are helping users to search or to query the data to get the most relevant documents out.
Recommender systems are to recommend information to users, to push information to users.
So those are two, complementary was of getting users connected to the most relevant data at the right time.
So this part is called text access, and this will be the next topic.
And after we cover that we are going to cover a number of topics, all about the search engines.
Now the text access topic is a brief topic, a brief coverage of the two kinds of systems.
In the remaining topics, we'll cover search engines in much more detail.
That includes text retrieval problem, text retrieval methods, how to evaluate these methods, implementation of the system, and web search applications.
And after these, we're going to go cover the recommender system.
So this is what you expect in the rest of this course.
Thanks.
So here are some specific examples of what we can't do today and part of speech tagging is still not easy to do 100% correctly.
So in the example, he turned off the highway verses he turned off the fan and the two offs actually have somewhat a differentness in their active categories and also its very difficult to get a complete the parsing correct.
Again, the example, a man saw a boy with a telescope can actually be very difficult to parse depending on the context.
Precise deep semantic analysis is also very hard.
For example, to define the meaning of own, precisely is very difficult in the sentence, like John owns a restaurant.
So the state of the off can be summarized as follows.
Robust and general NLP tends to be shallow while a deep understanding does not scale up.
For this reason in this course, the techniques that we cover are in general, shallow techniques for analyzing text data and mining text data and they are generally based on statistical analysis.
So there are robust and general and they are in the in category of shallow analysis.
So such techniques have the advantage of being able to be applied to any text data in any natural about any topic.
But the downside is that, they don't give use a deeper understanding of text.
For that, we have to rely on deeper natural language analysis.
That typically would require a human effort to annotate a lot of examples of analysis that would like to do and then computers can use machine learning techniques and learn from these training examples to do the task.
So in practical applications, we generally combine the two kinds of techniques with the general statistical and methods as a backbone as the basis.
These can be applied to any text data.
And on top of that, we're going to use humans to, and you take more data and to use supervised machine learning to do some tasks as well as we can, especially for those important tasks to bring humans into the loop to analyze text data more precisely.
But this course will cover the general statistical approaches that generally, don't require much human effort.
So they're practically, more useful that some of the deeper analysis techniques that require a lot of human effort to annotate the text today.
So to summarize, the main points we take are first NLP is the foundation for text mining.
So obviously, the better we can understand the text data, the better we can do text mining.
Computers today are far from being able to understand the natural language.
Deep NLP requires common sense knowledge and inferences.
Thus, only working for very limited domains not feasible for large scale text mining.
Shallow NLP based on statistical methods can be done in large scale and is the main topic of this course and they are generally applicable to a lot of applications.
They are in some sense also, more useful techniques.
In practice, we use statistical NLP as the basis and we'll have humans for help as needed in various ways.
In this lecture we give an overview of Text Mining and Analytics.
First, let's define the term text mining, and the term text analytics.
The title of this course is called Text Mining and Analytics.
But the two terms text mining, and text analytics are actually roughly the same.
So we are not really going to really distinguish them, and we're going to use them interchangeably.
But the reason that we have chosen to use both terms in the title is because there is also some subtle difference, if you look at the two phrases literally.
Mining emphasizes more on the process.
So it gives us a error rate medical view of the problem.
Analytics, on the other hand emphasizes more on the result, or having a problem in mind.
We are going to look at text data to help us solve a problem.
But again as I said, we can treat these two terms roughly the same.
And I think in the literature you probably will find the same.
So we're not going to really distinguish that in the course.
Both text mining and text analytics mean that we want to turn text data into high quality information, or actionable knowledge.
So in both cases, we have the problem of dealing with a lot of text data and we hope to.
Turn these text data into something more useful to us than the raw text data.
And here we distinguish two different results.
One is high-quality information, the other is actionable knowledge.
Sometimes the boundary between the two is not so clear.
But I also want to say a little bit about these two different angles of the result of text field mining.
In the case of high quality information, we refer to more concise information about the topic.
Which might be much easier for humans to digest than the raw text data.
For example, you might face a lot of reviews of a product.
A more concise form of information would be a very concise summary of the major opinions about the features of the product.
Positive about, let's say battery life of a laptop.
Now this kind of results are very useful to help people digest the text data.
And so this is to minimize a human effort in consuming text data in some sense.
The other kind of output is actually more knowledge.
Here we emphasize the utility of the information or knowledge we discover from text data.
It's actionable knowledge for some decision problem, or some actions to take.
For example, we might be able to determine which product is more appealing to us, or a better choice for a shocking decision.
Now, such an outcome could be called actionable knowledge, because a consumer can take the knowledge and make a decision, and act on it.
So, in this case text mining supplies knowledge for optimal decision making.
But again, the two are not so clearly distinguished, so we don't necessarily have to make a distinction.
Text mining is also related to text retrieval, which is a essential component in many text mining systems.
Now, text retrieval refers to finding relevant information from a large amount of text data.
So I've taught another separate book on text retrieval and search engines.
Where we discussed various techniques for text retrieval.
If you have taken that book, and you will find some overlap.
And it will be useful To know the background of text retrieval of understanding some of the topics in text mining.
But, if you have not taken that book, it's also fine because in this book on text mining and analytics, we're going to repeat some of the key concepts that are relevant for text mining.
But they're at the high level and they also explain the relation between text retrieval and text mining.
Text retrieval is very useful for text mining in two ways.
First, text retrieval can be a preprocessor for text mining.
Meaning that it can help us turn big text data into a relatively small amount of most relevant text data.
Which is often what's needed for solving a particular problem.
And in this sense, text retrieval also helps minimize human effort.
Text retrieval is also needed for knowledge provenance.
And this roughly corresponds to the interpretation of text mining as turning text data into actionable knowledge.
Once we find the patterns in text data, or actionable knowledge, we generally would have to verify the knowledge.
By looking at the original text data.
So the users would have to have some text retrieval support, go back to the original text data to interpret the pattern or to better understand an analogy or to verify whether a pattern is really reliable.
So this is a high level introduction to the concept of text mining, and the relationship between text mining and retrieval.
Next, let's talk about text data as a special kind of data.
Now it's interesting to view text data as data generated by humans as subjective sensors.
So, this slide shows an analogy between text data and non-text data.
And between humans as subjective sensors and physical sensors, such as a network sensor or a thermometer.
So in general a sensor would monitor the real world in some way.
It would sense some signal from the real world, and then would report the signal as data, in various forms.
For example, a thermometer would watch the temperature of real world and then we report the temperature being a particular format.
Similarly, a geo sensor would sense the location and then report.
The location specification, for example, in the form of longitude value and latitude value.
A network sends over the monitor network traffic, or activities in the network and are reported.
Some digital format of data.
Similarly we can think of humans as subjective sensors.
That will observe the real world and from some perspective.
And then humans will express what they have observed in the form of text data.
So, in this sense, human is actually a subjective sensor that would also sense what's happening in the world and then express what's observed in the form of data, in this case, text data.
Now, looking at the text data in this way has an advantage of being able to integrate all types of data together.
And that's indeed needed in most data mining problems.
So here we are looking at the general problem of data mining.
And in general we would Be dealing with a lot of data about our world that are related to a problem.
And in general it will be dealing with both non-text data and text data.
And of course the non-text data are usually produced by physical senses.
And those non-text data can be also of different formats.
Numerical data, categorical, or relational data, or multi-media data like video or speech.
So, these non text data are often very important in some problems.
But text data is also very important, mostly because they contain a lot of symmetrical content.
And they often contain knowledge about the users, especially preferences and opinions of users.
So, but by treating text data as the data observed from human sensors, we can treat all this data together in the same framework.
So the data mining problem is basically to turn such data, turn all the data in your actionable knowledge to that we can take advantage of it to change the real world of course for better.
So this means the data mining problem is basically taking a lot of data as input and giving actionable knowledge as output.
Inside of the data mining module, you can also see we have a number of different kind of mining algorithms.
And this is because, for different kinds of data, we generally need different algorithms for mining the data.
For example, video data might require computer vision to understand video content.
And that would facilitate the more effective mining.
And we also have a lot of general algorithms that are applicable to all kinds of data and those algorithms, of course, are very useful.
Although, for a particular kind of data, we generally want to also develop a special algorithm.
So this course will cover specialized algorithms that are particularly useful for mining text data.
This lecture is about the discriminative classifiers for text categorization.
In this lecture we're going to continue talking about how to do text categorization and cover discriminative approaches.
This is a slide that you have seen from the discussion of Naive Bayes Classifier, where we have shown that although Naive Bayes Classifier tries to model the generation of text data, from each categories, we can actually use Bayes' rule to eventually rewrite the scoring function as you see on this slide.
And this scoring function is basically a weighted combination of a lot of word features, where the feature values are word counts, and the feature weights are the log of probability ratios of the word given by two distributions here.
Now this kind of scoring function can be actually a general scoring function where we can in general present text data as a feature vector.
Of course the features don't have to be all the words.
Their features can be other signals that we want to use.
And we mentioned that this is precisely similar to logistic regression.
So, in this lecture we're going to introduce some discriminative classifiers.
They try to model the conditional distribution of labels given the data directly rather than using Bayes' rule to compute that interactively as we have seen in naive Bayes.
So the general idea of logistic regression is to model the dependency of a binary response variable Y on some predictors that are denoted as X.
So here we have also changed the notation to X for future values.
You may recall in the previous slides we have used FI to represent the future values.
And here we use the notation of X factor, which is more common when we introduce such discriminative algorithms.
So, X is our input.
It's a vector with n features and each feature has a value x sub i here.
And I will go with a model that dependency of this binary response variable of these features.
So in our categorization problem when I have two categories theta 1 and theta 2, and we can use the Y value to denote the two categories when Y is 1, it means the category of the document, the first class, is theta 1.
Now, the goal here is the model, the conditional property of Y given X directly as opposed to model of the generation of X and Y as in the case of Naive Bayes.
And another advantage of this kind of approach is that it would allow many other features than words to be used in this vector since we're not modeling the generation of this vector.
And we can plug in any signals that we want.
So this is potentially advantageous for doing text categorization.
So more specifically, in logistic regression, assume the functional form of Y depending on X is the following.
And this is very closely related to the log odds that I introduced in the Naive Bayes or log of probability ratio of the two categories that you have seen on the previous slide.
So this is what I meant.
So in the case of Naive Bayes, we compute this by using those words and eventually we have reached a formula that looks like this.
But here we actually would assume explicitly that we with the model our probability of Y given X directly as a function of these features.
So, most specifically we assume that the ratio of the probability of Y equals 1 and the probability of Y equals All right, so it's a function of x and it's a linear combination of these feature values controlled by theta values.
And it seems we know that the probability of Y equals zero is one minus probability of Y equals one and this can be also written in this way.
So this is a log out ratio here.
And so in logistic regression, we're basically assuming that the probability of Y equals 1.
Okay my X is dependent on this linear combination of all these features.
So it's just one of the many possible ways, assuming that the dependency.
But this particular form has been quite useful and it also has some nice properties.
So if we rewrite this equation to actually express the probability of Y given X.
In terms of X by getting rid of the logarithm we get this functional form, and this is called a logistical function.
It's a transformation of X into Y, as you see on the right side here, so that the X's will be map into a range of values from 0 to 1.0, you can see.
And that's precisely what we want since we have a probability here.
And the function form looks like this.
So this is the basic idea of logistic regression.
And it's a very useful classifier that can be used to do a lot of classification tasks including text categorization.
So as in all cases of model we would be interested in estimating the parameters.
And in fact in all of the machine running programs, once you set up with the model, set up object and function to model the file, then the next step is to compute the parameter values.
In general, we're going to adjust to these parameter values.
Optimize the performance of classify on the training data.
So in our case just assume we have the training data here, xi and yi, and each pair is basically a future vector of x and a known label for that x. Y is either 1 or 0.
So in our case we are interested maximize this conditional likelihood.
The conditional likelihood here is basically to model why given observe the x, so it's not like a moderate x, but rather we're going to model this.
Note that this is a conditional probability of Y given X and this is also precisely what we wanted For classification.
Now so the likelihood function would be just a product of all the training cases.
And in each case, this is the model of the probability of observing this particular training case.
So given a particular Xi, how likely we are to observe the corresponding Yi?
Of course, Yi could be 1 or 0, and in fact, the function found here would vary depending on whether Yi is 1 or 0.
If it's a 1, we'll be taking this form.
And that's basically the logistic regression function.
But what about this, if it's 0?
Well, if it's 0, then we have to use a different form, and that's this one.
Now, how do we get this one?
Well, that's just a 1 minus the probability of Y=1, right?
And you can easily see this.
Now the key point in here is that the function form here depends on the observer Yi, if it's a 1, it has a different form than when it's 0.
And if you think about when we want to maximize this probability, we're basically going to want this probability to be as high as possible.
When the label is 1, that means the document is in probability 1.
But if the document is not, we're going to maximize this value, and what's going to happen is actually to make this value as small as possible because this sum's 1.
When I maximize this one, it's equivalent to minimize this one.
So you can see basically, if we maximize the conditional likelihood, we're going to basically try to make the prediction on the training data as accurate as possible.
So as another occasion, when you compute the maximum likelihood data, basically you'll find a beta value, a set of beta values that would maximize this conditional likelihood.
And this, again, then gives us a standard optimization problem.
In this case, it can be also solved in many ways.
Newton's method is a popular way to solve this problem, there are other methods as well.
But in the end, we will look at a set of data values.
Once we have the beta values, then we have a way to find the scoring function to help us classify a document.
So what's the function?
Well, it's this one.
See, if we have all the beta values, are they known?
All we need is to compute the Xi for that document and then plug in those values.
That will give us an estimated probability that the document is in category one.
Okay so, so much for logistical regression.
Let's also introduce another discriminative classifier called K-Nearest Neighbors.
Now in general, I should say there are many such approaches, and a thorough introduction to all of them is clearly beyond the scope of this course.
And you should take a machine learning course or read more about machine learning to know about them.
Here, I just want to include the basic introduction to some of the most commonly used classifiers, since you might use them often for text calculation.
So the second classifier is called K-Nearest Neighbors.
In this approach, we're going to also estimate the conditional probability of label given data, but in a very different way.
So the idea is to keep all the training examples and then once we see a text object that we want to classify, we're going to find the K examples in the training set and that are most similar to this text object.
Basically, this is to find the neighbors of this text objector in the training data set.
So once we found the neighborhood and we found the object that are close to the object we are interested in classifying, and let's say we have found the K-Nearest Neighbors.
That's why this method is called K-Nearest Neighbors.
Then we're going to assign the category that's most common in these neighbors.
Basically we're going to allow these neighbors to vote for the category of the objective that we're interested in classifying.
Now that means if most of them have a particular category and it's a category one, they're going to say this current object will have category one.
This approach can also be improved by considering the distance of a neighbor and of a current object.
Basically, we can assume a closed neighbor would have more say about the category of the subject.
So, we can give such a neighbor more influence on the vote.
And we can take away some of the votes based on the distances.
But the general idea is look at the neighborhood, and then try to assess the category based on the categories of the neighbors.
Intuitively, this makes a lot of sense.
But mathematically, this can also be regarded as a way to directly estimate there's a conditional probability of label given data, that is p of Y given X.
Now I'm going to explain this intuition in a moment, but before we proceed, let me emphasize that we do need a similarity function here in order for this to work.
Note that in naive base class five, we did not need a similarity function.
And in logistical regression, we did not talk about those similarity function either, but here we explicitly require a similarity function.
Now this similarity function actually is a good opportunity for us to inject any of our insights about the features.
Basically effective features are those that would make the objects that are on the same category look more similar, but distinguishing objects in different categories.
So the design of this similarity function is closely tied it to the design of the features in logistical regression and other classifiers.
So let's illustrate how K-NN works.
Now suppose we have a lot of training instances here.
And I've colored them differently and to show just different categories.
Now suppose we have a new object in the center that we want to classify.
So according to this approach, you work on finding the neighbors.
Now, let's first think of a special case of finding just one neighbor, the closest neighbor.
Now in this case, let's assume the closest neighbor is the box filled with diamonds.
And so then we're going to say, well, since this is in this object that is in category of diamonds, let's say.
Then we're going to say, well, we're going to assign the same category to our text object.
But let's also look at another possibility of finding a larger neighborhood, so let's think about the four neighbors.
In this case, we're going to include a lot of other solid field boxes in red or pink, right?
So in this case now, we're going to notice that among the four neighbors, there are three neighbors in a different category.
So if we take a vote, then we'll conclude the object is actually of a different category.
So this both illustrates how can nearest neighbor works and also it illustrates some potential problems of this classifier.
Basically, the results might depend on the K and indeed, k's an important parameter to optimize.
Now, you can intuitively imagine if we have a lot of neighbors around this object, and then we'd be okay because we have a lot of neighbors who will help us decide the categories.
But if we have only a few, then the decision may not be reliable.
So on the one hand, we want to find more neighbor, right?
And then we have more votes.
But on the other hand, as we try to find more neighbors we actually could risk on getting neighbors that are not really similar to this instance.
They might actually be far away as you try to get more neighbors.
So although you get more neighbors but those neighbors aren't necessarily so helpful because they are not very similar to the object.
So the parameter still has to be set empirically.
And typically, you can optimize such a parameter by using cross validation.
Basically, you're going to separate your training data into two parts and then you're going to use one part to actually help you choose the parameter k here or some other parameters in other class files.
And then you're going to assume this number that works well on your training that will be actually be the best for your future data.
So as I mentioned, K-NN can be actually regarded as estimate of conditional problem within y given x an that's why we put this in the category of discriminative approaches.
So the key assumption that we made in this approach is that the distribution of the label given the document probability a category given for example probability of theta i given document d is locally smooth.
And that just means we're going to assume that this probability is the same for all the documents in these region R here.
And suppose we draw a neighborhood and we're going to assume in this neighborhood since the data instances are very similar we're going to assume that the conditional distribution of the label given the data will be roughly the same.
If these are very different then we're going to assume that the probability of c doc given d would be also similar.
So that's a very key assumption.
And that's actually important assumption that would allow us to do a lot of machinery.
But in reality, whether this is true of course, would depend on how we define similarity.
Because neighborhood is largely determined by our similarity function.
If our similarity function captures objects that do follow similar distributions then these assumptions are okay but if our similarity function could not capture that, obviously these assumption would be a problem and then the classifier would not be accurate.
Okay, let's proceed with these assumption.
Then what we are saying is that, in order to estimate the probability of category given a document.
We can try to estimate the probability of the category given that entire region.
Now, this has a benefit, of course, of bringing additional data points to help us estimate this probability.
And so this is precisely the idea of K-NN.
Basically now we can use the known categories of all the documents in this region to estimate this probability.
And I have even given a formula here where you can see we just count the topics in this region and then normalize that by the total number of documents in the region.
So the numerator that you see here, c of theta i and r, is a counter of the documents in region R was category theta i.
Since these are training document and we know they are categories.
We can simply count how many times it was since here.
How many times we have the same signs, etc.
And then the denominator is just the total number of training documents in this region.
So this gives us a rough estimate of which categories most popular in this neighborhood.
And we are going to assign the popular category to our data object since it falls into this region.
.
In this lecture, we're going to talk about how to improve the instant changing of the Vector Space Model.
This is the continued discussion of the Vector Space Model.
We're going to focus on how to improve the instant changing of this model.
In a previous lecture, you have seen that with simple situations of the Vector Space Model, we can come up with a simple scoring function that would give us, basically, a count of how many unique query terms are matching the document.
We also have seen that this function has a problem as shown on this slide.
In particular, if you look at these three documents, they will all get the same score because they match the three unique query words.
But intuitively we would like, d4 to be ranked above d3.
And d2 is really non relevant.
So the problem here is that this function couldn't capture the following characteristics.
First, we would like to give more gratitude to d4 because it matches the presidential more times than d3.
Second, intuitively matching presidential should be more important than matching about, because about is a very common word that occurs everywhere.
It doesn't really carry that much content.
So, in this lecture, let's see how we can improve the model to solve these two problems.
It's worth thinking at this point about why do we have these four problems.
If we look back at the assumptions we have made while substantiating the Vector Space Model, we will realize that the problem is really coming from some of the assumptions.
In particular, it has to do with how we place the vectors in the vector space.
So then, naturally, in order to fix these problems, we have to revisit those assumptions.
Perhaps, you will have to use different ways to instantiate the Vector Space Model.
In particular, we have to place the vectors in a different way.
So, let's see how can we prove this?
Well, our natural thought is in order to consider multiple times of a term in a document.
We should consider the term frequency instead of just the absence or presence.
In order to consider the difference between a document where a query term occurred multiple times and the one where the query term occurred just once.
We have to concede a term frequency, the count of a term being in the document.
In the simplest model, we only model the presence and absence of a term.
We ignore the actual number of times that a term occurs in a document.
So let's add this back.
So we're going to do then represent a document by a vector with term frequency as element.
So, that is to say, now, the elements of both the query vector and the document vector will not be zero once, but instead there will be the counts of a word in the query or the document.
So this would bring additional information about the document.
So this can be seen as a more accurate representation of our documents.
So, now let's see what the formula would look like if we change this representation.
So as you see on this slide, we still use that product, and, so the formula looks very similar in the form.
In fact, it looks identical, but inside of the sum of cos xi and yi are now different.
They're now the counts of words i in the query and the document.
Now at this point, I also suggest you to pause the lecture for moment and just we'll think about how we have interpret the score of this new function.
It's doing something very similar to what the simplest VSM is doing.
But because of the change of the vector, now the new score has a different interpretation.
Can you see the difference?
And it has to do with the consideration of multiple occurrences of the same time in the document.
More importantly, we''ll try to know whether this would fix the problem of the simplest vector space model.
So, let's look at the this example again.
So suppose, we change the vector to term frequency vectors.
Now, let's look at these three documents again.
The query vector is the same because all these words occurred exactly once in the query.
So the vector is still 0 1 vector.
And in fact, d2 is also essential in representing the same way because none of these words has been repeated many times.
As a result, the score is also the same, still three.
The same issue for d3 and we still have a 3.
But d4 would be different, because now, presidential occurred twice here.
So the end in the four presidential in the [INAUDIBLE] would be 2 instead of 1.
As a result, now the score for d4 is higher.
It's a four now.
So this means, by using term frequency, we can now rank d4 above d2 and d3 as we hope to.
So this solve the problem with default.
But, we can also see that d2 and d3 are still featured in the same way.
They still have identical scores, so it did not fix the problem here.
So, how can we fix this problem?
We would like, to give more credit for matching presidential than matching about.
But how can we solve the problem in a general way?
Is there any way to determine which word should be treated more importantly and which word can be, basically ignored.
About is such a word.
And which it does not really carry that much content, we can essentially ignore that.
We sometimes call such a word, a stock word.
Those are generally very frequent and they occur everywhere, matching it, doesn't really mean anything.
But computation how can we capture that?
So again, I encourage you to think a little bit about this.
Can you come up with any statistical approaches to somehow distinguish presidential from about.
If you think about it for a moment, you realize that, one difference is that a word like above occurs everywhere.
So if you count the currents of the water in the whole collection that we would see that about as much higher for this than presidential, which it tends to occur only in some documents.
So this idea suggests that we could somehow use the global statistics of terms or some other formation to try to down weight the element for about in the vector representation of d2.
At the same time, we hope to somehow increase the weight of presidential in the vector of d3.
If we can do that, then, we can expect that d2 will get the overall score to be less than three, while d3 will get the score about three.
Then, we'll be able to rank d3 on top of d2.
So how can we do this systematically?
Again, we can rely on some steps that people count.
And in this case, the particular idea is called the Inverse Document Frequency.
We have seen document frequency.
As one signal used in, the moding retrieval functions.
We discussed this in a previous lecture.
So here's the specific way of using it.
Document frequency is the count of documents that contain a particular term.
Here, we say inverse document frequency because we actually want to reword a word that doesn't occur in many documents.
And so, the way to incorporate this into our vector [INAUDIBLE] is then to modify the frequency count by multiplying it by the idea of the corresponding word as shown here.
If we didn't do that, then we can penalize common words which generally have a low idea of, and reward real words, which we're have a higher IDF.
So most specific [INAUDIBLE] IDF can be defined as the logarithm of M plus one divided by k, where M is the total number of documents in the collection,k is df or document frequency.
The total number of documents containing the word W. Now, if you plot this function by varying k, then you will see the curve that look like this.
In general, you can see it would give a higher value for a low DF word, a rare word.
You can also see the maximum value of this function is log of M plus 1.
Will be interesting for you to think about what's minimum value for this function?
This could be interesting exercise.
Now, the specific function may not be as important as the heuristic to simply penalize popular terms.
But it turns out this particular function form has also worked very well.
Now, whether there is a better form of function here, is the open research question.
But, it's also clear that if we use a linear kernalization like what's shown here with this line, then, it may not be as reasonable as the standard IDF.
In particular, you can see the difference in the standard IDF, and we, somehow have a [INAUDIBLE] point here.
After this point, we're going to say these terms are essentially not very useful.
They can be essentially ignored.
And this makes sense when the term occurs so frequently, and let's say a term occurs in more than 50% of the documents.
Then the term is unlikely very important and it's, it's basically, a common term.
It's not very important to match this word, so with the standard IDF, you can see it's, basically, assumed that they all have lower weights, there's no difference.
But if you look at the linear kernelization, at this point there is, there's some difference.
So intuitively, we want to focus more on the discrimination of low DF words, rather than these common words.
Well, of course, which one works better, still has to be validated by using the empirically related data set.
And we have to use users to judge which results of that.
So now let's see how this can solve problem two.
So now, let's look at the two documents again.
Now without IDF weighting, before, we just have [INAUDIBLE] vectors, but with IDF weighting we now can adjust the DF weight by multiplying the, with the IDF value.
For example here, you can see is the adjustment in particular for about, there is an adjustment by using the IDF value of about which is smaller than the IDF value of presidential.
So if you look at these, the IDF will distinguish these two words.
As a result, adjustment here would be larger, would make this weight larger.
So if we score with these new vectors, and what would happen is that the, of course, they share the same weights for news and the campaign, but the margin of about and presidential with this grouping may.
So now as a result of IDF weighting, we will have d3 to be ranked above d2.
Because it matched rail word, where as d2 matched common word.
So this shows that the idea of weighting can solve problem two.
So, how effective is this model in general when we use TF-IDF weighting?
Well, let's look at all these documents that we have seen before.
These are the new scores of the new documents.
But how effective is this new weighting method and new scoring function, all right?
So now let's see overall how effective is this new ranking function with TF-IDF Weighting?
Here, we show all the five documents that we have seen before, and these are their scores.
Now, we can see the scores for the first four documents here seem to be quite reasonable.
They are as we expected.
However, we also see a new problem.
Because now d5, here, which did not have a very high score with our simplest vector space model.
Now, after it has a very high score.
In fact, it has the highest score here.
So, this creates a new problem.
This actually a common phenomenon in designing material functions.
Basically, when you try to fix one problem, you tend to introduce other problems.
And that's why it's very tricky how to design effective ranking function.
And what's what's the best ranking function is the open research question.
Researchers are still working on that.
But in the next few lecture, we're going to also talk about some additional ideas to further improve this model and try to fix this problem.
So to summarize this lecture, we've talked about how to improve this vector space model.
And we've got to improve the [INAUDIBLE] of the vector space model based on TF-IDF weighting.
So the improvement, most of it, is on the placement of the vector.
Where we give higher weight to a term that occurred many times in the document, but infrequently in the whole collection.
And we have seen that this improved model indeed works better than the simplest vector space model, but it also still has some problems.
In the next lecture, we're going to look at the how to address these additional problems.
So now let's take a look at the specific, method that's based on regression.
Now this is one of the many different methods in fact, it's the one of the simplest methods.
And I choose this to explain the idea because it's it's so simple.
So in this approach we simply assume that the relevance of a document with respect to the query, is related to a linear combination of all the features.
Here I used the Xi to emote the feature.
So Xi of Q and D is a feature.
And we can have as many features as, we would like.
And we assume that these features can be combined in a linear manner.
And each feature is controlled by a parameter here.
And this beta is a parameter, that's a weighting parameter.
A larger value would mean the feature would have a higher weight and it would contribute more to the scoring function.
The specific form of the function actually also involves a transformation of the probability of relevance.
So this is the probability of relevance.
We know that the probability of relevance is within the range from 0 to 1.
And we could have just assumed that the scoring function is related to this linear combination.
Right, so we can do a, a linear regression but then the value of this linear combination could easily go beyond 1.
So this transformation here would map ze, range of real values.
You can, you can verify it, it by yourself.
So this allows us then to connect to the probability of relevance which is between 0 and 1 to a linear combination of arbitrary efficients.
And if we rewrite this into a probability function, we will get the next one.
So on this side on this equation, we will have the probability of relevance.
And on the right hand side, we will have this form.
Now this form is created non-active.
And it still involves the linear combination of features.
And it's also clear that is, if this value is, is.
Of the linear combination in the equation above.
If this this, this value here, if this value is large then it will mean this value is small.
And therefore, this probability, this whole probability, would be large.
And that's what we expect.
Basically, it would be if this combination gives us a high value, then the document's more likely relevant.
So this is our hypothesis.
Again, this is not necessarily the best hypothesis.
That this is a simple way to connect these features with the probability of relevance.
So now we have this this combination function.
The next task is to see how we need to estimate the parameters so that the function can truly be applied.
Right.
Without them knowing that they have values, it's, it's harder to apply this function, okay.
So let's how we can estimate, beta values.
All right.
Let's take a look, at a simple example.
In this example, we have three features.
One is BM25 score of the document under the query.
One is the page rank score of the document, which might or might not depend on the query.
Hm, we might have a top sensitive page rank.
That would depend on the query.
Otherwise, the general page rank doesn't really depend on the query.
And then we have BM25 score on the Anchor task of the document.
These are then the feature values for a particular doc, document query pair.
And in this case the document is D1.
And the, the judgment says that it's relevant.
Here's another training instance, and these features values.
But in this case it's non-relevant, okay?
This is a overly simplified case, where we just have two instances.
But it, it's sufficient to illustrate the point.
So what we can do is we use the maximum likelihood estimator to actually estimate the parameters.
Basically, we're going to do, predict the relevance status of the document, the, based on the feature values.
That is given that we observe these feature values here.
Can we predict the relevance?
Yeah.
And of course, the prediction will be using this function that you see here.
And we hypothesize this that the probability of relevance is related features in this way.
So we're going to see for what values of beta we can predict that the relevance well.
What do we mean?
Well, what, what do we mean by predicting the relevance well?
Well we just mean.
In the first case for D1, this expression here, right here, should give higher values.
In fact, they would hope this to give a value close to one.
Why?
Because this is a relevant document.
On the other hand, in the second case for D2 we hope this value would be small.
Right.
Why?
It's because it's a non-relevant document.
So now let's see how this can be mathematical expressed.
And this is similar to, expressing the probability of a document.
Only that we are not talking about the probability of words but talking about the probability of relevance, 1 or 0.
So what's the probability of this document?
The relevant if it has these feature values.
Well this is.
Just this expression, right?
We just need to pluck in the X, the Xis.
So that's what we'll get.
It's exactly like, what we have seen that, only that we replace these Xis.
With now specific values.
And so, for example, this 0.7 goes to here and this 0.11 goes to here.
And these are different feature values and we'll combine them in this particular way.
The beta values are still unknown.
But this gives us the probability that this document is relevant if we assume such a model.
Okay, and we want to maximize this probability since this is a random document.
What we do for the second document.
Well, we want to compute to the probability that the predictions is, is n, non-relevant.
So, this would mean, we have to compute a 1 minus, right this expression.
Since this expression.
Is actually the probability of relevance, so to compute the non relevance from relevance, we just do 1 minus the probability of relevance, okay?
So this whole expression then.
Just is our probability of predicting these two relevance values.
One is 1.
Here, one is a 0.
And this whole equation is our probability.
Of observing a 1 here and observing a 0 here.
Of course this probability depends on the beta values, right?
So then our goal is to adjust the beta values to make this whole thing reach its maximum.
Make that as large as possible.
So that means we are going to compute this.
The beta is just the, the parameter values that would maximize this for like holder expression.
And what it means is if look at the function is we're going to choose betas to make this as large as possible.
And make this also as large as possible which is equivalent to say make this the part as small as possible.
And this is precisely what we want.
So once we do the training, now we will know the beta values.
So then this function will be well defined once their values are known.
Both this and this will become pretty less specified.
So for any new query and new document we can simply compute the features For that pair and then we just use this formula to generate a ranking score.
And this scoring function can be used in for rank documents for a particular query.
So that's the basic idea of, learning to rank.
Hello.
Welcome to the course Text Mining and Analytics.
My name is ChengXiang Zhai.
I have a nickname, Cheng.
I am a professor of the Department of Computer Science at the University of Illinois at Urbana-Champaign.
This course is a part of a data mining specialization offered by the University of Illinois at Urbana-Champaign.
In addition to this course, there are four other courses offered by Professor Jiawei Han, Professor John Hart and me, followed by a capstone project course that all of us will teach together.
This course is particularly related to another course in the specialization, mainly text retrieval and search engines in that both courses are about text data.
In contrast, pattern discovery and cluster analysis are about algorithms more applicable to all kinds of data in general.
The visualization course is also relatively general in that the techniques can be applied to all kinds of data.
This course addresses a pressing need for harnessing big text data.
Text data has been growing dramatically recently, mostly because of the advance of technologies deployed on the web that would enable people to quickly generate text data.
So, I listed some of the examples on this slide that can show a variety of text data that are available today.
For example, if you think about the data on the internet, on the web, everyday we are seeing many web pages being created.
Blogs are another kind of new text data that are being generated quickly by people.
Anyone can write a blog article on the web.
New articles of course have always been a main kind of text data that being generated everyday.
Emails are yet another kind of text data.
And literature is also representing a large portion of text data.
It's also especially very important because of the high quality in the data.
That is, we encode our knowledge about the word using text data represented by all the literature articles.
It's a vast amount of knowledge of all the text and data in these literature articles.
Twitter is another representative text data representing social media.
Of course there are forums as well.
People are generating tweets very quickly indeed as we are speaking perhaps many people have already written many tweets.
So, as you can see there are all kinds of text data that are being generated very quickly.
Now these text data present some challenges for people.
It's very hard for anyone to digest all the text data quickly.
In particular, it's impossible for scientists to read all of the for example or for anyone to read all the tweets.
So there's a need for tools to help people digest text data more efficiently.
There is also another interesting opportunity provided by such big text data, and that is it's possible to leverage the amount of text data to discover interesting patterns to turn text data into actionable knowledge that can be useful for decision making.
So for example, product managers may be interested in knowing the feedback of customers about their products, knowing how well their products are being received as compared with the products of competitors.
This can be a good opportunity for leveraging text data as we have seen a lot of reviews of product on the web.
So if we can develop a master text mining techniques to tap into such a [INAUDIBLE] to extract the knowledge and opinions of people about these products, then we can help these product managers to gain business intelligence or to essentially feedback from their customers.
In scientific research, for example, scientists are interested in knowing the trends of research topics, knowing about what related fields have discovered.
This problem is especially important in biology research as well.
Different communities tend to use different terminologies, yet they're starting very similar problems.
So how can we integrate the knowledge that is covered in different communities to help study a particular problem?
It's very important, and it can speed up scientific discovery.
So there are many such examples where we can leverage the text data to discover useable knowledge to optimize our decision.
The main techniques for harnessing big text data are text retrieval and text mining.
So these are two very much related technologies.Yet, they have somewhat different purposes.
These two kinds of techniques are covered in the tool in this specialization.
So, text retrieval on search engines covers text retrieval, and this is necessary to turn big text data into a much smaller but more relevant text data, which are often the data that we need to handle a particular problem or to optimize a particular decision.
This course covers text mining which is a second step in this pipeline that can be used to further process the small amount of relevant data to extract the knowledge or to help people digest the text data easily.
So the two courses are clearly related, in fact, some of the techniques are shared by both text retrieval and text mining.
If you have already taken the text retrieval course, then you might see some of the content being repeated in this text mining course, although we'll be talking about the techniques from a very different perspective.
If you have not taken the text retrieval course, it's also fine because this course is self-contained and you can certainly understand all of the materials without a problem.
Of course, you might find it beneficial to take both courses and that will give you a very complete set of skills to handle big text data.
This lecture is about how to evaluate the text retrieval system when we have multiple levels of judgments.
In this lecture we will continue the discussion of evaluation.
We're going to look at how to evaluate the text retrieval system.
And we have multiple level of judgements.
So, so far we have talked about binding judgements, that means a documents is judged as being relevant or not-relevant.
But earlier we will also talk about, relevance as a matter of degree.
So we often can distinguish it very higher relevant options, those are very useful options, from you know, lower rated relevant options.
They are okay, they are useful perhaps.
And further from non-relevant documents.
Those are not useful.
Right?
So imagine you can have ratings for these pages.
Then you would have much more levels of ratings.
For example, here I show an example of three levels, three were relevant.
Sorry, three were very relevant.
Two for marginally relevant and one for non-relevant.
Now how do we evaluate such a new system using these judgements of use of the map doesn't work, average of precision doesn't work, precision and record doesn't work because they rely on vinyl judgement.
So let's look at the sum top regular results when using these judgments.
Right?
Imagine the user would be mostly care about the top ten results here.
Right.
And we mark the the rating levels or relevance levels for these documents as shown here.
Three, two, one, one, three, et cetera.
And we call these gain.
And the reason why we call it a gain, is because the measure that we are infusing is called, NTCG, normalizer discount of accumulative gain.
So this gain basically can mesh your, how much gain of random information a user can obtain by looking at each document, alright.
So looking after the first document the user can gain three points.
Looking at the non-relevant document the user would only gain one point.
Right.
Looking at the multi-level relevant or marginally relevant document the user would get two points et cetera.
So this gain usually matches the utility of a document from a user's perspective.
Of course if we assume the user stops at the ten documents, and we're looking at the cutoff at ten we can look after the total gain of the user.
And what's that, well that's simply the sum of these and we call it the cumulative gain.
So if we use a stops at the positua that's just a three.
If the user looks at another document that's a 3 plus 2.
If the user looks at the more documents.
Then the cumulative gain is more.
Of course, this is at the cost of spending more time to examine the list.
So cumulative gain gives us some idea about how much total gain the user would have if the user examines all these documents.
Now, in NDCG, we also have another letter here, D, discounted cumulative gain.
So why do we want to do discounting?
Well, if you look at this cumulative gain, there is one deficiency which is it did not consider the rank position of these these documents.
So, for example looking at the, this sum here and we only know there is only one highly relevant document, one marginally relevant document, two non-relevant documents.
We don't really care where they are ranked.
Ideally, we want these two to be ranked on the top.
Which is the case here.
But how can we capture that intuition?
Well we have to say, well this 3 here is not as good as this 3 on the top.
And that means the contribution of, the game from different positions, has to be weight by their position.
And this is the idea of discounting, basically.
So, we're going to say, well, the first one, doesn't it need to be discounted because the user can be assume that you always see this document, but the second one, this one will be discounted a little bit, because there's a small possibility that the user wouldn't notice it.
So, we divide this gain by the weight, based on the position.
So, log of two, two is the rank position of this document and, when we go to the third position, we, discount even more because the numbers is log of three, and so on and so forth.
So when we take a such a sum then a lowly ranked document would not contribute contribute that much as a highly ranked document.
So that means if you, for example, switch the position of this and let's say this position and this one, and then you would get more discount if you put for example, very relevant document here as opposed to two here.
Imagine if you put the three here, then it would have to be discounted.
So it's not as good as if you would put the three here.
So this is the idea of discounting.
Okay, so n, now at this point that we have got this discounted cumulative gain for measuring the utility of this ranked list with multiple levels of judgments.
So are we happy with this?
Well we can use this rank systems.
Now we still need to do a little bit more in order to make this measure comfortable across different topics.
And this is the last step.
And by the way, here we just show that DCG at the ten.
Alright.
So this is the total sum of DCG over all these ten documents.
So the last step is called N, normalization.
And if we do that then we get normalized DCG.
So how do we do that?
Well, the idea here is within the Normalized DCG by the Ideal DCG at the same cutoff.
What is the Ideal DCG?
Well this is a DCG of ideal ranking.
So imagine if we have nine documents in the whole collection rated a three here and that means in total we have nine documents rated three.
Then, our ideal ranked the Lister would have put all these nine documents on the very top.
So all these would have to be three and then this would be followed by a two here, because that's the best we could do after we have run out of threes.
But all these positions would be threes.
Right?
So this would be our ideal ranked list.
And then we can compute the DCG for this ideal rank list.
So this would be given by this formula you see here, and so this idea DCG would be used as the normalizer DCG.
Like so here, and this IdealDCG would be used as a normalizer.
So you can imagine now normalization essentially is to compare the actual DCG with the best decision you can possibly get for this topic.
Now why do we want to do this?
Well by doing this we'll map the DCG values in to a range of zero through one, so the best value, or the highest value for every query would be one.
That's when you're relevance is in fact the idealist.
But otherwise in general you will be lower than one.
Now what if we don't do that?
Well, you can see this transformation or this numberization, doesn't really affect the relative comparison of systems for just one topic, because this ideal DCG is the same for all the systems.
So the ranking of systems based on only DCG would be exactly the same.
As if you rank them based on the normalized decision.
The difference however is when we have multiple topics because if we don't do normalization, different topics will have different scales of DCG.
For a topic like this one we have nine highly relevant documents.
The DCG can get really high.
But imagine that in another case there are only two very relevant documents.
In total in the whole collection.
Then the highest DCG that any system could achieve for such a topic would not be very high.
So again we face the problem of different scales of DCG values and when we take an average we don't want the average to be dominated by those high values.
Those are again easy quires.
So by doing the normalization we have all, avoid the problem.
Making all the purists contribute equal to the average.
So this is the idea of NDCG.
It's used for measuring relevance based on much more level relevance judgments.
So more in the more general way, this is basically a measure that can be applied through any ranked task with much more level of, of judgments.
And the scale of the judgments can be multiple can be more than binary, not only more than binary, they can be multiple levels, like one's or five, or even more depending on your application.
And the main idea of this measure just to summarize, is to measure the total utility of the top k documents.
So you always choose a cutoff, and then you measure the total utility.
And it would discount the contribution from a lowly ranked document, and finally, it would do normalization to ensure comparability across queries
This lecture is about evaluation of text retrieval systems.
In the previous lectures, we have talked about a number of text retrieval methods.
Different kinds of ranking functions.
But how do we know which one works the best?
In order to answer this question, we have to compare them, and that means we'll have to evaluate these retrieval methods.
So this is the main topic of this lecture.
First, let's think about why do we have to do evaluation?
I already gave one reason.
And that is, we have to use evaluation to figure out which retrieval method works better.
Now this is very important for advancing our knowledge.
Otherwise we wouldn't know whether a new idea works better than old idea.
In the beginning of this course we talked about the, the problem of text retrieval we compare it with database retrieval.
There, we mentioned that text retrieval is imperative to find the problem.
So, evaluation must rely on users, which system works better, that would have to be judged by our users.
So this becomes very challenging problem.
Because how can we get users involved in, in matters, and how can we draw a fair comparison of different methods.
So just go back to the reasons for evaluation.
I listed two reasons here.
The second reason is basically what I just said but there is also another reason, which is to assess the actual utility of a test regional system.
Now imagine you're building your own applications.
Would be interested in knowing how well your search engine works for your users.
So in this case measures must reflect the utility to the actual users in the the real application.
And typically, this has been done by using user studies and using the real search engine.
In the second case or for the second reason, the measures actually all need to be correlated with the utility to actual users.
Thus they don't have to accurately reflect the, the exact utility to users.
So the measure only needs to be good enough to tell which method works better.
And this is usually done through test collection.
And this is the main idea that we'll be talking about in this course.
This has been very important for comparing different algorithms and for improving search engines systems in general.
So next we will talk about what to measure.
There are many aspects of a search engine we can measure, we can evaluate and here I list the three major aspects.
One is effectiveness or accuracy, how accurate are the search results?
In this case we're measuring a system's capability of ranking relevant documents on top of non relevant ones.
The second is efficiency.
How quickly can a user get the results?
How much computing resources are needed to answer a query?
So in this case we need to measure the space and time overhead of the system.
The third aspect is usability.
Basically the question is how useful is the system for real user tasks?
Here, obviously, interfaces and many other things are also important and we typically would have to do user studies.
Now, in this course, we're going to talk more, mostly about the effectiveness and accuracy measures because, the efficiency and usability dimensions are, not really unique to search engines, and so, they are, needed for evaluating any other software systems.
And there is also good coverage of such materials in other courses.
But how to evaluate a search engine is quite, you know accuracy is something you need to text retrieval, and we're going to talk a lot about this.
The main idea that people have proposed before using a attitude, evaluate a text retrieval algorithm, is called the Cranfield Evaluation Methodology.
This one actually was developed long time ago, developed in the 1960s.
It's a methodology for laboratory test of system components, it's actually a methodology that has been very useful, not just for search engine evaluation.
But also for evaluating virtually all kinds of empirical tasks.
And, for example in processing or in other fields where the problem is empirically defined we typically would need to use to use such a methodology.
And today was the big data challenge with the use of machine learning every where.
We general, this methodology has been very popular, but it was first developed for search engine application in the 1960s.
So the basic idea of this approach is it'll build a reusable test collections and define measures.
Once such a test collection is build it can be used again and again to test the different algorithms.
And we're going to define measures that would allow you to quantify performance of a system or an, an algorithm.
So how exactly would this work?
Well, we're going to do, have assembled collection of documents and this is just similar to real document collection in your search application.
We can also have a sample set of queries or topics.
This is to simulate the user's queries.
Then we'll have to have relevance judgments.
These are judgments of which documents should be returned for which queries.
Ideally, they have to made by users who formulated the queries because those are the people that know exactly what documents would be used for.
And then finally we have to have measures to quantify how well a system's result matches the ideal ranked list.
That would be constructed and based on users' relevant judgements.
So this methodology is very useful for starting retrieval algorithms because the test can actually, can be reused many times.
And it will also provide a fair comparison for all the methods.
We have the same criteria, same data set to use and to compare different algorithms.
This allows us to compare a new algorithm with an old algorithm, that was the method of many years ago.
By using the same standard.
So this is the illustration of how this works, so as I said, we need a queries that are shown here.
We have Q1, Q2, et cetera.
We also need a documents, and that's called the document collection, and on the right side, you see we need relevance judgment.
These are basically the binary judgments of documents with respect to a query.
So, for example D1 is judged as being relevant to Q1, D2 is judged as being relevant as well.
And D3 is judged as non relevant in the two, Q1, et cetera.
These would be created by users.
Once we have these, and we basically have a test, correction, and then, if you have two systems, you want to, compare them.
Then you can just run each system on these queries and documents and each system will then return results.
Let's say if the query is Q1 and then we would have the results here, here I show R sub A as results from system A.
So, this is remember we talked about task of computing approximation of the, relevant document setter.
So A is, the system A's approximation here, and also B is system B's approximation of relevant documents.
Now let's take a look at these results.
So which is better?
Now imagine for a user which one would you like?
All right lets take a look at both results.
And there are some differences and there are some documents that are return to both systems.
But if you look at the results you will feel that well, maybe an A is better in the sense that we don't have many number in documents.
And among the three documents returned the two of them are relevant, so that's good, it's precise.
On the other hand can also say maybe B is better because we've got more relevant documents, we've got three instead of two.
So which one is better and how do we quantify this?
Well obviously, this question highly depends on a user's task.
And, it depends on users as well.
You might be able to imagine, for some users may be system made is better.
If the user is not interested in getting all the relevant documents, right, in this case this is the user doesn't have to read.
User would see most relevant documents.
On the other hand on one count, imagine user might need to have as many relevant documents as possible, for example, taking a literature survey.
You might be in the second category, and then you might find that system B's better.
So in either case, we'll have to also define measures that would quantify them.
And we might need to define multiple measures because users have different perspectives of looking at results.
So we talked about a page rank as a way to to capture the Authorities.
Now we also looked at the, some other examples where a hub might be interesting.
So, there is another algorithm called the HITS and that's going to do compute the scores for us.
Authorities & Hubs.
Intuitions of, pages that are widely cited, good, sorry, there is, then, there is pages that are cited.
Many other pages are good Hubs, right?
But there, I think that the.
Most interesting idea of this algorithm HITS is, it's going to use, a reinforcement mechanism to kind of help improve the scoring for Hubs and the Authorities.
And here, so here's the idea, it will assume that good authorities are cited by good hubs.
That means if you're cited by many pages with good hub scores, then that increases your authority score.
And similarly, good hubs are those that pointed to good authorities.
So if you get you point it to a lot of good authority pages, then your hub score would be increased.
So you then, you would have iterative reinforce each other, because you can point it to some good hubs.
Sorry, you can point it to some good authorities.
To get a good hub score.
Whereas those authority scores, would be also improved, because they are pointed to by a good hub.
And this hub is also general, it can have many applications in graph and network analysis.
So just briefly, here's how it works.
We first also construct the matrix, but this time we're going to construct the Adjacency matrix.
We're not going to normalize the values, so if there's a link there's a y.
If there's no link that's zero.
Right again, it's the same graph and then, we're going to define the top score of page as a sum of the authority scores of all the pages that it appoints to.
So whether you are hub that really depends on whether you are pointing to a lot of, good authority pages.
That's what it says in the first equation.
Your second equation, will define the authority score of a page as a sum of the hub scores of all those pages.
That they point to, so whether you are a good authority would depend on whether those pages that are pointing to you are good Hubs.
So you can see this a forms a iterative reinforcement mechanism.
Now these two equations can be also written.
In the matrix fo-, format.
Right, so what we get here is then the hub vector is equal to the product of the Adjacency matrix.
And the authority vector.
And this is basically the first equation.
Right.
And similarly, the second equation can be returned as the authority vector is equal to the product of A transpose multiplied by the hub vector.
And these are just different ways of expressing these equations.
But what's interesting is that if you look at to the matrix form.
You can also plug-in the authority equation into the first one.
So if you do that, you can actually make it limited to the authority vector completely, and you get the equation of only hub scores.
Right, the hub score vector is equal to A multiplied by A transpose.
Multiplied by the hub score vector again.
And similarly we can do a transformation to have equation for just the authorities scores.
So although we framed the problem as computing Hubs & Authorities, we can actually eliminate the one of them to obtain equation just for one of them.
Now the difference between this and page is that, now the matrix is actually a multiplication of the mer-, Adjacency matrix and its transpose.
So this is different from page rank.
Right?
But mathematically then we would be computing the same problem.
So in ha, in hits, we're keeping would initialize the values that state one for all these values.
And then with the algorithm will apply these, these equations essentially and this is equivalent if you multiply that.
By, by the matrix.
A and A transpose.
Right.
And so the arrows of these are exactly the same in the debate rank.
But here, because the Adjacency matrix is not normalized, so what we have to do is to, what we have to do is after each iteration we have to do normalize.
And this would allow us to control the grooves of value.
Otherwise they would, grew larger and larger.
And if we do that, and then we will basically get a, HITS.
I was in the computer, the hub scores and also the scores for all of the pages.
And these scores can then be used, in ranging to start the PageRank scores.
So to summarize, in this lecture we have seen that link information is very useful.
In particular, the Anchor text base is very useful.
To increase the the text representation of a page.
And we also talk about the PageRank and HITS algorithm as two major link analysis algorithms.
Both can generate scores for.
What pages that can be used for the, the ranking function.
Those that PageRank and the HITS also very general algorithms, so they have many applications in analyzing other graphs or networks.
.
This lecture is about web indexing.
In this lecture, we will continue talking about web search, and we're going to talk about how to create a web scale index.
So once we crawl the web we've got a lot of web pages.
The next step is we use the indexer to create the inverted index.
In general, we can use the standard information retrieval techniques for creating the index, and that is what we talked about in the previous lecture.
But there are new challenges that we have to solve for web scale indexing, and the two main challenges of scalability and efficiency.
The index will be so large that it cannot actually fit into any single machine or single disk, so we have to store the data on multiple machines.
Also, because the data is so large, it's beneficial to process the data in parallel so that we can produce the index quickly.
To address these challenges, Google has made a number of innovations.
One is the Google File System, that's a general distributed file system that can help programmers manage files stored on a cluster of machines.
The second is MapReduce.
This is a general software framework for supporting parallel computation.
Hadoop is the most well known open source implementation of MapReduce, now used in many applications.
So this is the architecture of the Google File System.
It uses a very simple centralized management mechanism to manage all the specific locations of files.
So it maintains the file namespace and look up table to know where exactly each file is stored.
The application client would then talk to this GFS master.
And that obtains specific locations of the files that they want to process.
And once the GFS client obtained the specific information about the files, then the application client can talk to the specific servers where the data actually sits directly.
So that you can avoid avoid involving other nodes in the network.
So when this file system stores the files on machines the system also would create a fixed sizes of chunks.
So the data files are separate into many chunks, each chunk is 64 megabytes, so it's pretty big.
And that's appropriate for large data processing.
These chunks are replicated to ensure reliability.
So this is something that the, the programmer doesn't have to worry about, and it's all taken care of by this file system.
So from the application perspective, the programmer would see this as if it's a normal file.
The program doesn't have to know where exactly it's stored, and can just invoke high level operators to process the file.
And another feature is that the data transfer is directly between application and chunk servers, so it's, it's efficient in this sense.
On top of the Google file system, and Google also proposed MapReduce as a general framework for parallel programming.
Now, this is very useful to support a task like building inverted index.
And so this framework is hiding a lot of low level features from the programmer.
As a result, the programmer can make minimum effort to create a application that can be run on a large cluster in parallel.
So, some of the low level details hidden in the framework, including the specific natural communications, or load balancing, or where the tasks are executed, all these details are hidden from the programmer.
There is also a nice feature which is the built-in fault tolerance.
If one server is broken, let's say, so it's down, and then some tasks may not be finished, then the MapReduce mechanism would know that the task has not been done.
So it would automatically dispatch the task on other servers that can do the job.
And therefore, again, the programmer doesn't have to worry about that.
So here's how MapReduce works.
The input data will be separated into a number of key, value pairs.
Now, what exactly is in the value will depend on the data.
And it's actually a fairly general framework to allow you to just partition the data into different parts.
And each part can be then processed in parallel.
Each key, value pair will be then sent to a map function.
The programmer will write the map function, of course.
And then the map function will then process this key value pair and generate the, a number of other key value pairs.
Of course, the new key is usually different from the old key that's given to the map as input.
And these key value pairs are the output of the map function.
And all the outputs of all the map functions will be then collected.
And then they will be further sorted based on the key.
And the result is that all the values that are associated with the same key will be then grouped together.
So now we've got a pair of a key and a set of values that are attached to this key.
So this will then be sent to a reduce function.
Now, of course, each reduce function will handle a different each a different key.
So we will send this, these output values to multiple reduce functions, each handling a unique key.
A reduce function would then process the input, which is a key and a set of values, to produce another set of key values as the output.
So these output values would be then collected together to form the, the final output.
Right, so this is the, the general framework of MapReduce.
Now, the programmer only needs to write the the map function and the reduce function.
Everything else is actually taken care of by the MapReduce framework.
So, you can see the programmer really only needs to do minimum work.
And with such a framework, the input data can be partitioned into multiple parts.
Each is processed in parallel first by map, and then in the process after we reach the reduce stage, then much more reduce functions can also further process the different keys and their associated values in parallel.
So it achieves some it achieves the purpose of parallel processing of a large dataset.
So let's take a look at a simple example, and that's word counting.
The input is is files containing words.
And the output that we want to generate is the number of occurrences of each word, so it's the word count.
Right, we know this, this kind of counting would be useful to, for example, assess the popularity of a word in a large collection.
And this is useful for achieving a factor of IDF weighting for search.
So how can we solve this problem?
Well, one natural thought is that, well, this task can be done in parallel by simply counting different parts of the file in parallel and then in the end, we just combine all the counts.
And that's precisely the idea of what we can do with MapReduce.
We can parallelize lines in this input file.
So more specifically, we can assume the input to each map function is a key value pair that represents the line number and the stream on that line.
So the first line, for example, has a key of one.
And the value is Hello World Bye World, and just four words on that line.
So this key-value pair will be sent to a map function.
The map function would then just count the words in this line.
And in this case, of course, there are only four words.
Each word gets a count of one.
And these are the output that you see here on this slide, from this map function.
So, the map function is really very simple.
If you look at the, what the pseudocode looks like on the right side, you see, it simply needs to iterate over all the words in this line, and then just call a Collect function, which means it would then send the word and the counter to the collector.
The collector would then try to sort all these key value pairs from different map functions.
Right?
So the functions are very simple.
And the programmer specifies this function as a way to process each part of the data.
Of course, the second line will be handled by a different map function, which will produce a similar output.
Okay, now the output from the map functions will be then sent to a collector.
And the collector will do the internal grouping or sorting.
So at this stage, you can see we have collected multiple pairs.
Each pair is a word and its count in the line.
So once we see all these these pairs, then we can sort them based on the key, which is the word.
So we will collect all the counts of a word, like bye, here, together.
And similarly, we do that for other words.
Like Hadoop, hello, etc.
So each word now is attached to a number of values, a number of counts.
And these counts represent the occurrences of this word in different lines.
So now we have got a new pair of a key and a set of values, and this pair will then be fed into a reduce function.
So the reduce function now will have to finish the job of counting the total occurrences of this word.
Now it has already got all these partial counts, so all it needs to do is simply to add them up.
So the reduce function shown here is very simple as well.
You have a counter and then iterate over all the words that you see in this array, and then you just accumulate these counts, right.
And then finally, you output the key and and the total count, and that's precisely what we want as the output of this whole program.
So, you can see, this is already very similar to building a inverted index, and if you think about it, the output here is indexed by a word, and we have already got a dictionary, basically.
We have got the count.
But what's missing is the document IDs and the specific frequency counts of words in those documents.
So we can modify this slightly to actually build a inverted index in parallel.
So here's one way to do that.
So in this case, we can assume the input to a map function is a pair of a key which denotes the document ID and the value denoting the string for that document.
So it's all the words in that document.
And so the map function will do something very similar to what we have seen in the water company example.
It simply groups all the counts of this word in this document together.
And it will then generate a set of key value pairs.
Each key is a word.
And the value is the count of this word in this document plus the document ID.
Now, you can easily see why we need to add document ID here.
Of course, later, in the inverted index, we would like to keep this information, so the map function should keep track of it.
And this can then be sent to the reduce function later.
Now, similarly another document D2 can be processed in the same way.
So in the end, again, there is a sorting mechanism that would group them together.
And then we will have just a key like java associated with all the documents that match this key, or all the documents where java occurred, and their counts, right, so the counts of java in those documents.
And this will be collected together.
And this will be, so fed into the reduced function.
So, now you can see, the reduce function has already got input that looks like a inverted index entry, right?
So, it's just the word and all the documents that contain the word and the frequency of the word in those documents.
So, all you need to do is simply to concatenate them into a continuous chunk of data, and this can be then retained into a file system.
So basically, the reduce function is going to do very minimal work.
And so, this is pseudo-code for inverted index construction.
Here we see two functions, procedure Map and procedure Reduce.
And a programmer would specify these two functions to program on top of MapReduce.
And you can see, basically, they are doing what I just described.
In the case of Map, it's going to count the occurrences of a word using an associative array, and will output all the counts together with the document ID here.
Right?
So this, the reduce function, on the other hand simply concatenates all the input that it has been given and then put them together as one single entry for this key.
So this is a very simple MapReduce function, yet it would allow us to construct an inverted index at a very large scale, and data can be processed by different machines.
The program doesn't have to take care of the details.
So this is how we can do parallel index construction for web search.
So to summarize, web scale indexing requires some new techniques that go beyond the standard traditional indexing techniques.
Mainly, we have to store index on multiple machines, and this is usually done by using a file system like Google File System, a distributed file system.
And secondly, it requires creating the index in parallel, because it's so large, it takes a long time to create an index for all the documents.
So if we can do it in parallel, it would be much faster, and this is done by using the MapReduce framework.
Note that the both the GFS and MapReduce frameworks are very general, so they can also support many other applications.
This lecture is the first one about the text clustering.
In this lecture, we are going to talk about the text clustering.
This is a very important technique for doing topic mining and analysis.
In particular, in this lecture we're going to start with some basic questions about the clustering.
And that is, what is text clustering and why we are interested in text clustering.
In the following lectures, we are going to talk about how to do text clustering.
How to evaluate the clustering results?
So what is text clustering?
Well, clustering actually is a very general technique for data mining as you might have learned in some other courses.
The idea is to discover natural structures in the data.
In another words, we want to group similar objects together.
In our case, these objects are of course, text objects.
For example, they can be documents, terms, passages, sentences, or websites, and then I'll go group similar text objects together.
So let's see an example, well, here you don't really see text objects, but I just used some shapes to denote objects that can be grouped together.
Now if I ask you, what are some natural structures or natural groups where you, if you look at it and you might agree that we can group these objects based on chips, or their locations on this two dimensional space.
So we got the three clusters in this case.
And they may not be so much this agreement about these three clusters but it really depends on the perspective to look at the objects.
Maybe some of you have also seen thing in a different way, so we might get different clusters.
And you'll see another example about this ambiguity more clearly.
But the main point of here is, the problem is actually not so well defined.
And the problem lies in how to define similarity.
And what do you mean by similar objects?
Now this problem has to be clearly defined in order to have a well defined clustering problem.
And the problem is in general that any two objects can be similar depending on how you look at them.
So for example, this will kept the two words like car and horse.
So are the two words similar?
Well, it depends on how if we look at the physical properties of car and horse they are very different but if you look at them functionally, a car and a horse, can both be transportation tools.
So in that sense, they may be similar.
So as we can see, it really depends on our perspective, to look at the objects.
And so it ought to make the clustering problem well defined.
A user must define the perspective for assessing similarity.
And we call this perspective the clustering bias.
And when you define a clustering problem, it's important to specify your perspective for similarity or for defining the similarity that will be used to group similar objects.
because otherwise, the similarity is not well defined and one can have different ways to group objects.
So let's look at the example here.
You are seeing some objects, or some shapes, that are very similar to what you have seen on the first slide, but if I ask you to group these objects, again, you might feel there is more than here than on the previous slide.
For example, you might think, well, we can steer a group by ships, so that would give us cluster that looks like this.
However, you might also feel that, well, maybe the objects can be grouped based on their sizes.
So that would give us a different way to cluster the data if we look at the size and look at the similarity in size.
So as you can see clearly here, depending on the perspective, we'll get different clustering result.
So that also clearly tells us that in order to evaluate the clustering without, we must use perspective.
Without perspective, it's very hard to define what is the best clustering result.
So there are many examples of text clustering setup.
And so for example, we can cluster documents in the whole text collection.
So in this case, documents are the units to be clustered.
We may be able to cluster terms.
In this case, terms are objects.
And a cluster of terms can be used to define concept, or theme, or a topic.
In fact, there's a topic models that you have seen some previous lectures can give you cluster of terms in some sense if you take terms with high probabilities from word distribution.
Another example is just to cluster any text segments, for example, passages, sentences, or any segments that you can extract the former larger text objects.
For example, we might extract the order text segments about the topic, let's say, by using a topic model.
Now once we've got those text objects then we can cluster the segments that we've got to discover interesting clusters that might also ripple in the subtopics.
So this is a case of combining text clustering with some other techniques.
And in general you will see a lot of text mining can be accurate combined in a flexible way to achieve the goal of doing more sophisticated mining and analysis of text data.
We can also cluster fairly large text objects and by that, I just mean text objects may contain a lot of documents.
So for example, we might cluster websites.
Each website is actually compose of multiple documents.
Similarly, we can also cluster articles written by the same author, for example.
So we can trigger all the articles published by also as one unit for clustering.
In this way, we might group authors together based on whether they're published papers or similar.
For the more text clusters will be for the cluster to generate a hierarchy.
That's because we can in general cluster any text object at different levels.
So more generally why is text clustering interesting?
Well, it's because it's a very useful technique for text mining, particularly exploratory text analysis.
And so a typical scenario is that you were getting a lot of text data, let's say all the email messages from customers in some time period, all the literature articles, etc.
And then you hope to get a sense about what are the overall content of the connection, so for example, you might be interested in getting a sense about major topics, or what are some typical or representative documents in the connection.
And clustering to help us achieve this goal.
We sometimes also want to link a similar text objects together.
And these objects might be duplicated content, for example.
And in that case, such a technique can help us remove redundancy and remove duplicate documents.
Sometimes they are about the same topic and by linking them together we can have more complete coverage of a topic.
We may also used text clustering to create a structure on the text data and sometimes we can create a hierarchy of structures and this is very useful for problems.
We may also use text clustering to induce additional features to represent text data when we cluster documents together, we can treat each cluster as a feature.
And then we can say when a document is in this cluster and then the feature value would be one.
And if a document is not in this cluster, then the feature value is zero.
And this helps provide additional discrimination that might be used for text classification as we will discuss later.
So there are, in general, many applications of text clustering.
And I just thought of two very specific ones.
One is to cluster search results, for example, [INAUDIBLE] search engine can cluster such results so that the user can see overall structure of the results of return the fall query.
And when the query's ambiguous this is particularly useful because the clusters likely represent different senses of ambiguous word.
Another application is to understand the major complaints from customers based on their emails, right.
So in this case, we can cluster email messages and then find in the major clusters from there, we can understand what are the major complaints about them.
This lecture is about natural language content analysis.
Natural language content analysis is the foundation of text mining.
So we're going to first talk about this.
And in particular, natural language processing with a factor how we can present text data.
And this determines what algorithms can be used to analyze and mine text data.
We're going to take a look at the basic concepts in natural language first.
And I'm going to explain these concepts using a similar example that you've all seen here.
A dog is chasing a boy on the playground.
Now this is a very simple sentence.
When we read such a sentence we don't have to think about it to get the meaning of it.
But when a computer has to understand the sentence, the computer has to go through several steps.
First, the computer needs to know what are the words, how to segment the words in English.
And this is very easy, we can just look at the space.
And then the computer will need the know the categories of these words, syntactical categories.
So for example, dog is a noun, chasing's a verb, boy is another noun etc.
And this is called a Lexical analysis.
In particular, tagging these words with these syntactic categories is called a part-of-speech tagging.
After that the computer also needs to figure out the relationship between these words.
So a and dog would form a noun phrase.
On the playground would be a prepositional phrase, etc.
And there is certain way for them to be connected together in order for them to create meaning.
Some other combinations may not make sense.
And this is called syntactical parsing, or syntactical analysis, parsing of a natural language sentence.
The outcome is a parse tree that you are seeing here.
That tells us the structure of the sentence, so that we know how we can interpret this sentence.
But this is not semantics yet.
So in order to get the meaning we would have to map these phrases and these structures into some real world antithesis that we have in our mind.
So dog is a concept that we know, and boy is a concept that we know.
So connecting these phrases that we know is understanding.
Now for a computer, would have to formally represent these entities by using symbols.
So dog, d1 means d1 is a dog.
Boy, b1 means b1 refers to a boy etc.
And also represents the chasing action as a predicate.
So, chasing is a predicate here with three arguments, d1, b1, and p1.
Which is playground.
So this formal rendition of the semantics of this sentence.
Once we reach that level of understanding, we might also make inferences.
For example, if we assume there's a rule that says if someone's being chased then the person can get scared, then we can infer this boy might be scared.
This is the inferred meaning, based on additional knowledge.
And finally, we might even further infer what this sentence is requesting, or why the person who say it in a sentence, is saying the sentence.
And so, this has to do with purpose of saying the sentence.
This is called speech act analysis or pragmatic analysis.
Which first to the use of language.
So, in this case a person saying this may be reminding another person to bring back the dog.
So this means when saying a sentence, the person actually takes an action.
So the action here is to make a request.
Now, this slide clearly shows that in order to really understand a sentence there are a lot of things that a computer has to do.
Now, in general it's very hard for a computer will do everything, especially if you would want it to do everything correctly.
This is very difficult.
Now, the main reason why natural language processing is very difficult, it's because it's designed it will make human communications efficient.
As a result, for example, with only a lot of common sense knowledge.
Because we assume all of us have this knowledge, there's no need to encode this knowledge.
That makes communication efficient.
We also keep a lot of ambiguities, like, ambiguities of words.
And this is again, because we assume we have the ability to disambiguate the word.
So, there's no problem with having the same word to mean possibly different things in different context.
Yet for a computer this would be very difficult because a computer does not have the common sense knowledge that we do.
So the computer will be confused indeed.
And this makes it hard for natural language processing.
Indeed, it makes it very hard for every step in the slide that I showed you earlier.
Ambiguity is a main killer.
Meaning that in every step there are multiple choices, and the computer would have to decide whats the right choice and that decision can be very difficult as you will see also in a moment.
And in general, we need common sense reasoning in order to fully understand the natural language.
And computers today don't yet have that.
That's why it's very hard for computers to precisely understand the natural language at this point.
So here are some specific examples of challenges.
Think about the world-level ambiguity.
A word like design can be a noun or a verb, so we've got ambiguous part of speech tag.
Root also has multiple meanings, it can be of mathematical sense, like in the square of, or can be root of a plant.
Syntactic ambiguity refers to different interpretations of a sentence in terms structures.
So for example, natural language processing can actually be interpreted in two ways.
So one is the ordinary meaning that we will be getting as we're talking about this topic.
So, it's processing of natural language.
But there's is also another possible interpretation which is to say language processing is natural.
Now we don't generally have this problem, but imagine for the computer to determine the structure, the computer would have to make a choice between the two.
Another classic example is a man saw a boy with a telescope.
And this ambiguity lies in the question who had the telescope?
This is called a prepositional phrase attachment ambiguity.
Meaning where to attach this prepositional phrase with the telescope.
Should it modify the boy?
Or should it be modifying, saw, the verb.
Another problem is anaphora resolution.
In John persuaded Bill to buy a TV for himself.
Does himself refer to John or Bill?
Presupposition is another difficulty.
He has quit smoking implies that he smoked before, and we need to have such a knowledge in order to understand the languages.
Because of these problems, the state of the art natural language processing techniques can not do anything perfectly.
Even for the simplest part of speech tagging, we still can not solve the whole problem.
The accuracy that are listed here, which is about 97%, was just taken from some studies earlier.
And these studies obviously have to be using particular data sets so the numbers here are not really meaningful if you take it out of the context of the data set that are used for evaluation.
But I show these numbers mainly to give you some sense about the accuracy, or how well we can do things like this.
It doesn't mean any data set accuracy would be precisely 97%.
But, in general, we can do parsing speech tagging fairly well although not perfect.
Parsing would be more difficult, but for partial parsing, meaning to get some phrases correct, we can probably achieve 90% or better accuracy.
But to get the complete parse tree correctly is still very, very difficult.
For semantic analysis, we can also do some aspects of semantic analysis, particularly, extraction of entities and relations.
For example, recognizing this is the person, that's a location, and this person and that person met in some place etc.
We can also do word sense to some extent.
The occurrence of root in this sentence refers to the mathematical sense etc.
Sentiment analysis is another aspect of semantic analysis that we can do.
That means we can tag the senses as generally positive when it's talking about the product or talking about the person.
Inference, however, is very hard, and we generally cannot do that for any big domain and if it's only feasible for a very limited domain.
And that's a generally difficult problem in artificial intelligence.
Speech act analysis is also very difficult and we can only do this probably for very specialized cases.
And with a lot of help from humans to annotate enough data for the computers to learn from.
So the slide also shows that computers are far from being able to understand natural language precisely.
And that also explains why the text mining problem is difficult.
Because we cannot rely on mechanical approaches or computational methods to understand the language precisely.
Therefore, we have to use whatever we have today.
A particular statistical machine learning method of statistical analysis methods to try to get as much meaning out from the text as possible.
And, later you will see that there are actually many such algorithms that can indeed extract interesting model from text even though we cannot really fully understand it.
Meaning of all the natural language sentences precisely.
This lecture is about document length normalization in the vector space model.
In this lecture we are going to continue the discussion of the vector space model in particular we are going to discuss.
The issue of document length normalization.
So far in the lectures about the vector space model, we have used the various signals from the document to assess the matching of the document though with a preorder.
In particular we have considered the term frequency, the count of a term in a document.
We have also considered a, it's global statistics such as IDF in words document frequency.
But we have not considered a document length.
So, here I show two example documents.
D4 is much shorter with only 100 words.
D6 on the other hand has 5,000 words.
If you look at the matching of these query words we see that in D6 there are more matchings of the query words but one might reason that D6 may have matched these query words.
In a scattered manner.
So maybe the topic of d6 is not really about the topic of the query.
So the discussion of a campaign at the beginning of the document may have nothing to do with the mention of presidential at the end.
In general, if you think about the long documents, they would have a higher chance to match any query.
In fact, if you generate a, a long document that randomly sampling, sampling words from the distribution of words, then eventually you probably will match any query.
So in this sense we should penalize no documents because they just naturally have better chances to match any query.
And this is our idea of document answer.
We also need to be careful in avoiding to overpenalize small documents.
On the one hand, we want to penalize a long document.
But on the other hand, we also don't want to over-penalize them.
And the reason is because a document that may be long because of different reason.
In one case the document may be more long because it uses more words.
So for example think about the article of a research paper.
It would use more words than the corresponding abstract.
So this is the case where we probably should penalize the matching of a long document such as, full paper.
When we compare the matching of words in such long document with matching of the words in the short abstract.
Then long papers generally have a higher chance of matching query words.
Therefore we should penalize them.
However, there is another case when the document is long and that is when the document simply has more content.
Now consider another case of a long document, where we simply concatenated a lot of abstracts of different papers.
In such a case, obviously, we don't want to penalize such a long document.
Indeed, we probably don't want to penalize such a document because it's long.
So that's why we need to be careful.
About using the right degree of penalization.
A method that has been working well based on recent research is called, pivot length normalization.
And in this case the idea is to use.
The average document length as a P word, as a reference point.
That means we will assume that for the average length documents, the score is about right.
So, the normalizer would be 1.
But if a document is longer than the average document length then there will be some penalization.
Where as if it's shorter than there's even some reward.
So this is an illustrator that using this slide.
On the axis, s axis you can see the length of document.
On the y-axis we show the normalizer, in the case pivoted length normalization formula for the normalizer is is seem to be interpolation of one and the normalize the document lengths, controlled by a parameter b here.
So, you can see here, when we first divide the lengths of the document by the average document length.
This not only gives us some sense about the, how this document is compared with the average document length, but also gives us a benefit of not worrying about the unit of length, we can measure the length by words or by characters.
Anyway this normalizer has an interesting property.
First we see that if we set the parameter b to 0 then the value would be 1, so there's no pair, length normalization at all.
So b in this sense controls the length normalization, where as if we set d to a non-zero value, then the normalizer will look like this, right.
So the value would be higher for documents that are longer than the average document length.
Where as the value of the normalizer will be short- will be smaller for shorter documents.
So in this sense we see there's a penalization for long documents.
And there's a reward for short documents.
The degree of penalization is conjured by b.
Because if we set b to a larger value then the normalizer.
What looked like this.
There's even more penalization for long documents and more reward for the short documents.
By adjusting b which varies from zero to one we can control the degree of length normalization.
So if we're plucking this length normalization factor into the vector space model ranking functions that we have already examined.
Then we will end up heading with formulas, and these are in fact the state of the are vector space models.
Formulas.
So, let's talk an that, let's take a look at the each of them.
The first one's called a pivoted length normalization vector space model.
And, a reference in the end has detail about the derivation of this model.
And, here, we see that it's basically the TFIDF weighting model that we have discussed.
The IDF component should be very familiar now to you.
There is also a query term frequency component, here.
And, and then in the middle there is.
And normalize the TF.
And in this case, we see we use the double algorithm, as we discussed before, and this is to achieve a sublinear transformation.
But we also put document length normalizer in the bottom, all right so this would cause penalty for a long document, because the larger the denominator is, the denominator is then the smaller the shift weight is.
And this is of course controlled by the parameter b here.
And you can see again, b is set to 0, and there, there is no length normalization.
Okay.
So this is one of the two most effective.
Not this base model of formulas.
The next one called a BM25, or Okapi, is, also similar.
In that, it also has a i, df component here, and a query df component here.
But in the middle, the normalization's a little bit different.
As we expand there is this or copied here for transformation here.
And that does, sublinear transformation with an upper bound.
In this case we have put the length normalization factor here.
We are adjusting k, but it achieves a similar factor because we put a normalizer in the denominator.
Therefore again, if a document is longer, then the term weight will be smaller.
So, you can see, after we have gone through all the instances that we talked about, and we have, in the end, reached the, basically the state of the art mutual function.
So, so far we have talked about mainly how to place the document matter in the matter space.
And this has played an important role in uh,determining the factors of the function.
But there are also other dimensions where we did not really examine detail.
For example can we further improve the instantiation of the dimension of the vector space model.
Now we've just assumed that the back of words.
So each dimension is a word.
But obviously we can see there are many other choices.
For example, stemmed words, those are the words that have been transformed into the same rule form.
So that computation and computing will all become the same and they can be matched.
We need to stop water removal.
This is removes on very common words that don't carry any content.
Like the or of, we use the phrases to define that .
We can even use late in the semantica, an answer sort of find in the sum cluster.
So words that represent a legend of concept as one.
We can also use smaller units, like a character in grams.
Those are sequences of n characters for dimensions.
However, in practice people have found that the bag-of-words representation with the phrases is where the the most effective one.
And it's also efficient so this is still so far the most popular dimension instantiation method and it's used in all the major search engines.
I should also mention that sometimes we did to do language specific and domain specific organization.
And this is actually very important as we might have variations of the terms.
That might prevent us from matching them with each other.
Even though they mean the same thing.
And some of them, which is like Chinese, the results of the.
Segmenting text to obtain word boundaries.
Because it's just a sequence of characters.
A word might, might correspond to one character or two characters or even three characters.
So it's easier in English when we have a space to separate the words.
But in some other languages we may need to do some natural language processing to figure out the, where are the boundaries for words.
There is also possibility to improve this in narrative function.
And so far we have used the about product, but one can imagine there are other matches.
For example we can match the cosine of the angle between two vectors, or we can use Euclidean distance measure.
And these are all possible.
The dot product seems still the best and one of the reasons is because it's very general.
In fact, it's sufficiently general.
If you consider the possibilities of doing weighting in different ways.
So, for example, cosine measure can be regarded as the dot product of two normalized vectors.
That means we first normalize each vector, and then we take the dot product.
That would be equivalent to the cosine measure.
I just mentioned that the BM25.
Seems to be one of the most effective formulas.
But there has been also further development in, improving BM25, although none of these works have changed the BM25 fundamentally.
So in one line of work, people have derived BM25 F. Here F stands for field, and this is a little use BM25 for documents with a structures.
For example you might consider title field, the abstract, or body of the reasearch article, or even anchor text on the web pages.
Those are the text fields that describe links to other pages.
And these can all be combined with a appropriate weight on different fields to help improve scoring for document.
Use BM25 for such a document.
And the obvious choice is to apply BM25 for each field, and then combine the scores.
Basically, the ideal of BM25F, is to first combine the frequency counts of tons in all the fields and then apply BM25.
Now this has advantage of avoiding over counting the first occurrence of the term.
Remember in the sublinear transformation of TF, the first recurrence is very important then, and contributes a large weight.
And if we do that for all the fields, then the same term might have gained a, a lot of advantage in every field, but when we combine these word frequencies together.
We just do the transformation one time, and that time then the extra occurrences will not be counted as fresh first occurrences.
And this method has been working very well for scoring structured documents.
The other line of extension is called a BM25 plus and this line, arresters have addressed the problem of over penalization of long documents by BM25.
So to address this problem, the fix is actually quite simple.
We can simply add a small constant to the TF normalization formula.
But what's interesting is that we can analytically prove that by doing such a small modification, we will fix the problem of a, over penalization of long documents by the original BM25.
So the new formula called BM25-plus is empirically and analytically shown to be better than BM25.
So to summarize all what we have said about the Vector Space Model.
Here are the major takeaway points.
First, in such a model, we use the similarity notion of relevance, assuming that the relevance of a document with respect to a query is basically proportional to the similarity between the query and the document.
So, naturally, that implies that the query and document must be represented in the same way, and in this case, we represent them as vectors in high dimensional vector space.
Where the dimensions are defined by words or concepts or terms in general.
And we generally need to use a lot of heuristics to design a ranking function.
We use some examples which show the need for several heuristics, including TF waiting and transformation.
And IDF weighting, and document length normalization.
These major heuristics are the most important heuristics to ensure such a general ranking function to work well for all kinds of tasks.
And finally BM25 and Pivoted normalization seem to be the most effective formulas out of that Space Model.
Now I have to say that, I've put BM25 in the category of Vector Space Model.
But in fact the BM25 has been derived using model.
So the reason why I've put it in the vector space model is first the ranking function actually has a nice interpretation in the vector space model.
We can easily see it looks very much like a vector space model with a special weighting function.
The second reason is because the original BM25 has a somewhat different from of IDF.
And that form of IDF actually doesn't really work so well as the standard IDF that you have seen here.
So as a effective original function BM25 should probably use a heuristic modification of the IDF to make that even more like a vector space model.
There are some additional readings.
The first is a paper about the pivoted length normalization.
It's an excellent example of using empirical data enhances to suggest a need for length normalization, and then further derived a length normalization formula.
The second is the original paper when the was proposed.
The third paper has a thorough discussion of and its extensions, particularly BM-25F.
And finally, the last paper has a discussion of improving BM-25 to correct the overpenalization of long documents.
This lecture is about topic mining and analysis.
We're going to talk about using a term as topic.
This is a slide that you have seen in a earlier lecture where we define the task of topic mining and analysis.
We also raised the question, how do we exactly define the topic of theta?
So in this lecture, we're going to offer one way to define it, and that's our initial idea.
Our idea here is defining a topic simply as a term.
A term can be a word or a phrase.
And in general, we can use these terms to describe topics.
So our first thought is just to define a topic as one term.
For example, we might have terms like sports, travel, or science, as you see here.
Now if we define a topic in this way, we can then analyze the coverage of such topics in each document.
Here for example, we might want to discover to what extent document one covers sports.
And we found that 30% of the content of document one is about sports.
And 12% is about the travel, etc.
We might also discover document two does not cover sports at all.
So the coverage is zero, etc.
So now, of course, as we discussed in the task definition for topic mining and analysis, we have two tasks.
One is to discover the topics.
And the second is to analyze coverage.
So let's first think about how we can discover topics if we represent each topic by a term.
So that means we need to mine k topical terms from a collection.
Now there are, of course, many different ways of doing that.
And we're going to talk about a natural way of doing that, which is also likely effective.
So first of all, we're going to parse the text data in the collection to obtain candidate terms.
Here candidate terms can be words or phrases.
Let's say the simplest solution is to just take each word as a term.
These words then become candidate topics.
Then we're going to design a scoring function to match how good each term is as a topic.
So how can we design such a function?
Well there are many things that we can consider.
For example, we can use pure statistics to design such a scoring function.
Intuitively, we would like to favor representative terms, meaning terms that can represent a lot of content in the collection.
So that would mean we want to favor a frequent term.
However, if we simply use the frequency to design the scoring function, then the highest scored terms would be general terms or functional terms like the, etc.
Those terms occur very frequently English.
So we also want to avoid having such words on the top so we want to penalize such words.
But in general, we would like to favor terms that are fairly frequent but not so frequent.
So a particular approach could be based on TF-IDF weighting from retrieval.
And TF stands for term frequency.
IDF stands for inverse document frequency.
We talked about some of these ideas in the lectures about the discovery of word associations.
So these are statistical methods, meaning that the function is defined mostly based on statistics.
So the scoring function would be very general.
It can be applied to any language, any text.
But when we apply such a approach to a particular problem, we might also be able to leverage some domain-specific heuristics.
For example, in news we might favor title words actually general.
We might want to favor title words because the authors tend to use the title to describe the topic of an article.
If we're dealing with tweets, we could also favor hashtags, which are invented to denote topics.
So naturally, hashtags can be good candidates for representing topics.
Anyway, after we have this design scoring function, then we can discover the k topical terms by simply picking k terms with the highest scores.
Now, of course, we might encounter situation where the highest scored terms are all very similar.
They're semantically similar, or closely related, or even synonyms.
So that's not desirable.
So we also want to have coverage over all the content in the collection.
So we would like to remove redundancy.
And one way to do that is to do a greedy algorithm, which is sometimes called a maximal marginal relevance ranking.
Basically, the idea is to go down the list based on our scoring function and gradually take terms to collect the k topical terms.
The first term, of course, will be picked.
When we pick the next term, we're going to look at what terms have already been picked and try to avoid picking a term that's too similar.
So while we are considering the ranking of a term in the list, we are also considering the redundancy of the candidate term with respect to the terms that we already picked.
And with some thresholding, then we can get a balance of the redundancy removal and also high score of a term.
Okay, so after this that will get k topical terms.
And those can be regarded as the topics that we discovered from the connection.
Next, let's think about how we're going to compute the topic coverage pi sub ij.
So looking at this picture, we have sports, travel and science and these topics.
And now suppose you are give a document.
How should we pick out coverage of each topic in the document?
Well, one approach can be to simply count occurrences of these terms.
So for example, sports might have occurred four times in this this document and travel occurred twice, etc.
And then we can just normalize these counts as our estimate of the coverage probability for each topic.
So in general, the formula would be to collect the counts of all the terms that represent the topics.
And then simply normalize them so that the coverage of each topic in the document would add to one.
This forms a distribution of the topics for the document to characterize coverage of different topics in the document.
Now, as always, when we think about idea for solving problem, we have to ask the question, how good is this one?
Or is this the best way of solving problem?
So now let's examine this approach.
In general, we have to do some empirical evaluation by using actual data sets and to see how well it works.
Well, in this case let's take a look at a simple example here.
And we have a text document that's about a NBA basketball game.
So in terms of the content, it's about sports.
But if we simply count these words that represent our topics, we will find that the word sports actually did not occur in the article, even though the content is about the sports.
So the count of sports is zero.
That means the coverage of sports would be estimated as zero.
Now of course, the term science also did not occur in the document and it's estimate is also zero.
And that's okay.
But sports certainly is not okay because we know the content is about sports.
So this estimate has problem.
What's worse, the term travel actually occurred in the document.
So when we estimate the coverage of the topic travel, we have got a non-zero count.
So its estimated coverage will be non-zero.
So this obviously is also not desirable.
So this simple example illustrates some problems of this approach.
First, when we count what words belong to to the topic, we also need to consider related words.
We can't simply just count the topic word sports.
In this case, it did not occur at all.
But there are many related words like basketball, game, etc.
So we need to count the related words also.
The second problem is that a word like star can be actually ambiguous.
So here it probably means a basketball star, but we can imagine it might also mean a star on the sky.
So in that case, the star might actually suggest, perhaps, a topic of science.
So we need to deal with that as well.
Finally, a main restriction of this approach is that we have only one term to describe the topic, so it cannot really describe complicated topics.
For example, a very specialized topic in sports would be harder to describe by using just a word or one phrase.
We need to use more words.
So this example illustrates some general problems with this approach of treating a term as topic.
First, it lacks expressive power.
Meaning that it can only represent the simple general topics, but it cannot represent the complicated topics that might require more words to describe.
Second, it's incomplete in vocabulary coverage, meaning that the topic itself is only represented as one term.
It does not suggest what other terms are related to the topic.
Even if we're talking about sports, there are many terms that are related.
So it does not allow us to easily count related terms to order, conversion to coverage of this topic.
Finally, there is this problem of word sense disintegration.
A topical term or related term can be ambiguous.
For example, basketball star versus star in the sky.
So in the next lecture, we're going to talk about how to solve the problem with of a topic.
In this lecture we continue discussing Paradigmatic Relation Discovery.
Earlier we introduced a method called Expected Overlap of Words in Context.
In this method we represent each context by a word of vector that represents the probability of a word in the context.
And we measure the similarity by using the dot product which can be interpreted as the probability that two randomly picked words from the two contexts are identical.
We also discussed the two problems of this method.
The first is that it favors matching one frequent term very well over matching more distinct terms.
It put too much emphasis on matching one term very well.
The second is that it treats every word equally.
Even a common word like the would contribute equally as content word like eats.
So now we are going to talk about how to solve this problems.
More specifically we're going to introduce some retrieval heuristics used in text retrieval and these heuristics can effectively solve these problems as these problems also occur in text retrieval when we match a query with a document, so to address the first problem, we can use a sublinear transformation of term frequency.
That is, we don't have to use raw frequency count of the term to represent the context.
We can transform it into some form that wouldn't emphasize so much on the raw frequency to address the problem, we can put more weight on rare terms.
And that is, we ran reward a matching a rare word.
And this heuristic is called IDF term weighting in text retrieval.
IDF stands for inverse document frequency.
So now we're going to talk about the two heuristics in more detail.
First, let's talk about the TF transformation.
That is, it'll convert the raw count of a word in the document into some weight that reflects our belief about how important this wording.
The document.
And so, that would be denoted by TF of w and d. That's shown in the Y axis.
Now, in general, there are many ways to map that.
And let's first look at the the simple way of mapping.
In this case, we're going to say, well, any non zero counts will be mapped to one.
And the zero count will be mapped to zero.
So with this mapping, all the frequencies will be mapped to only two values, zero or one.
And the mapping function is shown here as a flat line here.
This is naive because in order the frequency of words, however, this actually has advantage of emphasizing, matching all the words in the context.
It does not allow a frequent word to dominate the match now the approach that we have taken earlier in the overlap account approach is a linear transformation we basically take y as the same as x so we use the raw count as a representation and that created the problem that we just talked about.
Namely, it emphasizes too much on matching one frequent term.
Matching one frequent term can contribute a lot.
We can have a lot of other interesting transformations in between the two extremes.
And they generally form a sub linear transformation.
So for example, one a logarithm of the row count.
And this will give us curve that looks like this that you are seeing here.
In this case, you can see the high frequency counts.
The high counts are penalized a little bit all right, so the curve is a sub linear curve.
And it brings down the weight of those really high counts.
And this what we want because it prevents that kind of terms from dominating the scoring function.
Now, there is also another interesting transformation called a BM25 transformation, which as been shown to be very effective for retrieval.
And in this transformation we have a form that looks like this.
So it's k plus one multiplies by x, divided by x plus k. Where k is a parameter.
X is the count.
The raw count of a word.
Now the transformation is very interesting, in that it can actually kind of go from one extreme to the other extreme by varying k, and it also is interesting that it has upper bound, k + 1 in this case.
So, this puts a very strict constraint on high frequency terms, because their weight will never exceed k + 1.
As we vary k, we can simulate the two extremes.
So, when is set to zero, we roughly have the zero one vector.
Whereas, when we set the k to a very large value, it will behave more like, immediate transformation.
So this transformation function is by far the most effective transformation function for tax and retrieval, and it also makes sense for our problem set up.
So we just talked about how to solve the problem of overemphasizing a frequently, a frequently tongue.
Now let's look at the second problem, and that is how we can penalize popular terms, matching the is not surprising because the occurs everywhere.
But matching eats would count a lot so how can we address that problem.
In this case we can use the IDF weight.
Pop that's commonly used in retrieval.
IDF stands for inverse document frequency.
Now frequency means the count of the total number of documents that contain a particular word.
So here we show that the IDF measure is defined as a logarithm function of the number of documents that match a term or document frequency.
So, k is the number of documents containing a word, or document frequency.
And M here is the total number of documents in the collection.
The IDF function is giving a higher value for a lower k, meaning that it rewards a rare term, and the maximum value is log of M+1.
That's when the word occurred just once in the context, so that's a very rare term.
The rarest term in the whole collection.
The lowest value you can see here is when K reaches its maximum, which would be M. All right so, that would be a very low value, close to zero in fact.
So, this of course measure is used in search.
Where we naturally have a collection.
In our case, what would be our collection?
Well, we can also use the context that we had collected for all the words as our collection.
And that is to say, a word that's populating the collection in general.
Would also have a low IDF because depending on the dataset we can Construct the context vectors in the different ways.
But in the end, if a term is very frequently original data set.
Then it will still be frequenting the collective context documents.
So how can we add these heuristics to improve our similarity function well here's one way.
And there are many other ways that are possible.
But this is a reasonable way.
Where we can adapt the BM25 retrieval model for paradigmatic relation mining.
So here, we define, in this case we define the document vector as containing elements representing normalized BM25 values.
So in this normalization function, we see, we take a sum over, sum of all the words.
And we normalize the weight of each word by the sum of the weights of all the words.
And this is to, again, ensure all the xi's will sum to 1 in this vector.
So this would be very similar to what we had before, in that this vector is actually something similar to a word distribution.
Or the xis with sum to 1.
Now the weight of BM25 for each word is defined here.
And if you compare this with our old definition where we just have a normalized count, of this one so we only have this one and the document lens of the total counts of words.
Being that context document and that's what we had before.
But now with the BM25 transformation, we're introduced to something else.
First off, because this extra occurrence of this count is just to achieve the of normalization.
But we also see we introduced the parameter k here.
And this parameter is generally non active number although zero is also possible.
This controls the upper bound and the kind of all to what extent it simulates the linear transformation.
And so this is one parameter, but we also see there was another parameter here, B.
And this would be within 0 an 1.
And this is a parameter to control length] normalization.
And in this case, the normalization formula has average document length here.
And this is computed by taking the average of the lengths of all the documents in the collection.
In this case, all the lengths of all the context documents.
That we are considering.
So this average document will be a constant for any given collection.
So it actually is only affecting the factor of the parameter b here because this is a constant.
But I kept it here because it's constant and that's useful in retrieval where it would give us a stabilized interpretation of parameter B.
But, for our purpose it would be a constant.
So it would only be affecting the length normalization together with parameter b.
Now with this definition then, we have a new way to define our document of vectors.
And we can compute the vector d2 in the same way.
The difference is that the high frequency terms will now have a somewhat lower weight.
And this would help us control the influence of these high frequency terms.
Now, the idea can be added here in the scoring function.
That means we will introduce a way for matching each time.
You may recall, this is sum that indicates all the possible words that can be overlapped between the two contacts.
And the Xi and the Yi are probabilities of picking the word from both context, therefore, it indicates how likely we'll see a match on this word.
Now, IDF would give us the importance of matching this word.
A common word will be worth less than a rare word, and so we emphasize more on matching rare words now.
So, with this modification, then the new function.
When likely to address those two problems.
Now interestingly, we can also use this approach to discover syntagmatical relations.
In general, when we represent a term vector to replant a context with a term vector we would likely see, some terms have higher weights, and other terms have lower weights.
Depending on how we assign weights to these terms, we might be able to use these weights to discover the words that are strongly associated with a candidate of word in the context.
It's interesting that we can also use this context for similarity function based on BM25 to discover syntagmatic relations.
So, the idea is to use the converted implantation of the context.
To see which terms are scored high.
And if a term has high weight, then that term might be more strongly related to the candidate word.
So let's take a look at the vector in more detail here.
And we have each Xi defined as a normalized weight of BM25.
Now this weight alone only reflects how frequent the word occurs in the context.
But, we can't just say an infrequent term in the context would be correlated with the candidate word because many common words like the will occur frequently out of context.
But if we apply IDF weighting as you see here, we can then re weigh these terms based on IDF.
That means the words that are common, like the, will get penalized.
so now the highest weighted terms will not be those common terms because they have lower IDFs.
Instead, those terms would be the terms that are frequently in the context but not frequent in the collection.
So those are clearly the words that tend to occur in the context of the candidate word, for example, cat.
So, for this reason, the highly weighted terms in this idea of weighted vector can also be assumed to be candidates for syntagmatic relations.
Now, of course, this is only a byproduct of how approach is for discovering parathmatic relations.
And in the next lecture, we're going to talk more about how to discover syntagmatic relations.
But it clearly shows the relation between discovering the two relations.
And indeed they can be discussed.
Discovered in a joined manner by leveraging such associations, namely syntactical relation words that are similar in, yeah it also shows the relation between syntagmatic relation discovery and the paradgratical relations discovery.
We may be able to leverage the relation to join the discovery of two kinds of relations.
This also shows some interesting connections between the discovery of syntagmatic relation and the paradigmatic relation.
Specifically those words that are paradigmatic related tend to be having a syntagmatic relation with the same word.
So to summarize the main idea of what is covering paradigmatic relations is to collect the context of a candidate word to form a pseudo document, and this is typically represented as a bag of words.
And then compute similarity of the corresponding context documents of two candidate words.
And then we can take the highly similar word pairs and treat them as having paradigmatic relations.
These are the words that share similar contexts.
There are many different ways to implement this general idea, and we just talked about some of the approaches, and more specifically we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations.
More specifically we have used the BM25 and IDF weighting to discover paradigmatic relation.
And these approaches also represent the state of the art.
In text retrieval techniques.
Finally, syntagmatic relations can also be discovered as a by product when we discover paradigmatic relations.
This lecture is about the Text-Based Prediction.
In this lecture, we're going to start talking about the mining a different kind of knowledge, as you can see here on this slide.
Namely we're going to use text data to infer values of some other variables in the real world that may not be directly related to the text.
Or only remotely related to text data.
So this is very different from content analysis or topic mining where we directly characterize the content of text.
It's also different from opinion mining or sentiment analysis, which still have to do is characterizing mostly the content.
Only that we focus more on the subject of content which reflects what we know about the opinion holder.
But this only provides limited review of what we can predict.
In this lecture and the following lectures, we're going to talk more about how we can predict more Information about the world.
How can we get the sophisticated patterns of text together with other kind of data?
It would be useful first to take a look at the big picture of prediction, and data mining in general, and I call this data mining loop.
So the picture that you are seeing right now is that there are multiple sensors, including human sensors, to report what we have seen in the real world in the form of data.
Of course the data in the form of non-text data, and text data.
And our goal is to see if we can predict some values of important real world variables that matter to us.
For example, someone's house condition, or the weather, or etc.
And so these variables would be important because we might want to act on that.
We might want to make decisions based on that.
So how can we get from the data to these predicted values?
Well in general we'll first have to do data mining and analysis of the data.
Because we, in general, should treat all the data that we collected in such a prediction problem set up.
We are very much interested in joint mining of non-text and text data, which should combine all the data together.
And then, through analysis, generally there are multiple predictors of this interesting variable to us.
And we call these features.
And these features can then be put into a predictive model, to actually predict the value of any interesting variable.
So this then allows us to change the world.
And so this basically is the general process for making a prediction based on data, including the test data.
Now it's important to emphasize that a human actually plays a very important role in this process.
Especially because of the involvement of text data.
So human first would be involved in the mining of the data.
It would control the generation of these features.
And it would also help us understand the text data, because text data are created to be consumed by humans.
Humans are the best in consuming or interpreting text data.
But when there are, of course, a lot of text data then machines have to help and that's why we need to do text data mining.
Sometimes machines can see patterns in a lot of data that humans may not see.
But in general human would play an important role in analyzing some text data, or applications.
Next, human also must be involved in predictive model building and adjusting or testing.
So in particular, we will have a lot of domain knowledge about the problem of prediction that we can build into this predictive model.
And then next, of course, when we have predictive values for the variables, then humans would be involved in taking actions to change a word or make decisions based on these particular values.
And finally it's interesting that a human could be involved in controlling the sensors.
And this is so that we can adjust to the sensors to collect the most useful data for prediction.
So that's why I call this data mining loop.
Because as we perturb the sensors, it'll collect the new data and more useful data then we will obtain more data for prediction.
And this data generally will help us improve the predicting accuracy.
And in this loop, humans will recognize what additional data will need to be collected.
And machines, of course, help humans identify what data should be collected next.
In general, we want to collect data that is most useful for learning.
And there was actually a subarea in machine learning called active learning that has to do with this.
How do you identify data points that would be most helpful in machine learning programs?
If you can label them, right?
So, in general, you can see there is a loop here from data acquisition to data analysis.
Or data mining to prediction of values.
And to take actions to change the word, and then observe what happens.
And then you can then decide what additional data have to be collected by adjusting the sensors.
Or from the prediction arrows, you can also note what additional data we need to acquire in order to improve the accuracy of prediction.
And this big picture is actually very general and it's reflecting a lot of important applications of big data.
So, it's useful to keep that in mind while we are looking at some text mining techniques.
So from text mining perspective and we're interested in text based prediction.
Of course, sometimes texts alone can make predictions.
And this is most useful for prediction about human behavior or human preferences or opinions.
But in general text data will be put together as non-text data.
So the interesting questions here would be, first, how can we design effective predictors?
And how do we generate such effective predictors from text?
And this question has been addressed to some extent in some previous lectures where we talked about what kind of features we can design for text data.
And it has also been addressed to some extent by talking about the other knowledge that we can mine from text.
So, for example, topic mining can be very useful to generate the patterns or topic based indicators or predictors that can be further fed into a predictive model.
So topics can be intermediate recognition of text.
That would allow us to do design high level features or predictors that are useful for prediction of some other variable.
It may be also generated from original text data, it provides a much better implementation of the problem and it serves as more effective predictors.
And similarly similar analysis can lead to such predictors, as well.
So, those other data mining or text mining algorithms can be used to generate predictors.
The other question is, how can we join the mine text and non-text data together?
Now, this is a question that we have not addressed yet.
So, in this lecture, and in the following lectures, we're going to address this problem.
Because this is where we can generate much more enriched features for prediction.
And allows us to review a lot of interesting knowledge about the world.
These patterns that are generated from text and non-text data themselves can sometimes, already be useful for prediction.
But, when they are put together with many other predictors they can really help improving the prediction.
Basically, you can see text-based prediction can actually serve as a unified framework to combine many text mining and analysis techniques.
Including topic mining and any content mining techniques or segment analysis.
The goal here is mainly to evoke values of real-world variables.
But in order to achieve the goal we can do some other preparations.
And these are subtasks.
So one subtask could mine the content of text data, like topic mining.
And the other could be to mine knowledge about the observer.
So sentiment analysis, opinion.
And both can help provide predictors for the prediction problem.
And of course we can also add non-text data directly to the predicted model, but then non-text data also helps provide a context for text analyst.
And that further improves the topic mining and the opinion analysis.
And such improvement often leads to more effective predictors for our problems.
It would enlarge the space of patterns of opinions of topics that we can mine from text and that we'll discuss more later.
So the joint analysis of text and non-text data can be actually understood from two perspectives.
One perspective, we have non-text can help with testimony.
Because non-text data can provide a context for mining text data provide a way to partition data in different ways.
And this leads to a number of type of techniques for contextual types of mining.
And that's the mine text in the context defined by non-text data.
And you see this reference here, for a large body of work, in this direction.
And I will need to highlight some of them, in the next lectures.
Now, the other perspective is text data can help with non-text data mining as well.
And this is because text data can help interpret patterns discovered from non-text data.
Let's say you discover some frequent patterns from non-text data.
Now we can use the text data associated with instances where the pattern occurs as well as text data that is associated with instances where the pattern doesn't look up.
And this gives us two sets of text data.
And then we can see what's the difference.
And this difference in text data is interpretable because text content is easy to digest.
And that difference might suggest some meaning for this pattern that we found from non-text data.
So, it helps interpret such patterns.
And this technique is called pattern annotation.
And you can see this reference listed here for more detail.
So here are the references that I just mentioned.
The first is reference for pattern annotation.
The second is, Qiaozhu Mei's dissertation on contextual text mining.
It contains a large body of work on contextual text mining techniques.
.
And here what will do is talk about basic strategy, and that would be based on similarity of users and then predicting the rating of an object by a, a, active user using the ratings of similar users to this active user.
This is called a memory-based approach because it's a little bit similar to storing all the user information.
And when we are considering a particular user, we're going to try to kind of retrieve the relevant users, or the similar users through this user case.
And then try to use that user's information about those users to predict the preference of this user.
So here's the general idea, and we use some notations here, so.
X sub i j denotes the rating of object o j by user u i.
And n sub i is average rating of all objects by this user.
So this n i is needed.
Because we would like to normalize the ratings of objects by this user.
So how do you do normalization?
Well, where do you adjust that?
Subtract the, the average rating from all the ratings.
Now this is the normalized ratings so that the ratings from different users will be comparable.
Because some users might be more generous and they generally give more high ratings.
But, some others might be more critical.
So, their ratings can not be directly compared with each other or aggregated them together.
So, we need to do this normalization.
Now, the prediction of the rating.
On the item by another user or active user, u sub a here can be based on the average ratings of similar users.
So the user u sub a is the user that we are interested in recommending items to.
And we now are interested in recommending this o sub j.
So we're interested in knowing how likely this user will like this object.
How do we know that?
Well the idea here is to look at the how whether similar users to this user have liked this object.
So mathematically, this is, as you say, the predict the rating of this user on this app, object.
User A on object Oj is basically combination of the normalized ratings of different users.
And in fact, here, we're picking a sum of all the users.
But not all users contribute equally to the average.
And this is controlled by the weights.
So this.
Weight controls the inference of a user on the prediction.
And of course, naturally this weight should be related to the similarity between ua and this particular user, ui.
The more similar they are then the more contribution we would like user u i to make in predicting the preference of u a.
So the formula is extremely simple.
You're going to see it's a sum of all the possible users.
And inside the sum, we have their ratings, well their normalized ratings as I just explained.
The ratings need to be normalized in order to be comfortable with each other.
And then these ratings are rated by their similarity.
So we can imagine a W of A and I is just a similarity of user A user I.
Now, what's k here?
Well, k is a simpler normalizer.
It's just it's just one over the sum of all the weights, over all the users.
And so this means, basically, if you consider the weight here together with k. And we have coefficients or weights that would sum to one for all the users.
And it's just a normalization strategy, so that you get this predicted rating in the same range as the these ratings that we use to make the prediction.
Right?
So, this is basically the main idea of memory-based approaches for collaborative filtering, okay?
Once we make this prediction, we also would like to map back to the rating that the user.
The user would actually make.
And this is to further add the, mean rating or average rating of this user u sub a to the predicted value.
This would recover.
A meaningful rating for this user.
So if this user is generous, then the average would be somewhat high, and when we added that, the rating will be adjusted to a relatively high rating.
Now, when you recommend an item to a user, this actually doesn't really matter because you are interested in basically the normalized rating that's more meaningful.
But when they evaluate these collaborative filtering approach is typically assumed that actual ratings of user on these objects to be unknown.
And then you do the prediction and then you compare the predicted ratings with their actual ratings.
So they, you do have access to the actual ratings.
But then you pretend you don't know.
And then you compare real systems predictions with the actual ratings.
In that case, obviously the system's prediction would have to be adjusted to match the actual result the user, and this is not what's happening here, basically.
Okay?
So this is the memory-based approach.
Now of course if you look at the formula, if you want to write the program to implement it.
You still face the problem of determining what is this w function, right?
Once you know the w function, then the formula is very easy to implement.
So indeed there are many different ways to compute this function or this weight, w. And, specific approaches generally differ in how this is computed.
So, here are some possibilities.
And, you can imagine, there are many pro, other possibilities.
One popular approach is we use the Pearson Correlation Coefficient.
This would be a sum of a common range of items, and the formula is a standard Pearson correlation coefficient formula, as shown here.
So, this basically measures weather the two users tended to all give higher ratings to similar items, or lower ratings to similar items.
Another measure is the cosine measure and this is the retreat the rating vectors as vectors in the vector space, and then we're going to measure the the angel and compute the cosign of the angle of the two vectors.
And this measure has been used in the vector space more for retrieval as well.
So as you can imagine, there are so many different ways of doing that.
In all these cases, note that the user similarity is based on their preferences on items, and we did not actually use any content information of these items.
It didn't matter what these items are.
They can be movies, they can be books, they can be products, they can be tax documents.
We just didn't care about the content.
And so this allows such approach to be applied to a wide range of problems.
Now in some newer approaches of course, we would like to use more information about the user.
Clearly, we know more about the user, not just a, these preferences on these items.
And so in a actual filtering system, using collaborative filtering, we could also combine that with content-based filtering, we could use context information.
And those are all interesting approaches that people are still studying.
There are newer approaches proposed.
But this approach has been shown to work reasonably well and it's easy to implement.
And practical applications could be a starting point to see if the strand here works well for your application.
So there are some obvious ways to also improve this approach.
And mainly would like to improve the user similarity measure.
And there are some practical issues to deal with here as well.
So for example, there will be a lot of missing values.
What do you do with them?
Well, you can set them to default values or the average ratings of the user.
And that will be a simple solution.
But there are advantages to approaches that can actually try to predict those missing values and then use the predicted values to improve the similarity.
So in fact, the memory database approach, you can predict those with missing values, right?
So you can imagine, you have iterative approach where you first do some preliminary prediction and then you can use the predictor values to further improve the similarity function.
Right so this is here is a way to solve the problem.
And the strategy of this in the effect of the performance of clarity filtering, just like in the other heuristics, we improve the similarity function.
Another idea which is actually very similar to the idea of IDF that we have seen in text research, is called the inverse user frequency or IUF.
Now here the idea is to look at the where the two users share similar ratings.
If the item is a popular item that has been aah, viewed by many people and seemingly leads to people interested in this item may not be so interesting.
But if it's a rare item and has not been viewed by many users.
But, these two users [INAUDIBLE] to this item.
And they give similar ratings, and it says more about their similarity, right?
So it's kind of to emphasize more on similarity on items that are not viewed by many users.
This lecture is about Text Representation.
In this lecture we're going to discuss text representation and discuss how natural language processing can allow us to represent text in many different ways.
Let's take a look at this example sentence again.
We can represent this sentence in many different ways.
First, we can always represent such a sentence as a string of characters.
This is true for all the languages.
When we store them in the computer.
When we store a natural language sentence as a string of characters.
We have perhaps the most general way of representing text since we can always use this approach to represent any text data.
But unfortunately using such a representation will not help us to semantic analysis, which is often needed for many applications of text mining.
The reason is because we're not even recognizing words.
So as a string we are going to keep all of the spaces and these ascii symbols.
We can perhaps count out what's the most frequent character in the English text or the correlation between those characters.
But we can't really analyze semantics, yet this is the most general way of representing text because we hadn't used this to represent any natural language or text.
If we try to do a little bit more natural language processing by doing word segmentation, then we can obtain a representation of the same text, but in the form of a sequence of words.
So here we see that we can identify words, like a dog is chasing, etc.
Now with this level of representation we suddenly can do a lot of things.
And this is mainly because words are the basic units of human communication and natural language.
So they are very powerful.
By identifying words, we can for example, easily count what are the most frequent words in this document or in the whole collection, etc.
And these words can be used to form topics.
When we combine related words together and some words positive and some words are negatives or we can also do analysis.
So representing text data as a sequence of words opens up a lot of interesting analysis possibilities.
However, this level of representation is slightly less general than string of characters.
Because in some languages, such as Chinese, it's actually not that easy to identified all the word boundaries, because in such a language you see text as a sequence of characters with no space in between.
So you have to rely on some special techniques to identify words.
In such a language of course then we might make mistakes in segmenting words.
So the sequence of words representation is not as robust as string of characters.
But in English, it's very easy to obtain this level of representation.
So we can do that all the time.
Now if we go further to do in that round of processing we can add a part of these text.
Now once we do that we can count, for example, the most frequent nouns or what kind of nouns are associated with what kind of verbs, etc.
So, this opens up a little bit more interesting opportunities for further analysis.
Note that I use a plus sign here because by representing text as a sequence of part of speech tags, we don't necessarily replace the original word sequence written.
Instead, we add this as an additional way or representing text data.
So now the data is represented as both a sequence of words and a sequence of part of speech tags.
This enriches the representation of text data, and, thus also enables a more interesting analysis.
If we go further, then we'll be pausing the sentence to obtain a syntactic structure.
Now this of course will further open up more interesting analysis of, for example, the writing styles or correcting grammar mistakes.
If we go further for semantic analysis.
Then we might be able to recognize dog as an animal.
And we also can recognize boy as a person, and playground as a location.
And we can further analyse their relations.
For example, dog was chasing the boy, and boy is on the playground.
This will add more entities and relations, through entity relation recreation.
At this level, we can do even more interesting things.
For example, now we can counter easily the most frequent person that's managing this whole collection of news articles.
Or whenever you mention this person you also tend to see mentioning of another person, etc.
So this is very a useful representation.
And it's also related to the knowledge graph that some of you may have heard of that Google is doing as a more semantic way of representing text data.
However it's also less robust sequence of words.
Or even syntactical analysis, because it's not always easy to identify all the entities with the right types and we might make mistakes.
And relations are even harder to find and we might make mistakes.
This makes this level of representation less robust, yet it's very useful.
Now if we move further to logic group condition then we have predicates and inference rules.
With inference rules we can infer interesting derived facts from the text.
So that's very useful but unfortunately, this level of representation is even less robust and we can make mistakes.
And we can't do that all the time for all kinds of sentences.
And finally speech acts would add a yet another level of rendition of the intent of saying this sentence.
So in this case it might be a request.
So knowing that would allow us to you know analyze more even more interesting things about the observer or the author of this sentence.
What's the intention of saying that?
What scenarios or what kind of actions will be made?
So this is, Another role of analysis that would be very interesting.
So this picture shows that if we move down, we generally see more sophisticated and natural language processing techniques will be used.
And unfortunately such techniques would require more human effort.
And they are less accurate.
That means there are mistakes.
So if we analyze our text at the levels that are representing deeper analysis of language then we have to tolerate errors.
So that also means it's still necessary to combine such deep analysis with shallow analysis based on, for example, sequence of words.
On the right side, you see the arrow points down to indicate that as we go down, with our representation of text is closer to knowledge representation in our mind and need for solving a lot of problems.
Now, this is desirable because as we can represent text as a level of knowledge, we can easily extract the knowledge.
That's the purpose of text mining.
So, there was a trade off here.
Between doing deeper analysis that might have errors but would give us direct knowledge that can be extracted from text.
And doing shadow analysis which is more robust but wouldn't actually give us the necessary deeper representation of knowledge.
I should also say that text data are generated by humans, and are meant to be consumed by humans.
So as a result, in text data analysis, text mining, humans play a very important role.
They are always in the loop, meaning that we should optimize a collaboration of humans and computers.
So, in that sense it's okay that computers may not be able to have completely accurate representation of text data.
And patterns that are extracted from text data can be interpreted by humans.
And then humans can guide the computers to do more accurate analysis by annotating more data, by providing features to guide machine learning programs, to make them work more effectively.
This lecture is about, how we can evaluate a ranked list?
In this lecture, we will continue the discussion of evaluation.
In particular, we are going to look at, how we can evaluate a ranked list of results.
In the previous lecture, we talked about, precision-recall.
These are the two basic measures for, quantitatively measuring the performance of a search result.
But, as we talked about, ranking, before, we framed that the text of retrieval problem, as a ranking problem.
So, we also need to evaluate the, the quality of a ranked list.
How can we use precision-recall to evaluate, a ranked list?
Well, naturally, we have to look after the precision-recall at different, cut-offs.
Because in the end, the approximation of relevant documents, set, given by a ranked list, is determined by where the user stops browsing.
Right?
If we assume the user, securely browses, the list of results, the user would, stop at some point, and that point would determine the set.
And then, that's the most important, cut-off, that we have to consider, when we compute the precision-recall.
Without knowing where exactly user would stop, then we have to consider, all the positions where the user could stop.
So, let's look at these positions.
Look at this slide, and then, let's look at the, what if the user stops at the, the first document?
What's the precision-recall at this point?
What do you think?
Well, it's easy to see, that this document is So, the precision is one out of one.
We have, got one document, and that's relevent.
What about the recall?
Well, note that, we're assuming that, there are ten relevant documents, for this query in the collection, so, it's one out of ten.
What if the user stops at the second position?
Top two.
Well, the precision is the same, 100%, two out of two.
And, the record is two out of ten.
What if the user stops at the third position?
Well, this is interesting, because in this case, we have not got any, additional relevant document, so, the record does not change.
But the precision is lower, because we've got number [INAUDIBLE] so, what's exactly the precision?
Well, it's two out of three, right?
And, recall is the same, two out of ten.
So, when would see another point, where the recall would be different?
Now, if you look down the list, well, it won't happen until, we have, seeing another relevant document.
In this case D5, at that point, the, the recall is increased through three out of ten, and, the precision is three out of five.
So, you can see, if we keep doing this, we can also get to D8.
And then, we will have a precision of four out of eight, because there are eight documents, and four of them are relevant.
And, the recall is a four out of ten.
Now, when can we get, a recall of five out of ten?
Well, in this list, we don't have it, so, we have to go down on the list.
We don't know, where it is?
But, as convenience, we often assume that, the precision is zero, at all the, the othe, the precision are zero at all the other levels of recall, that are beyond the search results.
So, of course, this is a pessimistic assumption, the actual position would be higher, but we make, make this assumption, in order to, have an easy way to, compute another measure called Average Precision, that we will discuss later.
Now, I should also say, now, here you see, we make these assumptions that are clearly not, accurate.
But, this is okay, for the purpose of comparing to, text methods.
And, this is for the relative comparison, so, it's okay, if the actual measure, or actual, actual number deviates a little bit, from the true number.
As long as the deviation, is not biased toward any particular retrieval method, we are okay.
We can still, accurately tell which method works better.
And, this is important point, to keep in mind.
When you compare different algorithms, the key's to avoid any bias toward each method.
And, as long as, you can avoid that.
It's okay, for you to do transformation of these measures anyway, so, you can preserve the order.
Okay, so, we'll just talk about, we can get a lot of precision-recall numbers at different positions.
So, now, you can imagine, we can plot a curve.
And, this just shows on the, x-axis, we show the recalls.
And, on the y-axis, we show the precision.
So, the precision line was marked as .1, .2, .3, and, 1.0.
Right?
So, this is, the different, levels of recall.
And,, the y-axis also has, different amounts, that's for precision.
So, we plot the, these, precision-recall numbers, that we have got, as points on this picture.
Now, we can further, and link these points to form a curve.
As you'll see, we assumed all the other, precision as the high-level recalls, be zero.
And, that's why, they are down here, so, they are all zero.
And this, the actual curve probably will be something like this, but, as we just discussed, it, it doesn't matter that much, for comparing two methods.
because this would be, underestimated, for all the method.
Okay, so, now that we, have this precision-recall curve, how can we compare ranked to back list?
All right, so, that means, we have to compare two PR curves.
And here, we show, two cases.
Where system A is showing red, system B is showing blue, there's crosses.
All right, so, which one is better?
I hope you can see, where system A is clearly better.
Why?
Because, for the same level of recall, see same level of recall here, and you can see, the precision point by system A is better, system B.
So, there's no question.
In here, you can imagine, what does the code look like, for ideal search system?
Well, it has to have perfect, precision at all the recall points, so, it has to be this line.
That would be the ideal system.
In general, the higher the curve is, the better, right?
The problem is that, we might see a case like this.
This actually happens often.
Like, the two curves cross each other.
Now, in this case, which one is better?
What do you think?
Now, this is a real problem, that you actually, might have face.
Suppose, you build a search engine, and you have a old algorithm, that's shown here in blue, or system B.
And, you have come up with a new idea.
And, you test it.
And, the results are shown in red, curve A.
Now, your question is, is your new method better than the old method?
Or more, practically, do you have to replace the algorithm that you're already using, your, in your search engine, with another, new algorithm?
So, should we use system, method A, to replace method B?
This is going to be a real decision, that you to have to make.
If you make the replacement, the search engine would behave like system A here, whereas, if you don't do that, it will be like a system B.
So, what do you do?
Now, if you want to spend more time to think about this, pause the video.
And, it's actually very useful to think about that.
As I said, it's a real decision that you have to make, if you are building your own search engine, or if you're working, for a company that, cares about the search.
Now, if you have thought about this for a moment, you might realize that, well, in this case, it's hard to say.
Now, some users might like a system A, some users might like, like system B.
So, what's the difference here?
Well, the difference is just that, you know, in the, low level of recall, in this region, system B is better.
There's a higher precision.
But in high recall region, system A is better.
Now, so, that also means, it depends on whether the user cares about the high recall, or low recall, but high precision.
You can imagine, if someone is just going to check out, what's happening today, and want to find out something relevant in the news.
Well, which one is better?
What do you think?
In this case, clearly, system B is better, because the user is unlikely examining a lot of results.
The user doesn't care about high recall.
On the other hand, if you think about a case, where a user is doing you are, starting a problem.
You want to find, whether your idea ha, has been started before.
In that case, you emphasize high recall.
So, you want to see, as many relevant documents as possible.
Therefore, you might, favor, system A.
So, that means, which one is better?
That actually depends on users, and more precisely, users task.
So, this means, you may not necessarily be able to come up with one number, that would accurately depict the performance.
You have to look at the overall picture.
Yet, as I said, when you have a practical decision to make, whether you replace ours with another, then you may have to actually come up with a single number, to quantify each, method.
Or, when we compare many different methods in research, ideally, we have one number to compare, them with, so, that we can easily make a lot of comparisons.
So, for all these reasons, it is desirable to have one, single number to match it up.
So, how do we do that?
And, that, needs a number to summarize the range.
So, here again it's the precision-recall curve, right?
And, one way to summarize this whole ranked, list, for this whole curve, is look at the area underneath the curve.
Right?
So, this is one way to measure that.
There are other ways to measure that, but, it just turns out that,, this particular way of matching it has been very, popular, and has been used, since a long time ago for text And, this is, basically, in this way, and it's called the average precision.
Basically, we're going to take a, a look at the, every different, recall point.
And then, look out for the precision.
So, we know, you know, this is one precision.
And, this is another, with, different recall.
Now, this, we don't count to this one, because the recall level is the same, and we're going to, look at the, this number, and that's precision at a different recall level et cetera.
So, we have all these, you know, added up.
These are the precisions at the different points, corresponding to retrieving the first relevant document, the second, and then, the third, that follows, et cetera.
Now, we missed the many relevant documents, so, in all of those cases, we just, assume, that they have zero precisions.
And then, finally, we take the average.
So, we divide it by ten, and which is the total number of relevant documents in the collection.
Note that here, we're not dividing this sum by four.
Which is a number retrieved relevant documents.
Now, imagine, if I divide by four, what would happen?
Now, think about this, for a moment.
It's a common mistake that people, sometimes, overlook.
Right, so, if we, we divide this by four, it's actually not very good.
In fact, that you are favoring a system, that would retrieve very few random documents, as in that case, the denominator would be very small.
So, this would be, not a good matching.
So, note that this denomina, denominator is ten, the total number of relevant documents.
And, this will basically ,compute the area, and the needs occur.
And, this is the standard method, used for evaluating a ranked list.
Note that, it actually combines recall and, precision.
But first, you know, we have precision numbers here, but secondly, we also consider recall, because if missed many, there would be many zeros here.
All right, so, it combines precision and recall.
And furthermore, you can see this measure is sensitive to a small change of a position of a relevant document.
Let's say, if I move this relevant document up a little bit, now, it would increase this means, this average precision.
Whereas, if I move any relevant document, down, let's say, I move this relevant document down, then it would decrease, uh,the average precision.
So, this is a very good, because it's a very sensitive to the ranking of every relevant document.
It can tell, small differences between two ranked lists.
And, that is what we want, sometimes one algorithm only works slightly better than another.
And, we want to see this difference.
In contrast, if we look at the precision at the ten documents.
If we look at this, this whole set, well, what, what's the precision, what do you think?
Well, it's easy to see, that's a four out of ten, right?
So, that precision is very meaningful, because it tells us, what user would see?
So, that's pretty useful, right?
So, it's a meaningful measure, from a users perspective.
But, if we use this measure to compare systems, it wouldn't be good, because it wouldn't be sensitive to where these four relevant documents are ranked.
If I move them around the precision at ten, still, the same.
Right.
So, this is not a good measure for comparing different algorithms.
In contrast, the average precision is a much better measure.
It can tell the difference of, different, a difference in ranked list in, subtle ways.
This lecture is about the sentiment classification.
If we assume that most of the elements in the opinion representation are all ready known, then our only task may be just a sentiment classification, as shown in this case.
So suppose we know who's the opinion holder and what's the opinion target, and also know the content and the context of the opinion, then we mainly need to decide the opinion sentiment of the review.
So this is a case of just using sentiment classification for understanding opinion.
Sentiment classification can be defined more specifically as follows.
The input is opinionated text object, the output is typically a sentiment label, or a sentiment tag, and that can be designed in two ways.
One is polarity analysis, where we have categories such as positive, negative, or neutral.
The other is emotion analysis that can go beyond a polarity to characterize the feeling of the opinion holder.
In the case of polarity analysis, we sometimes also have numerical ratings as you often see in some reviews on the web.
Five might denote the most positive, and one maybe the most negative, for example.
In general, you have just disk holder categories to characterize the sentiment.
In emotion analysis, of course, there are also different ways for design the categories.
The six most frequently used categories are happy, sad, fearful, angry, surprised, and disgusted.
So as you can see, the task is essentially a classification task, or categorization task, as we've seen before, so it's a special case of text categorization.
This also means any textual categorization method can be used to do sentiment classification.
Now of course if you just do that, the accuracy may not be good because sentiment classification does requires some improvement over regular text categorization technique, or simple text categorization technique.
In particular, it needs two kind of improvements.
One is to use more sophisticated features that may be more appropriate for sentiment tagging as I will discuss in a moment.
The other is to consider the order of these categories, and especially in polarity analysis, it's very clear there's an order here, and so these categories are not all that independent.
There's order among them, and so it's useful to consider the order.
For example, we could use ordinal regression to do that, and that's something that we'll talk more about later.
So now, let's talk about some features that are often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis.
So let's start from the simplest one, which is character n-grams.
You can just have a sequence of characters as a unit, and they can be mixed with different n's, different lengths.
All right, and this is a very general way and very robust way to represent the text data.
And you could do that for any language, pretty much.
And this is also robust to spelling errors or recognition errors, right?
So if you misspell a word by one character and this representation actually would allow you to match this word when it occurs in the text correctly.
Right, so misspell the word and the correct form can be matched because they contain some common n-grams of characters.
But of course such a recommendation would not be as discriminating as words.
So next, we have word n-grams, a sequence of words and again, we can mix them with different n's.
Unigram's are actually often very effective for a lot of text processing tasks, and it's mostly because words are word designed features by humans for communication, and so they are often good enough for many tasks.
But it's not good, or not sufficient for sentiment analysis clearly.
For example, we might see a sentence like, it's not good or it's not as good as something else, right?
So in such a case if you just take a good and that would suggest positive that's not good, all right so it's not accurate.
But if you take a bigram, not good together, and then it's more accurate.
So longer n-grams are generally more discriminative, and they're more specific.
If you match it, and it says a lot, and it's accurate it's unlikely, very ambiguous.
But it may cause overfitting because with such very unique features that machine oriented program can easily pick up such features from the training set and to rely on such unique features to distinguish the categories.
And obviously, that kind of classify, one would generalize word to future there when such discriminative features will not necessarily occur.
So that's a problem of overfitting that's not desirable.
We can also consider part of speech tag, n-grams if we can do part of speech tagging an, for example, adjective noun could form a pair.
We can also mix n-grams of words and n-grams of part of speech tags.
For example, the word great might be followed by a noun, and this could become a feature, a hybrid feature, that could be useful for sentiment analysis.
So next we can also have word classes.
So these classes can be syntactic like a part of speech tags, or could be semantic, and they might represent concepts in the thesaurus or ontology, like WordNet.
Or they can be recognized the name entities, like people or place, and these categories can be used to enrich the presentation as additional features.
We can also learn word clusters and parodically, for example, we've talked about the mining associations of words.
And so we can have cluster of paradigmatically related words or syntaxmatically related words, and these clusters can be features to supplement the word base representation.
Furthermore, we can also have frequent pattern syntax, and these could be frequent word set, the words that form the pattern do not necessarily occur together or next to each other.
But we'll also have locations where the words my occur more closely together, and such patterns provide a more discriminative features than words obviously.
And they may also generalize better than just regular n-grams because they are frequent.
So you expected them to occur also in tested data.
So they have a lot of advantages, but they might still face the problem of overfeeding as the features become more complex.
This is a problem in general, and the same is true for parse tree-based features, when you can use a parse tree to derive features such as frequent subtrees, or paths, and those are even more discriminating, but they're also are more likely to cause over fitting.
And in general, pattern discovery algorithm's are very useful for feature construction because they allow us to search in a large space of possible features that are more complex than words that are sometimes useful.
So in general, natural language processing is very important that they derive complex features, and they can enrich text representation.
So for example, this is a simple sentence that I showed you a long time ago in another lecture.
So from these words we can only derive simple word n-grams, representations or character n-grams.
But with NLP, we can enrich the representation with a lot of other information such as part of speech tags, parse trees or entities, or even speech act.
Now with such enriching information of course, then we can generate a lot of other features, more complex features like a mixed grams of a word and the part of speech tags, or even a part of a parse tree.
So in general, feature design actually affects categorization accuracy significantly, and it's a very important part of any machine learning application.
In general, I think it would be most effective if you can combine machine learning, error analysis, and domain knowledge in design features.
So first you want to use the main knowledge, your understanding of the problem, the design seed features, and you can also define a basic feature space with a lot of possible features for the machine learning program to work on, and machine can be applied to select the most effective features or construct the new features.
That's feature learning, and these features can then be further analyzed by humans through error analysis.
And you can look at the categorization errors, and then further analyze what features can help you recover from those errors, or what features cause overfitting and cause those errors.
And so this can lead into feature validation that will revised the feature set, and then you can iterate.
And we might consider using a different features space.
So NLP enriches text recognition as I just said, and because it enriches the feature space, it allows much larger such a space of features and there are also many, many more features that can be very useful for a lot of tasks.
But be careful not to use a lot of category features because it can cause overfitting, or otherwise you would have to training careful not to let overflow happen.
So a main challenge in design features, a common challenge is to optimize a trade off between exhaustivity and the specificity, and this trade off turns out to be very difficult.
Now exhaustivity means we want the features to actually have high coverage of a lot of documents.
And so in that sense, you want the features to be frequent.
Specifity requires the feature to be discriminative, so naturally infrequent the features tend to be more discriminative.
So this really cause a trade off between frequent versus infrequent features.
And that's why a featured design is usually odd.
And that's probably the most important part in machine learning any problem in particularly in our case, for text categoration or more specifically the senitment classification.
This lecture is about the implementation of text retrieval systems.
In this lecture, we will discuss how we can implement a text retrieval method to build a search engine.
The main challenge is to manage a lot of text data and to enable a query to be answered very quickly and to respond to many queries.
This is a typical text retrieval system architecture.
We can see the documents are first processed by a tokenizer to get tokenizer units, for example words.
And then these words or tokens would be processed by an indexer that would create an index, which is a data structure for the search engine to use to quickly answer a query.
And the query will be going through a similar processing step.
So, the tokenizer will be apprised to query as well so that the text can be processed in the same way.
The same units will be matched with each other.
And the query's representation will then be given to the scorer.
Which would use a index to quickly answer a user's query by scoring the documents and then ranking them.
The results will be given to the user.
And then the user can look at the results and and provide some feedback that can be expressed judgements about which documents are good, which documents are bad, or implicit feedback such as pixels so the user doesn't have to any, anything extra.
The user will just look at the results and skip some and click on some results to view.
So these interaction signals can be used by the system to improve the ranking accuracy by assuming that viewed documents are better than the skipped ones.
So, a search engine system then can be divided into three parts.
The first part is the indexer, and the second part is the scorer, that responds to the user's query.
And the third part is the feedback mechanism.
Now typically, the indexer is done in the offline manner so you can pre-process the correct data and to build the inverter index which we will introduce in a moment.
And this data structure can then be used by the online module which is a scorer to process a user's query dynamically and quickly generate search results.
The feedback mechanism can be done online or offline depending on the method.
The implementation of the index and the, the scorer is fairly standard, and this is the main topic of this lecture and the next few lectures.
The feedback mechanism, on the other hand has variations.
It depends on what method is used.
So that is usually done in a algorithm-specific way.
Let's first talk about the tokenize.
Tokenization is a normalize lexical units into the same form so that semantically similar words can be matched with each other.
Now in the language of English stemming is often used and this what map all the inflectional forms of words into the same root form.
So for example, computer computation and computing can all be matched to the root form compute.
This way, all these different forms of computing can be matched with each other.
Normally this is a good idea to increase the coverage of documents that are matched with this query.
But it's also not always beneficial because sometimes the subtlest difference between computer and computation might still suggest the difference in the coverage of the content.
But in most cases, stemming seems to be beneficial.
When we tokenize the text in some other languages, for example Chinese, we might face some special challenges in segmenting the text to find the word boundaries.
Because it's not ob, obvious where the boundary is as there's no space separating them.
So, here, of course, we have to use some language-specific natural language processing techniques.
Once we do tokenization, then we would index the text documents, and that it will convert the documents into some data structure that can enable fast search.
The basic idea is to precompute as much as we can, basically.
So the most commonly used index is called a inverted index.
And this has been used, to, in many search engines to support basic search algorithms.
Sometimes other indices, for example a document index, might be needed in order to support a, a feedback.
Like I said, this, this kind of techniques are not really standard in that they vary a lot according to the feedback methods.
To understand why we are using inverted index.
It will be useful for you to think about how you would respond to a single term query quickly.
So if you want to use more time to think about that, pause the video.
So think about how you can preprocess the text data so that you can quickly respond to a query with just one word.
Well, if you have thought about question, you might realize that where the best is to simply create a list of documents that match every term in the vocabulary.
In this way, you can basically pre-construct the answers.
So when you see a term, you can simply just fetch the ranked list of documents for that term and return the list to the user.
So that's the fastest way to respond to single term query.
Now the idea of invert index is actually basically like that.
We can do, pre-construct such a index.
That would allow us to quickly find the, all the documents that match a particular term.
So let's take a look at this example.
We have three documents here, and these are the documents that you have seen in some previous lectures.
Suppose we want to create invert index for these documents, then we will need to maintain a dictionary.
In the dictionary we'll have one entry for each term.
And we're going to store some basic statistics about the term.
For example, the number of documents that match the term or the total number of, fre, total frequency of the term, which means we would encounter duplicated occurrences of the term.
And so, for example, news.
This term occurred in all the three documents.
So the count of documents is three.
And you might also realize we needed this count of documents or document frequency for computing some statistics to be used in the vector space model.
Can you think of that?
So, what waiting heuristic would need this count?
Well, that's the IDF, right, inverse document frequency.
So IDF is a property of the term, and we can compute it right here.
So with the document account here, it's easy to compute the IDF either at this time or when we build an index or.
At running time when we see a query.
Now in addition to these basic statistics we also saw all the documents that matched news.
And these entries are stored in a file called a Postings.
So in this case it matched 3 documents and we store Information about these 3 documents here.
This is the document id, document 1, and the frequency is 1.
The TF is 1 for news.
In the second document it's also 1, etc.
So from this list that we can get all the documents that match the term news.
And we can also know the frequency of news in these documents.
So, if the query has just one word, news, and we can easily look up in this table to find the entry and go quickly to the postings to fetch all the documents that match news.
So, let's take a look at another term.
Now this time let's take a look at the word presidential.
All right, this word occurred in only 1 document, document 3.
So, the document frequency is 1, but it occurred twice in this document.
And so the frequency count is 2, and the frequency count is used for, in some other retrieval method where we might use the frequency to assess the popularity of a, a term in the collection.
And similarly, we'll have a pointer to the postings, right here.
And in this case there is only one entry here because the term occurred in just one document.
And that's here.
The document id is 3, and it occurred twice.
So this is the basic idea of inverted index.
It's actually pretty simple, right?
With this structure we can easily fetch all the documents that match a term.
And this will be the basis for storing documents for our query.
Now sometimes we also want to store the positions of these terms.
So, in many of these cases the term occurred just once in the document so there's only one position, for example in this case.
But in this case the term occurred twice so it would store two positions.
Now the position information is very useful for checking whether the matching of query terms is actually within a small window of, let's say, five words, or ten words, or whether the matching of, the two query terms, is in fact a phrase of two words.
This can all be checked quickly by using the position information.
So why is inverted index good for faster search?
Well we just talked about the possibility of using the two ends of a single-word query.
And that's very easy.
What about a multiple-term queries?
Well, let's look at the, some special cases of the Boolean query.
A Boolean query is basically a Boolean expression, like this.
So I want the relevant document to match both term A AND term B.
All right, so that's one conjunctive query.
Or, I want the relevant documents to match term A OR term B.
That's a disjunctive query.
Now how can we answer such a query by using inverted index?
Well if you think a, a bit about it, it would be obvious.
Because we have simply to fetch all the documents that match term A and also fetch all the documents that match term B.
And then just take the intersection to answer a query like A and B.
Or to take the union to answer the query A or B.
So this is all very easy to answer.
It's going to be very quick.
Now what about the multi-term keyword query?
We talked about the vector space model for example.
And we would match such a query with a document and generate a score.
And the score is based on aggregated term weights.
So in this case it's not a Boolean query, but the scoring can be actually done in a similar way.
Basically it's similar to disjunctive Boolean query.
Basically It's like A OR B.
We take the union of all the, documents that matched at least one query term, and then we would aggregate the term weights.
So this is a, a, a basic idea of using inverted index for scoring documents in general.
And we're going to talk about this in more detail later.
But for now, let's just look at the question, why is inverted index, a good idea?
Basically, why is it more efficient than sequentially just scanning documents?
Right?
This is, the obvious approach.
You can just compute the score for each document, and then you can score them, sorry, you can then sort them.
This is a, a straightforward method.
But this is going to be very slow.
Imagine the web.
It has a lot of documents.
If you do this, then it will take a long time to answer your query.
So the question now is, why would the in, the inverted index be much faster?
Well it has to do with the word distribution in text.
So, here's some common phenomenon of word distribution in text.
There are some language-in, independent patterns that seem to be stable.
And these patterns are basically characterized by the following pattern.
A few words like the common words like the a, or we, occur very, very frequently in text.
So they account for a large percent of occurrences of words.
But most word would occur just rarely.
There are many words that occur just once, let's say, in a document, or once in the collection.
And there are many such single terms.
It's also true that the most frequent words in one corpus may actually be rare in another.
That means, although the general phenomenon is applicable or is observed in many cases, the exact words that are common may vary from context to context.
So this phenomena is characterized by what's called a Zipf's Law.
This law says that the rank of a word multiplied by, the frequency of the word is roughly constant.
So formally if we use F of w to denote the, frequency, r of w to denote the rank of a word, then this is the formula.
It basically says the same thing, just mathematical term, where C is, basically a constant, right, so as, so.
And there is also parameter alpha that might, be adjusted to better fit any empirical observations.
So if I plot the word frequencies in sorted order, then you can see this more easily.
The x-axis is basically the word rank.
And this is r of w. And the y-axis is the word frequency, or F of w. Now, this curve basically shows that the product of the two is roughly the constant.
Now, if you look these words, we can see.
They can be separated into three group2s.
In the middle it's the immediate frequency words.
These words tend to occur in quite a few documents, right?
But they're not like those most frequent words.
And they are also not very rare.
So they tend to be often used in in, in queries.
And they also tend to have high TFI diff weights in these intermediate frequency words.
But if you look at the left part of the curve.
These are the highest frequency words.
They occur very frequently.
They are usually stopper words, the, we, of, et cetera.
Those words are very, very frequently.
They are, in fact, a too frequently to be discriminated.
And they generally are not very useful for, for retrieval.
So, they are often removed, and this is called a stop words removal.
So you can use pretty much just the count of words in the collection to kind of infer what words might be stop words.
Those are basically the highest frequency words.
And they also occupy a lot of space in the invert index.
You can imagine the posting entries for such a word would be very long.
And then therefore, if you can remove such words, you can save a lot of space in the invert index.
We also show the tail part, which is, has a lot of rare words.
Those words don't occur very frequently, and there are many such words.
Those words are actually very useful for search, also, if a user happens to be interested in such a topic.
But because they're rare it's often true that users are, aren't the necessary interest in those words.
But retain them would allow us to match such a document accurately, and they generally have very high IDFs.
So what kind of data structures should we use to to store inverted index?
Well, it has two parts, right?
If you recall we have a dictionary, and we also have postings.
The dictionary has modest size, although for the web, it still wouldn't be very large.
But compared with postings, it's modest.
And we also need to have fast, random access to the entries because we want to look up the query term very quickly.
So, therefore, we prefer to keep such a dictionary in memory if it's possible.
Or, or, or if the connection is not very large, and this is visible.
But if the connection is very large, then it's in general not possible.
If the vocabulary size is very large, obviously we can't do that.
So, but in general, that's our goal.
So the data structures that we often use for storing dictionary would be direct access data structures, like a hash table or B-tree if we can't store everything in memory of the newest disk.
And but to try to build a structure that would allow it to quickly look up our entries.
Right.
For postings, they're huge, you can see.
And in general, we don't have to have direct access to a specific engine.
We generally would just look up a, a sequence of document IDs and frequencies for all of the documents that match a query term.
So we would read those entries sequentially.
And therefore, because it's large and we generate, have store postings on disk, so they have to stay on disk.
And they would contain information such as document IDs, term frequencies, or term positions, et cetera.
Now because they're very large, compression is often desirable.
Now this is not only to save disk space and this is of course, one benefit of compression.
It's not going to occupy that much space.
But it's also to help improving speed.
Can you see why?
Well, we know that input and output will cost a lot of time in comparison with the time taken by CPU.
So CPU is much faster.
But IO takes time.
And so by compressing the inverted index, the posting files will become smaller.
And the entries that we have to read into memory to process a query done, would would be smaller.
And then so we, we can reduce the amount of traffic and IO.
And that can save a lot of time.
Of course, we have to then do more processing of the data when we uncompress the, the data in the memory.
But as I said, CPU is fast, so overall, we can still save time.
So compression here is both to save disk space and to speed up the loading of the inverted index.
So now let's talk about the exchanging of PLSA to of LDA and to motivate that, we need to talk about some deficiencies of PLSA.
First, it's not really a generative model because we can compute the probability of a new document.
You can see why, and that's because the pis are needed to generate the document, but the pis are tied to the document that we have in the training data.
So we can't compute the pis for future document.
And there's some heuristic workaround, though.
Secondly, it has many parameters, and I've asked you to compute how many parameters exactly there are in PLSA, and you will see there are many parameters.
That means that model is very complex.
And this also means that there are many local maxima and it's prone to overfitting.
And that means it's very hard to also find a good local maximum.
And that we are representing global maximum.
And in terms of explaining future data, we might find that it will overfit the training data because of the complexity of the model.
The model is so flexible to fit precisely what the training data looks like.
And then it doesn't allow us to generalize the model for using other data.
This however is not a necessary problem for text mining because here we're often only interested in hitting the training documents that we have.
We are not always interested in modern future data, but in other cases, or if we would care about the generality, we would worry about this overfitting.
So LDA is proposing to improve that, and basically to make PLSA a generative model by imposing a Dirichlet prior on the model parameters.
Dirichlet is just a special distribution that we can use to specify product.
So in this sense, LDA is just a Bayesian version of PLSA, and the parameters are now much more regularized.
You will see there are many few parameters and you can achieve the same goal as PLSA for text mining.
It means it can compute the top coverage and topic word distributions as in PLSA.
However, there's no.
Why are the parameters for PLSA here are much fewer, there are fewer parameters and in order to compute a topic coverage and word distributions, we again face a problem of influence of these variables because they are not parameters of the model.
So the influence part again face the local maximum problem.
So essentially they are doing something very similar, but theoretically, LDA is a more elegant way of looking at the top and bottom problem.
So let's see how we can generalize the PLSA to LDA or a standard PLSA to have LDA.
Now a full treatment of LDA is beyond the scope of this course and we just don't have time to go in depth on that talking about that.
But here, I just want to give you a brief idea about what's extending and what it enables, all right.
So this is the picture of LDA.
Now, I remove the background of model just for simplicity.
Now, in this model, all these parameters are free to change and we do not impose any prior.
So these word distributions are now represented as theta vectors.
So these are word distributions, so here.
And the other set of parameters are pis.
And we would present it as a vector also.
And this is more convenient to introduce LDA.
And we have one vector for each document.
And in this case, in theta, we have one vector for each topic.
Now, the difference between LDA and PLSA is that in LDA, we're not going to allow them to free the chain.
Instead, we're going to force them to be drawn from another distribution.
So more specifically, they will be drawn from two Dirichlet distributions respectively, but the Dirichlet distribution is a distribution over vectors.
So it gives us a probability of four particular choice of a vector.
Take, for example, pis, right.
So this Dirichlet distribution tells us which vectors of pi is more likely.
And this distribution in itself is controlled by another vector of parameters of alphas.
Depending on the alphas, we can characterize the distribution in different ways but with full certain choices of pis to be more likely than others.
For example, you might favor the choice of a relatively uniform distribution of all the topics.
Or you might favor generating a skewed coverage of topics, and this is controlled by alpha.
And similarly here, the topic or word distributions are drawn from another Dirichlet distribution with beta parameters.
And note that here, alpha has k parameters, corresponding to our inference on the k values of pis for our document.
Whereas here, beta has n values corresponding to controlling the m words in our vocabulary.
Now once we impose this price, then the generation process will be different.
And we start with joined pis from the Dirichlet distribution and this pi will tell us these probabilities.
And then, we're going to use the pi to further choose which topic to use, and this is of course very similar to the PLSA model.
And similar here, we're not going to have these distributions free.
Instead, we're going to draw one from the Dirichlet distribution.
And then from this, then we're going to further sample a word.
And the rest is very similar to the.
The likelihood function now is more complicated for LDA.
But there's a close connection between the likelihood function of LDA and the PLSA.
So I'm going to illustrate the difference here.
So in the top, you see PLSA likelihood function that you have already seen before.
It's copied from previous slide.
Only that I dropped the background for simplicity.
So in the LDA formulas you see very similar things.
You see the first equation is essentially the same.
And this is the probability of generating a word from multiple word distributions.
And this formula is a sum of all the possibilities of generating a word.
Inside a sum is a product of the probability of choosing a topic multiplied by the probability of observing the word from that topic.
So this is a very important formula, as I've stressed multiple times.
And this is actually the core assumption in all the topic models.
And you might see other topic models that are extensions of LDA or PLSA.
And they all rely on this.
So it's very important to understand this.
And this gives us a probability of getting a word from a mixture model.
Now, next in the probability of a document, we see there is a PLSA component in the LDA formula, but the LDA formula will add a sum integral here.
And that's to account for the fact that the pis are not fixed.
So they are drawn from the original distribution, and that's shown here.
That's why we have to take an integral, to consider all the possible pis that we could possibly draw from this Dirichlet distribution.
And similarly in the likelihood for the whole collection, we also see further components added, another integral here.
Right?
So basically in the area we're just adding this integrals to account for the uncertainties and we added of course the Dirichlet distributions to cover the choice of this parameters, pis, and theta.
So this is a likelihood function for LDA.
Now, next to this, let's talk about the parameter as estimation and inferences.
Now the parameters can be now estimated using exactly the same approach maximum likelihood estimate for LDA.
Now you might think about how many parameters are there in LDA versus PLSA.
You'll see there're a fewer parameters in LDA because in this case the only parameters are alphas and the betas.
So we can use the maximum likelihood estimator to compute that.
Of course, it's more complicated because the form of likelihood function is more complicated.
But what's also important is notice that now these parameters that we are interested in name and topics, and the coverage are no longer parameters in LDA.
In this case we have to use basic inference or posterior inference to compute them based on the parameters of alpha and the beta.
Unfortunately, this computation is intractable.
So we generally have to resort to approximate inference.
And there are many methods available for that and I'm sure you will see them when you use different tool kits for LDA, or when you read papers about these different extensions of LDA.
Now here we, of course, can't give in-depth instruction to that, but just know that they are computed based in inference by using the parameters alphas and betas.
But our math [INAUDIBLE], actually, in the end, in some of our math list, it's very similar to PLSA.
And, especially when we use algorithm called class assembly, then the algorithm looks very similar to the Algorithm.
So in the end, they are doing something very similar.
So to summarize our discussion of properties of topic models, these models provide a general principle or way of mining and analyzing topics in text with many applications.
The best basic task setup is to take test data as input and we're going to output the k topics.
Each topic is characterized by word distribution.
And we're going to also output proportions of these topics covered in each document.
And PLSA is the basic topic model, and in fact the most basic of the topic model.
And this is often adequate for most applications.
That's why we spend a lot of time to explain PLSA in detail.
Now LDA improves over PLSA by imposing priors.
This has led to theoretically more appealing models.
However, in practice, LDA and PLSA tend to give similar performance, so in practice PLSA and LDA would work equally well for most of the tasks.
Now here are some suggested readings if you want to know more about the topic.
First is a nice review of probabilistic topic models.
The second has a discussion about how to automatically label a topic model.
Now I've shown you some distributions and they intuitively suggest a topic.
But what exactly is a topic?
Can we use phrases to label the topic?
To make it the more easy to understand and this paper is about the techniques for doing that.
The third one is empirical comparison of LDA and the PLSA for various tasks.
The conclusion is that they tend to perform similarly.
In this lecture, we're going to talk about text access.
In the previously lecture, we talked about natural language content analysis.
We explained that the state of the art natural language processing techniques are still not good enough to process a lot of unrestricted text data in a robust manner.
As a result, bag of words representation remains very popular in applications like search engines.
In this lecture we're going to talk about some high level strategies to help users get access to the text data.
This is also important step to convert raw, big text data into small relevant data that are actually needed in a specific application.
So the main question we address here is, how can a text information system help users get access to the relevant text data?
We're going to cover two complementary strategies, push vs pull.
And then we're going to talk about two ways to implement the pull mode, querying vs browsing.
So first, push vs pull.
These are two different ways to connect users with the right information at the right time.
The difference is which takes the initiative, which party it takes in the initiative.
In the pull mode, the users would take the initiative to start the information access process.
And in this case, a user typically would use a search engine to fulfill the goal.
For example, the user may type in a query, and then browse results to find the relevant information.
So this is usually appropriate for satisfying a user's ad hoc information need.
An ad hoc information need is a temporary information need.
For example, you want to buy a product so you suddenly have a need to read reviews about related products.
But after you have collected information, you have purchased your product, you generally no longer need such information.
So it's a temporary information need.
In such a case, it's very hard for a system to predict your need, and it's more appropriate for the users to take the initiative.
And that's why search engines are very useful today, because many people have many ad hoc information needs all the time.
So as we are speaking Google probably is processing many queries from this, and those are all, or mostly ad hoc information needs.
So this is a pull mode.
In contrast, in the push mode the system will take the initiative to push the information to the user or to recommend the information to the user.
So in this case, this is usually supported by a recommender system.
Now this would be appropriate if the user has a stable information need.
For example, you may have a research interest in some topic, and that interest tends to stay for a while, so it's relatively stable.
Your hobby is another example of a stable information need.
In such a case, the system can interact with you and can learn your interest, and then can monitor the information stream.
If it is, the system hasn't seen any relevant items to your interest, the system could then take the initiative to recommend information to you.
So for example, a news filter or news recommender system could monitor the news stream and identify interest in news to you, and simply push the news articles to you.
This mode of information access may be also appropriate when the system has a good knowledge about the user's need.
And this happens in the search context.
So for example, when you search for information on the web a search engine might infer you might be also interested in some related information.
And they would recommend the information to you.
So that should remind you for example, advertisement placed on a search page.
So this is about the, the two high level strategies or two modes of text access.
Now let's look at the pull mode in more detail.
In the pull mode, we can further this in usually two ways to help users, querying vs browsing.
In querying, a user would just enter a query, typically a keyword query, and the search engine system would return relevant documents to users.
And this works well when the user knows what exactly key, are the keywords to be used.
So if you know exactly what you're looking for you tend to know the right keywords, and then query would work very well.
And we do that all the time.
But we also know that sometimes it doesn't work so well, when you don't know the right keywords to use in the query or you want to browse information in some topic area.
In this case browsing would be more useful.
So in this case in the case of browsing the users would simply navigate into the relevant information by following the path that's supported by the structures on the documents.
So the system would maintain some kind of structures, and then the user could follow these structures to navigate.
So this strategy works well when the user wants to explore information space or the user doesn't know what are the keywords to use in the query.
Or simply because the user, finds it inconvenient to type in the query.
So even if a user knows what query to type in, if the user is using a cell phone to search for information, then it's still hard to enter the query.
In such a case again, browsing tends to be more convenient.
The relationship between browsing and the query is best understood by making an analogy to sightseeing.
Imagine if you are touring a city.
Now if you know the exact address of a attraction, then taking a taxi there is perhaps the fastest way, you can go directly to the site.
But if you don't know the exact address, you may need to walk around, or you can take a taxi to a nearby place, and then walk around.
It turns out that we do exactly the same in the information space.
If you know exactly what you are looking for, then you can use the right keywords in your query to find the information directly.
That's usually the fastest way to do find information.
But what if you don't know the exact keywords to use?
Well, your query probably won't work so well, and you will land on some related pages, and then you need to also walk around in the information space.
Meaning by following the links or by browsing, you can then finally get into the relevant page.
If you want to learn about a topic again, you you will likely do a lot of browsing.
So just like you are looking around in some area and you want to see some interesting attractions in a related in the same region.
So this analogy also tells us that today we have very good support for querying, but we don't really have good support for browsing.
And this is because in order to browse effectively, we need a a map to guide us, just like you need a map of Chicago to tour the city of Chicago.
You need a topical map to tour the information space.
So how to construct such a topical map is in fact a very interesting research question that likely will bring us more interesting browsing experience on the web or in other applications.
So to summarize this lecture, we have talked about two high level strategies for text access, push and pull.
Push tends to be supported by a recommender system and pull tends to be supported by a search engine.
Of course in the sophisticated intent in the information system, we should combine the two.
In the pull mode we have further distinguished querying and browsing.
Again, we generally want to combine the two ways to help users so that you can support both querying and browsing.
If you want to know more about the relationship between pull and push, you can read this article.
This gives a excellent discussion of the relationship between information filtering and information retrieval.
Here information filtering is similar to information recommendation, or the push mode of information access.
This lecture is a summary of this whole course.
First, let's revisit the topics that we covered in this course.
In the beginning, we talked about the natural language processing and how it can enrich text representation.
We then talked about how to mine knowledge about the language, natural language used to express the, what's observing the world in text and data.
In particular, we talked about how to mine word associations.
We then talked about how to analyze topics in text.
How to discover topics and analyze them.
This can be regarded as knowledge about observed world, and then we talked about how to mine knowledge about the observer and particularly talk about the, how to mine opinions and do sentiment analysis.
And finally, we will talk about the text-based prediction, which has to do with predicting values of other real world variables based on text data.
And in discussing this, we will also discuss the role of non-text data, which can contribute additional predictors for the prediction problem, and also can provide context for analyzing text data, and in particular we talked about how to use context to analyze topics.
So here are the key high-level take away messages from this cost.
I going to go over these major topics and point out what are the key take-away messages that you should remember.
First the NLP and text representation.
You should realize that NLP is always very important for any text replication because it enriches text representation.
The more NLP the better text representation we can have.
And this further enables more accurate knowledge discovery, to discover deeper knowledge, buried in text.
However, the current estate of art of natural energy processing is, still not robust enough.
So, as an result, the robust text mining technologies today, tend to be based on world [INAUDIBLE].
And tend to rely a lot on statistical analysis, as we've discussed in this course.
And you may recall we've mostly used word based representations.
And we've relied a lot on statistical techniques, statistical learning techniques particularly.
In word-association mining and analysis the important points first, we are introduced the two concepts for two basic and complementary relations of words, paradigmatic and syntagmatic relations.
These are actually very general relations between elements sequences.
If you take it as meaning elements that occur in similar context in the sequence and elements that tend to co-occur with each other.
And these relations might be also meaningful for other sequences of data.
We also talked a lot about test the similarity then we discuss how to discover paradynamic similarities compare the context of words discover words that share similar context.
At that point level, we talked about representing text data with a vector space model.
And we talked about some retrieval techniques such as BM25 for measuring similarity of text and for assigning weights to terms, tf-idf weighting, et cetera.
And this part is well-connected to text retrieval.
There are other techniques that can be relevant here also.
The next point is about co-occurrence analysis of text, and we introduce some information theory concepts such as entropy, conditional entropy, and mutual information.
These are not only very useful for measuring the co-occurrences of words, they are also very useful for analyzing other kind of data, and they are useful for, for example, for feature selection in text categorization as well.
So this is another important concept, good to know.
And then we talked about the topic mining and analysis, and that's where we introduce in the probabilistic topic model.
We spent a lot of time to explain the basic topic model, PLSA in detail and this is, those are the basics for understanding LDA which is.
Theoretically, a more opinion model, but we did not have enough time to really go in depth in introducing LDA.
But in practice, PLSA seems as effective as LDA and it's simpler to implement and it's also more efficient.
In this part of Wilson videos is some general concepts that would be useful to know, one is generative model, and this is a general method for modeling text data and modeling other kinds of data as well.
And we talked about the maximum life erase data, the EM algorithm for solving the problem of computing maximum estimator.
So, these are all general techniques that tend to be very useful in other scenarios as well.
Then we talked about the text clustering and the text categorization.
Those are two important building blocks in any text mining application systems.
In text with clustering we talked about how we can solve the problem by using a slightly different mixture module than the probabilistic topic model.
and we then also prefer to view the similarity based approaches to test for cuss word.
In categorization we also talk about the two kinds of approaches.
One is generative classifies that rely on to base word to infer the condition of or probability of a category given text data, in deeper we'll introduce you should use [INAUDIBLE] base in detail.
This is the practical use for technique, for a lot of text, capitalization tasks.
We also introduce the some discriminative classifiers, particularly logistical regression, can nearest labor and SBN.
They also very important, they are very popular, they are very useful for text capitalization as well.
In both parts, we'll also discuss how to evaluate the results.
Evaluation is quite important because if the matches that you use don't really reflect the volatility of the method then it would give you misleading results so its very important to get the variation right.
And we talked about variation of categorization in detail was a lot of specific measures.
Then we talked about the sentiment analysis and the paradigm and that's where we introduced sentiment classification problem.
And although it's a special case of text recalculation, but we talked about how to extend or improve the text recalculation method by using more sophisticated features that would be needed for sentiment analysis.
We did a review of some common use for complex features for text analysis, and then we also talked about how to capture the order of these categories, in sentiment classification, and in particular we introduced ordinal logistical regression then we also talked about Latent Aspect Rating Analysis.
This is an unsupervised way of using a generative model to understand and review data in more detail.
In particular, it allows us to understand the composed ratings of a reviewer on different aspects of a topic.
So given text reviews with overall ratings, the method allows even further ratings on different aspects.
And it also allows us to infer, the viewers laying their weights on these aspects or which aspects are more important to a viewer can be revealed as well.
And this enables a lot of interesting applications.
Finally, in the discussion of prediction, we mainly talk about the joint mining of text and non text data, as they are both very important for prediction.
We particularly talked about how text data can help non-text data and vice versa.
In the case of using non-text data to help text data analysis, we talked about the contextual text mining.
We introduced the contextual PLSA as a generalizing or generalized model of PLSA to allows us to incorporate the context of variables, such as time and location.
And this is a general way to allow us to reveal a lot of interesting topic of patterns in text data.
We also introduced the net PLSA, in this case we used social network or network in general of text data to help analyze puppets.
And finally we talk about how can be used as context to mine potentially causal Topics in text layer.
Now, in the other way of using text to help interpret patterns discovered from LAM text data, we did not really discuss anything in detail but just provide a reference but I should stress that that's after a very important direction to know about, if you want to build a practical text mining systems, because understanding and interpreting patterns is quite important.
So this is a summary of the key take away messages, and I hope these will be very useful to you for building any text mining applications or to you for the starting of these algorithms.
And this should provide a good basis for you to read from your research papers, to know more about more of allowance for other organisms or to invent new hours in yourself.
So to know more about this topic, I would suggest you to look into other areas in more depth.
And during this short period of time of this course, we could only touch the basic concepts, basic principles, of text mining and we emphasize the coverage of practical algorithms.
And this is after the cost of covering algorithms and in many cases we omit the discussion of a lot of algorithms.
So to learn more about the subject you should definitely learn more about the natural language process because this is foundation for all text based applications.
The more NLP you can do, the better the additional text that you can get, and then the deeper knowledge you can discover.
So this is very important.
The second area you should look into is the Statistical Machine Learning.
And these techniques are now the backbone techniques for not just text analysis applications but also for NLP.
A lot of NLP techniques are nowadays actually based on supervised machinery.
So, they are very important because they are a key to also understanding some advancing NLP techniques and naturally they will provide more tools for doing text analysis in general.
Now, a particularly interesting area, called deep learning has attracted a lot of attention recently.
It has also shown promise in many application areas, especially in speech and vision, and has been applied to text data as well.
So, for example, recently there has work on using deep learning to do segment analysis to achieve better accuracy.
So that's one example of [INAUDIBLE] techniques that we weren't able to cover, but that's also very important.
And the other area that has emerged in status learning is the water and baring technique, where they can learn better recognition of words.
And then these better recognitions will allow you confuse similarity of words.
As you can see, this provides directly a way to discover the paradigmatic relations of words.
And results that people have got, so far, are very impressive.
That's another promising technique that we did not have time to touch, but, of course, whether these new techniques would lead to practical useful techniques that work much better than the current technologies is still an open question that has to be examined.
And no serious evaluation has been done yet.
In, for example, examining the practical value of word embedding, other than word similarity and basic evaluation.
But nevertheless, these are advanced techniques that surely will make impact in text mining in the future.
So its very important to know more about these.
Statistical learning is also the key to predictive modeling which is very crucial for many big data applications and we did not talk about that predictive modeling component but this is mostly about the regression or categorization techniques and this is another reason why statistical learning is important.
We also suggest that you learn more about data mining, and that's simply because general data mining algorithms can always be applied to text data, which can be regarded as as special case of general data.
So there are many applications of data mining techniques.
In particular for example, pattern discovery would be very useful to generate the interesting features for test analysis and the reason that an information network that mining techniques can also be used to analyze text information at work.
So these are all good to know.
In order to develop effective text analysis techniques.
And finally, we also recommend you to learn more about the text retrieval, information retrieval, of search engines.
This is especially important if you are interested in building practical text application systems.
And a search ending would be an essential system component in any text-based applications.
And that's because texts data are created for humans to consume.
So humans are at the best position to understand text data and it's important to have human in the loop in big text data applications, so it can in particular help text mining systems in two ways.
One is through effectively reduce the data size from a large collection to a small collection with the most relevant text data that only matter for the particular interpretation.
So the other is to provide a way to annotate it, to explain parents, and this has to do with knowledge providence.
Once we discover some knowledge, we have to figure out whether or not the discovery is really reliable.
So we need to go back to the original text to verify that.
And that is why the search engine is very important.
Moreover, some techniques of information retrieval, for example BM25, vector space and are also very useful for text data mining.
We only mention some of them, but if you know more about text retrieval you'll see that there are many techniques that are used for it.
Another technique that it's used for is indexing technique that enables quick response of search engine to a user's query, and such techniques can be very useful for building efficient text mining systems as well.
So, finally, I want to remind you of this big picture for harnessing big text data that I showed you at your beginning of the semester.
So in general, to deal with a big text application system, we need two kinds text, text retrieval and text mining.
And text retrieval, as I explained, is to help convert big text data into a small amount of most relevant data for a particular problem, and can also help providing knowledge provenance, help interpreting patterns later.
Text mining has to do with further analyzing the relevant data to discover the actionable knowledge that can be directly useful for decision making or many other tasks.
So this course covers text mining.
And there's a companion course called Text Retrieval and Search Engines that covers text retrieval.
If you haven't taken that course, it would be useful for you to take it, especially if you are interested in building a text caching system.
And taking both courses will give you a complete set of practical skills for building such a system.
So in [INAUDIBLE] I just would like to thank you for taking this course.
I hope you have learned useful knowledge and skills in test mining and [INAUDIBLE].
As you see from our discussions there are a lot of opportunities for this kind of techniques and there are also a lot of open channels.
So I hope you can use what you have learned to build a lot of use for applications will benefit society and to also join the research community to discover new techniques for text mining and benefits.
Thank you.
This lecture is about how to use generative probabilistic models for text categorization.
There are in general about two kinds of approaches to text categorization by using machine learning.
One is by generating probabilistic models.
The other is discriminative approaches.
In this lecture, we're going to talk about the generative models.
In the next lecture, we're going to talk about discriminative approaches.
So the problem of text categorization is actually a very similar to document clustering.
In that, we'll assume that each document it belongs to one category or one cluster.
The main difference is that in clustering we don't really know what are the predefined categories are, what are the clusters.
In fact, that's the goal of text clustering.
We want to find such clusters in the data.
But in the case of categorization, we are given the categories.
So we kind of have pre-defined categories and then based on these categories and training data, we would like to allocate a document to one of these categories or sometimes multiple categories.
But because of the similarity of the two problems, we can actually get the document clustering models for text categorization.
And we understand how we can use generated models to do text categorization from the perspective of clustering.
And so, this is a slide that we've talked about before, about text clustering, where we assume there are multiple topics represented by word distributions.
Each topic is one cluster.
So once we estimated such a model, we faced a problem of deciding which cluster document d should belong to.
And this question boils down to decide which theta i has been used to generate d. Now, suppose d has L words represented as xi here.
Now, how can you compute the probability that a particular topic word distribution zeta i has been used to generate this document?
Well, in general, we use base wall to make this influence and you can see this prior information here that we need to consider if a topic or cluster has a higher prior then it's more likely that the document has been from this cluster.
And so, we should favor such a cluster.
The other is a likelihood part, it's this part.
And this has to do with whether the topic word of distribution can explain the content of this document well.
And we want to pick a topic that's high by both values.
So more specifically, we just multiply them together and then choose which topic has the highest product.
So more rigorously, this is what we'd be doing.
So we're going to choose the topic that would maximize.
This posterior probability at the top of a given document gets posterior because this one, p of the i, is the prior.
That's our belief about which topic is more likely, before we observe any document.
But this conditional probability here is the posterior probability of the topic after we have observed the document of d. And base wall allows us to update this probability based on the prior and I have shown the details, below here you can see how the prior here is related to the posterior, on the left-hand side.
And this is related to how well this word distribution explains the document here, and the two are related in this way.
So to find the topic that has the higher posterior probability here it's equivalent to maximize this product as we have seen also, multiple times in this course.
And we can then change the probability of document in your product of the probability of each word, and that's just because we've made an assumption about independence in generating each word.
So this is just something that you have seen in document clustering.
And we now can see clearly how we can assign a document to a category based on the information about word distributions for these categories and the prior on these categories.
So this idea can be directly adapted to do categorization.
And this is precisely what a Naive Bayes Classifier is doing.
So here it's most really the same information except that we're looking at the categorization problem now.
So we assume that if theta i represents category i accurately, that means the word distribution characterizes the content of documents in category i accurately.
Then, what we can do is precisely like what we did for text clustering.
Namely we're going to assign document d to the category that has the highest probability of generating this document.
In other words, we're going to maximize this posterior probability as well.
And this is related to the prior and the [INAUDIBLE] as you have seen on the previous slide.
And so, naturally we can decompose this [INAUDIBLE] into a product as you see here.
Now, here, I change the notation so that we will write down the product as product of all the words in the vocabulary, and even though the document doesn't contain all the words.
And the product is still accurately representing the product of all the words in the document because of this count here.
When a word, it doesn't occur in the document.
The count would be 0, so this time will just disappear.
So if actively we'll just have the product over other words in the document.
So basically, with Naive Bayes Classifier, we're going to score each category for the document by this function.
Now, you may notice that here it involves a product of a lot of small probabilities.
And this can cause and the four problem.
So one way to solve the problem is thru take logarithm of this function, which it doesn't changes all the often these categories.
But will helps us preserve precision.
And so, this is often the function that we actually use to score each category and then we're going to choose the category that has the highest score by this function.
So this is called an Naive Bayes Classifier, now the keyword base is understandable because we are applying a base rule here when we go from the posterior probability of the topic to a product of the likelihood and the prior.
Now, it's also called a naive because we've made an assumption that every word in the document is generated independently, and this is indeed a naive assumption because in reality they're not generating independently.
Once you see some word, then other words will more likely occur.
For example, if you have seen a word like a text.
Than that mixed category, they see more clustering more likely to appear than if you have not the same text.
But this assumption allows us to simplify the problem.
And it's actually quite effective for many text categorization tasks.
But you should know that this kind of model doesn't have to make this assumption.
We could for example, assume that words may be dependent on each other.
So that would make it a bigram analogy model or a trigram analogy model.
And of course you can even use a mixture model to model what the document looks like in each category.
So in nature, they will be all using base rule to do classification.
But the actual generating model for documents in each category can vary.
And here, we just talk about very simple case perhaps, the simplest case.
So now the question is, how can we make sure theta i actually represents category i accurately?
Now in clustering, we learned that this category i or what are the distributions for category i from the data.
But in our case, what can we do to make sure this theta i represents indeed category i.
Well if you think about the question, and you likely come up with the idea of using the training data.
Indeed in the textbook, we typically assume that there is training data available and those are the documents that unknown to have generator from which category.
In other words, these are the documents with known categories assigned and of course human experts must do that.
In here, you see that T1 represents the set of documents that are known to have the generator from category 1.
And T2 represents the documents that are known to have been generated from category 2, etc.
Now if you look at this picture, you'll see that the model here is really a simplified unigram language model.
It's no longer mixed modal, why?
Because we already know which distribution has been used to generate which documents.
There's no uncertainty here, there's no mixing of different categories here.
So the estimation problem of course would be simplified.
But in general, you can imagine what we want to do is estimate these probabilities that I marked here.
And what other probability is that we have to estimate it in order to do relation.
Well there are two kinds.
So one is the prior, the probability of theta i and this indicates how popular each category is or how likely will it have observed the document in that category.
The other kind is the water distributions and we want to know what words have high probabilities for each category.
So the idea then is to just use observe the training data to estimate these two probabilities.
And in general, we can do this separately for the different categories.
That's just because these documents are known to be generated from a specific category.
So once we know that, it's in some sense irrelevant of what other categories we are also dealing with.
So now this is a statistical estimation problem.
We have observed some data from some model and we want to guess the parameters of this model.
We want to take our best guess of the parameters.
And this is a problem that we have seen also several times in this course.
Now, if you haven't thought about this problem, haven't seen life based classifier.
It would be very useful for you to pause the video for a moment and to think about how to solve this problem.
So let me state the problem again.
So let's just think about with category 1, we know there is one word of distribution that has been used to generate documents.
And we generate each word in the document independently, and we know that we have observed a set of n sub 1 documents in the set of Q1.
These documents have been all generated from category 1.
Namely have been all generated using this same word distribution.
Now the question is, what would be your guess or estimate of the probability of each word in this distribution?
And what would be your guess of the entire probability of this category?
Of course, this singular probability depends on how likely are you to see documents in other categories?
So think for a moment, how do you use all this training data including all these documents that are known to be in these k categories, to estimate all these parameters?
Now, if you spend some time to think about this and it would help you understand the following few slides.
So do spend some time to make sure that you can try to solve this problem, or do you best to solve the problem yourself.
Now if you have thought about and then you will realize the following to it.
First, what's the bases for estimating the prior or the probability of each category.
Well this has to do with whether you have observed a lot of documents form that category.
Intuitively, you have seen a lot of documents in sports and very few in medical science.
Then you guess is that the probability of the sports category is larger or your prior on the category would be larger.
And what about the basis for estimating the probability of where each category?
Well the same, and you'll be just assuming that words that are observed frequently in the documents that are known to be generated from a category will likely have a higher probability.
And that's just a maximum Naive Bayes made of.
Indeed, that's what we can do, so this made the probability of which category and to answer the question, which category is most popular?
Then we can simply normalize, the count of documents in each category.
So here you see N sub i denotes the number of documents in each category.
And we simply just normalize these counts to make this a probability.
In other words, we make this probability proportional to the size of training intercept in each category that's a size of the set t sub i.
Now what about the word distribution?
Well, we do the same.
Again this time we can do this for each category.
So let's say, we're considering category i or theta i.
So which word has a higher probability?
Well, we simply count the word occurrences in the documents that are known to be generated from theta i.
And then we put together all the counts of the same word in the set.
And then we just normalize these counts to make this distribution of all the words make all the probabilities off these words to 1.
So in this case, you're going to see this is a proportional through the count of the word in the collection of training documents T sub i and that's denoted by c of w and T sub i.
Now, you may notice that we often write down probable estimate in the form of being proportional for certain numbers.
And this is often sufficient, because we have some constraints on these distributions.
So the normalizer is dictated by the constraint.
So in this case, it will be useful for you to think about what are the constraints on these two kinds of probabilities?
So once you figure out the answer to this question, and you will know how to normalize these accounts.
And so this is a good exercise to work on if it's not obvious to you.
There is another issue in Naive Bayes which is a smoothing.
In fact the smoothing is a general problem in older estimate of language morals.
And this has to do with, what would happen if you have observed a small amount of data?
So smoothing is an important technique to address that outsmarts this.
In our case, the training data can be small and when the data set is small when we use maximum likely estimator we often face the problem of zero probability.
That means if an event is not observed then the estimated probability would be zero.
In this case, if we have not seen a word in the training documents for let's say, category i.
Then our estimator would be zero for the probability of this one in this category, and this is generally not accurate.
So we have to do smoothing to make sure it's not zero probability.
The other reason for smoothing is that this is a way to bring prior knowledge, and this is also generally true for a lot of situations of smoothing.
When the data set is small, we tend to rely on some prior knowledge to solve the problem.
So in this case our [INAUDIBLE] says that no word should have zero probability.
So smoothing allows us to inject these to prior initial that no order has a real zero probability.
There is also a third reason which us sometimes not very obvious, but we explain that in a moment.
And that is to help achieve discriminative weighting of terms.
And this is also called IDF weighting, inverse document frequency weighting that you have seen in mining word relations.
So how do we do smoothing?
Well in general we add pseudo counts to these events, we'll make sure that no event has 0 count.
So one possible way of smoothing the probability of the category is to simply add a small non active constant delta to the count.
Let's pretend that every category has actually some extra number of documents represented by delta.
And in the denominator we also add a k multiplied by delta because we want the probability to some to 1.
So in total we've added delta k times because we have a k categories.
Therefore in this sum, we have to also add k multiply by delta as a total pseudocount that we add up to the estimate.
Now, it's interesting to think about the influence of that data, obvious data is a smoothing parameter here.
Meaning that the larger data is and the more we will do smoothing and that means we'll more rely on pseudocounts.
And we might indeed ignore the actual counts if they are delta is set to infinity.
Imagine what would happen if there are approaches positively to infinity?
Well, we are going to say every category has an infinite amount of documents.
And then there's no distinction to them so it become just a uniform.
What if delta is 0?
Well, we just go back to the original estimate based on the observed training data to estimate to estimate the probability of each category.
Now we can do the same for the word distribution.
But in this case, sometimes we find it useful to use a nonuniform seudocount for the word.
So here you'll see we'll add a pseudocounts to each word and that's mule multiplied by the probability of the word given by a background language model, theta sub b.
Now that background model in general can be estimated by using a logic collection of tests.
Or in this case we will use the whole set of all the training data to estimate this background language model.
But we don't have to use this one, we can use larger test data that are available from somewhere else.
Now if we use such a background language model that has pseudocounts, we'll find that some words will receive more pseudocounts.
So what are those words?
Well those are the common words because they get a high probability by the background average model.
So the pseudocounts added for such words will be higher.
Real words on the other hand will have smaller pseudocounts.
Now this addition of background model would cause a nonuniform smoothing of these word distributions.
We're going to bring the probability of those common words to a higher level, because of the background model.
Now this helps make the difference of the probability of such words smaller across categories.
Because every category has some help from the background four words, and I get the, a, which have high probabilities.
Therefore, it's not always so important that each category has documents that contain a lot of occurrences of such words or the estimate is more influenced by the background model.
And the consequence is that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities from the background language model.
Those words don't get some help from the background language model.
So the difference would be primary because of the differences of the occurrences in the training documents in different categories.
We also see another smoothing parameter mu here, which controls the amount of smoothing and just like a delta does for the other probability.
And you can easily understand why we add mu to the denominator, because that represents the sum of all the pseudocounts that we add for all the words.
So view is also a non negative constant and it's [INAUDIBLE] set to control smoothing.
Now there are some interesting special cases to think about as well.
First, let's think about when mu approaches infinity what would happen?
Well in this case the estimate would approach to the background language model we'll attempt to the background language model.
So we will bring every word distribution to the same background language model and that essentially remove the difference between these categories.
Obviously, we don't want to do that.
The other special case is the thing about the background model and suppose, we actually set the two uniform distribution.
And let's say, So each one has the same probability, then this smoothing formula is going to be very similar to the one on the top when we add delta.
It's because we're going to add a constant pseudocounts to every word.
So in general, in Naive Bayes categorization we have to do such a small thing.
And then once we have these probabilities, then we can compute the score for each category.
For a document and then choose the category where it was the highest score as we discussed earlier.
Now, it's useful to further understand whether the Naive Bayes scoring function actually makes sense.
So to understand that, and also to understand why adding a background model will actually achieve the effect of IDF weighting and to penalize common words.
So suppose we have just two categories and we're going to score based on their ratio of probability, right?
So this is the.
Lets say this is our scoring function for two categories, right?
So, this is a score of a document for these two categories.
And we're going to score based on this probability ratio.
So if the ratio is larger, then it means it's more likely to be in category one.
So the larger the score is the more likely the document is in category one.
So by using Bayes' rule, we can write down this ratio as follows, and you have seen this before.
Now, we generally take logarithm of this ratio, and to avoid small probabilities.
And this would then give us this formula in the second line.
And here we see something really interesting, because this is our scoring function for deciding between the two categories.
And if you look at this function, we'll see it has several parts.
The first part here is actually log of probability ratio.
And so this is a category bias.
It doesn't really depend on the document.
It just says which category is more likely and then we would then favor this category slightly, right?
So, the second part has a sum of all the words, right?
So, these are the words that are observed in the document but in general we can consider all the words in the vocabulary.
So here we're going to collect the evidence about which category is more likely, right?
So inside of the sum you can see there is product of two things.
The first, is a count of the word.
And this count of the word serves as a feature to represent the document.
And this is what we can collect from document.
The second part is the weight of this feature, here it's the weight on which word, right?
This weight tells us to what extent observing this word helps contribute in our decision to put this document in category one.
Now remember, the higher the scoring function is, the more likely it's in category one.
Now if you look at this ratio, basically, sorry this weight it's basically based on the ratio of the probability of the word from each of the two distributions.
Essentially we're comparing the probability of the word from the two distributions.
And if it's a higher according to theta 1, then according to theta 2, then this weight would be positive.
And therefore it means when we observe such a word, we will say that it's more likely to be from category one.
And the more we observe such a word, the more likely the document will be classified as theta 1.
If, on the other hand, the probability of the word from theta 1 is smaller than the probability of the word from theta 2, then you can see that this word is negative.
Therefore, this is negative evidence for supporting category one.
That means the more we observe such a word, the more likely the document is actually from theta 2.
So this formula now makes a little sense, right?
So we're going to aggregate all the evidence from the document, we take a sum of all the words.
We can call this the features that we collected from the document that would help us make the decision.
And then each feature has a weight that tells us how does this feature support category one or just support category two.
And this is estimated as the log of probability ratio here in nave Bayes.
And then finally we have this constant of bias here.
So that formula actually is a formula that can be generalized to accommodate more features and that's why I have introduce some other symbols here.
To introduce beta 0 to denote the Bayes and fi to denote the each feature and beta sub i to denote the weight on each feature.
Now we do this generalisation, what we see is that in general we can represent the document by feature vector fi, here of course in this case fi is the count of a word.
But in general, we can put any features that we think are relevant for categorization.
For example, document length or font size or count of other patterns in the document.
And then our scoring function can be defined as a sum of a constant beta 0 and the sum of the feature weights of all the features.
So if each f sub i is a feature value then we multiply the value by the corresponding weight, beta sub i, and we just take the sum.
And this is the aggregate of all evidence that we can collect from all these features.
And of course there are parameters here.
So what are the parameters?
Well, these are the betas.
These betas are weights.
And with a proper setting of the weights, then we can expect such a scoring function to work well to classify documents, just like in the case of naive Bayes.
We can clearly see naive Bayes classifier as a special case of this general classifier.
Actually, this general form is very close to a classifier called a logistical regression, and this is actually one of those conditional approaches or discriminative approaches to classification.
And we're going to talk more about such approaches later, but here I want you to note that there is a strong connection, a close connection between the two kinds of approaches.
And this slide shows how naive Bayes classifier can be connected to a logistic regression.
And you can also see that in discriminative classifiers that tend to use more general form on the bottom, we can accommodate more features to solve the problem.
This lecture is about the expectation maximization algorithms or also called the EM algorithms.
In this lecture, we're going to continue the discussion of probabilistic topic models.
In particular, we're going to introduce the EM algorithm.
Which is a family of useful algorithms for computing the maximum life or estimate of mixture models.
So, this is now a familiar scenario of using two components, the mixture model to try to fact out the background words from one topic or word distribution.
Yeah.
So, we're interested in computing this estimate and we're going to try to adjust these probability values to maximize the probability of the observed documents.
And know that we're assumed all the other parameters are known.
So, the only thing unknown is these water properties, this given by zero something.
And in this lecture, we're going to look into how to compute this maximum like or estimate.
Now this started with the idea of separating the words in the text data into two groups.
One group will be explained by the background model.
The other group will be explained by the unknown topical order.
After all this is the basic idea of the mixture model.
But, suppose we actually know which word is from which distribution.
So that would mean, for example, these words, the, is, and we, are known to be from this background origin, distribution.
On the other hand, the other words, text mining, clustering, etcetera are known to be from the topic word, distribution.
If you can see the color, that these are showing blue.
These blue words are, they are assumed to be from the topic word, distribution.
If we already know how to separate these words.
Then the problem of estimating the word distribution would be extremely simple, right?
If you think about this for a moment, you'll realize that, well, we can simply take all these words that are known to be from this word distribution, see that's a d and normalize them.
So indeed this problem would be very easy to solve if we had known which words are from which it is written precisely.
And this is in fact, making this model no longer a mystery model because we can already observe which of these distribution has been used to generate which part of the data.
So we, actually go back to the single order distribution problem.
And in this case, let's call these words that are known to be from theta d, a pseudo document of d prime.
And now all we have to do is just normalize these word accounts for each word, w sub i.
And that's fairly straightforward, and it's just dictated by the maximum estimator.
Now, this idea, however, doesn't work because we in practice, don't really know which word is from which distribution.
But this gives us an idea of perhaps we can guess which word is from which distribution.
Specifically, given all the parameters, can we infer the distribution a word is from?
So let's assume that we actually know tentative probabilities for these words in theta sub d. So now all the parameters are known for this mystery model.
Now let's consider word, like a text.
So the question is, do you think text is more likely, having been generated from theta sub d or from theta sub b?
So, in other words, we are to infer which distribution has been used to generate this text.
Now, this inference process is a typical of basing an inference situation, where we have some prior about these two distributions.
So can you see what is our prior here?
Well, the prior here is the probability of each distribution, right.
So the prior is given by these two probabilities.
In this case, the prior is saying that each model is equally likely.
But we can imagine perhaps a different apply is possible.
So this is called a pry because this is our guess of which distribution has been used to generate the word.
Before we even observed the word.
So that's why we call it a pry.
If we don't observe the word we don't know what word has been observed.
Our best guess is to say, well, they're equally likely.
So it's just like flipping a coin.
Now in basic inference, we typical them with our belief after we have observed the evidence.
So what is the evidence here?
Well, the evidence here is the word text.
Now that we know we're interested in the word text.
So text can be regarded as evidence.
And if we use base rule to combine the prior and the theta likelihood, what we will end up with is to combine the prior with the likelihood that you see here.
Which is basically the probability of the word text from each distribution.
And we see that in both cases text is possible.
Note that even in the background it is still possible, it just has a very small probability.
So intuitively what would be your guess seeing this case?
Now if you're like many others, you would guess text is probably from c.subd it's more likely from c.subd, why?
And you will probably see that it's because text has a much higher probability here by the C now sub D than by the background model which has a very small probability.
And by this we're going to say well, text is more likely from theta sub d. So you see our guess of which distributing has been used with the generated text would depend on how high the probability of the data, the text, is in each word distribution.
We can do tentative guess that distribution that gives is a word higher probability.
And this is likely to maximize the likelihood.
All right, so we are going to choose a word that has a higher likelihood.
So, in other words we are going to compare these two probabilities of the word given by each of these distributions.
But our guess must also be affected by the prior.
So we also need to compare these two priors.
Why?
Because imagine if we adjust these probabilities.
We're going to say, the probability of choosing a background model is almost 100%.
Now if we have that kind of strong prior, then that would affect your gas.
You might think, well, wait a moment, maybe texter could have been from the background as well.
Although the probability is very small here the prior is very high.
So in the end, we have to combine the two.
And the base formula provides us a solid and principle way of making this kind of guess to quantify that.
So more specifically, let's think about the probability that this word text has been generated in fact from theta sub d. Well, in order for text to be generated from theta sub d, two things must happen.
First, the theta sub d must have been selected.
So, we have the selection probability here.
And secondly we also have to actually have observed the text from the distribution.
So, when we multiply the two together, we get the probability that text has in fact been generated from zero sub d. Similarly, for the background model and the probability of generating text is another product of similar form.
Now we also introduced late in the variable z here to denote whether the word is from the background or the topic.
When z is 0, it means it's from the topic, theta sub d. When it's 1, it means it's from the background, theta sub B.
So now we have the probability that text is generated from each, then we can simply normalize them to have estimate of the probability that the word text is from theta sub d or from theta sub B.
And equivalently the probability that Z is equal to zero, given that the observed evidence is text.
So this is application of base rule.
But this step is very crucial for understanding the EM hours.
Because if we can do this, then we would be able to first, initialize the parameter values somewhat randomly.
And then, we're going to take a guess of these Z values and all, which distributing has been used to generate which word.
And the initialize the parameter values would allow us to have a complete specification of the mixture model, which allows us to apply Bayes' rule to infer which distribution is more likely to generate each word.
And this prediction essentially helped us to separate words from the two distributions.
Although we can't separate them for sure, but we can separate then probabilistically as shown here.
So average precision is computer for just one.
one query.
But we generally experiment with many different queries and this is to avoid the variance across queries.
Depending on the queries you use you might make different conclusions.
Right, so it's better then using more queries.
If you use more queries then, you will also have to take the average of the average precision over all these queries.
So how can we do that?
Well, you can naturally.
Think of just doing arithmetic mean as we always tend to, to think in, in this way.
So, this would give us what's called a "Mean Average Position", or MAP.
In this case, we take arithmetic mean of all the average precisions over several queries or topics.
But as I just mentioned in another lecture, is this good?
We call that.
We talked about the different ways of combining precision and recall.
And we conclude that the arithmetic mean is not as good as the MAP measure.
But here it's the same.
We can also think about the alternative ways of aggregating the numbers.
Don't just automatically assume that, though.
Let's just also take the arithmetic mean of the average position over these queries.
Let's think about what's the best way of aggregating them.
If you think about the different ways, naturally you will, probably be able to think about another way, which is geometric mean.
And we call this kind of average a gMAP.
This is another way.
So now, once you think about the two different ways.
Of doing the same thing.
The natural question to ask is, which one is better?
So.
So, do you use MAP or gMAP?
Again, that's important question.
Imagine you are again testing a new algorithm in, by comparing the ways your old algorithms made the search engine.
Now you tested multiple topics.
Now you've got the average precision for these topics.
Now you are thinking of looking at the overall performance.
You have to take the average.
But which, which strategy would you use?
Now first, you should also think about the question, well did it make a difference?
Can you think of scenarios where using one of them would make a difference?
That is they would give different rankings of those methods.
And that also means depending on the way you average or detect the.
Average of these average positions.
You will get different conclusions.
This makes the question becoming even more important.
Right?
So, which one would you use?
Well again, if you look at the difference between these.
Different ways of aggregating the average position.
You'll realize in arithmetic mean, the sum is dominating by large values.
So what does large value here mean?
It means the query is relatively easy.
You can have a high pres, average position.
Whereas gMAP tends to be affected more by low values.
And those are the queries that don't have good performance.
The average precision is low.
So if you think about the, improving the search engine for those difficult queries, then gMAP would be preferred, right?
On the other hand, if you just want to.
Have improved a lot.
Over all the kinds of queries or particular popular queries that might be easy and you want to make the perfect and maybe MAP would be then preferred.
So again, the answer depends on your users, your users tasks and their pref, their preferences.
So the point that here is to think about the multiple ways to solve the same problem, and then compare them, and think carefully about the differences.
And which one makes more sense.
Often, when one of them might make sense in one situation and another might make more sense in a different situation.
So it's important to pick out under what situations one is preferred.
As a special case of the mean average position, we can also think about the case where there was precisely one rank in the document.
And this happens often, for example, in what's called a known item search.
Where you know a target page, let's say you have to find Amazon, homepage.
You have one relevant document there, and you hope to find it.
That's call a "known item search".
In that case, there's precisely one relevant document.
Or in another application, like a question and answering, maybe there's only one answer.
Are there.
So if you rank the answers, then your goal is to rank that one particular answer on top, right?
So in this case, you can easily verify the average position, will basically boil down to reciprocal rank.
That is, 1 over r where r is the rank position of that single relevant document.
So if that document is ranked on the very top or is 1, and then it's 1 for reciprocal rank.
If it's ranked at the, the second, then it's 1 over 2.
Et cetera.
And then we can also take a, a average of all these average precision or reciprocal rank over a set of topics, and that would give us something called a mean reciprocal rank.
It's a very popular measure.
For no item search or, you know, an problem where you have just one relevant item.
Now again here, you can see this r actually is meaningful here.
And this r is basically indicating how much effort a user would have to make in order to find that relevant document.
If it's ranked on the top it's low effort that you have to make, or little effort.
But if it's ranked at 100 then you actually have to, read presumably 100 documents in order to find it.
So, in this sense r is also a meaningful measure and the reciprocal rank will take the reciprocal of r, instead of using r directly.
So my natural question here is why not simply using r?
I imagine if you were to design a ratio to, measure the performance of a random system, when there is only one relevant item.
You might have thought about using r directly as the measure.
After all, that measures the user's effort, right?
But, think about if you take a average of this over a large number of topics.
Again it would make a difference.
Right, for one single topic, using r or using 1 over r wouldn't make any difference.
It's the same.
Larger r with corresponds to a small 1 over r, right?
But the difference would only show when, show up when you have many topics.
So again, think about the average of Mean Reciprocal Rank versus average of just r. What's the difference?
Do you see any difference?
And would, would this difference change the oath of systems.
In our conclusion.
And this, it turns out that, there is actually a big difference, and if you think about it, if you want to think about it and then, yourself, then pause the video.
Basically, the difference is, if you take some of our directory, then.
Again it will be dominated by large values of r. So what are those values?
Those are basically large values that indicate that lower ranked results.
That means the relevant items rank very low down on the list.
And the sum that's also the average that would then be dominated by.
Where those relevant documents are ranked in, in ,in, in the lower portion of the ranked.
But from a users perspective we care more about the highly ranked documents.
So by taking this transformation by using reciprocal rank.
Here we emphasize more on the difference on the top.
You know, think about the difference between 1 and the 2, it would make a big difference, in 1 over r, but think about the 100, and 1, and where and when won't make much difference if you use this.
But if you use this there will be a big difference in 100 and let's say 1,000, right.
So this is not the desirable.
On the other hand, a 1 and So this is yet another case where there may be multiple choices of doing the same thing and then you need to figure out which one makes more sense.
So to summarize, we showed that the precision-recall curve.
Can characterize the overall accuracy of a ranked list.
And we emphasized that the actual utility of a ranked list depends on how many top ranked results a user would actually examine.
Some users will examine more.
Than others.
An average person uses a standard measure for comparing two ranking methods.
It combines precision and recall and it's sensitive to the rank of every random document.
This lecture is about the syntagmatic relation discovery and conditional entropy.
In this lecture, we're going to continue the discussion of word association mining and analysis.
We're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations.
Earlier, we talked about using entropy to capture how easy it is to predict the presence or absence of a word.
Now, we'll address a different scenario where we assume that we know something about the text segment.
So now the question is, suppose we know that eats occurred in the segment.
How would that help us predict the presence or absence of water, like in meat?
And in particular, we want to know whether the presence of eats has helped us predict the presence of meat.
And if we frame this using entrophy, that would mean we are interested in knowing whether knowing the presence of eats could reduce uncertainty about the meats.
Or, reduce the entrophy of the random variable corresponding to the presence or absence of meat.
We can also ask as a question, what if we know of the absents of eats?
Would that also help us predict the presence or absence of meat?
These questions can be addressed by using another concept called a conditioning entropy.
So to explain this concept, let's first look at the scenario we had before, when we know nothing about the segment.
So we have these probabilities indicating whether a word like meat occurs, or it doesn't occur in the segment.
And we have an entropy function that looks like what you see on the slide.
Now suppose we know eats is present, so now we know the value of another random variable that denotes eats.
Now, that would change all these probabilities to conditional probabilities.
Where we look at the presence or absence of meat, given that we know eats occurred in the context.
So as a result, if we replace these probabilities with their corresponding conditional probabilities in the entropy function, we'll get the conditional entropy.
So this equation now here would be the conditional entropy.
Conditional on the presence of eats.
So, you can see this is essentially the same entropy function as you have seen before, except that all the probabilities now have a condition.
And this then tells us the entropy of meat, after we have known eats occurring in the segment.
And of course, we can also define this conditional entropy for the scenario where we don't see eats.
So if we know it did not occur in the segment, then this entry condition of entropy would capture the instances of meat in that condition.
So now, putting different scenarios together, we have the completed definition of conditional entropy as follows.
Basically, we're going to consider both scenarios of the value of eats zero, one, and this gives us a probability that eats is equal to zero or one.
Basically, whether eats is present or absent.
And this of course, is the conditional entropy of meat in that particular scenario.
So if you expanded this entropy, then you have the following equation.
Where you see the involvement of those conditional probabilities.
Now in general, for any discrete random variables x and y, we have the conditional entropy is no larger than the entropy of the variable x.
So basically, this is upper bound for the conditional entropy.
That means by knowing more information about the segment, we want to be able to increase uncertainty.
We can only reduce uncertainty.
And that intuitively makes sense because as we know more information, it should always help us make the prediction.
And cannot hurt the prediction in any case.
Now, what's interesting here is also to think about what's the minimum possible value of this conditional entropy?
Now, we know that the maximum value is the entropy of X.
But what about the minimum, so what do you think?
I hope you can reach the conclusion that the minimum possible value, would be zero.
And it will be interesting to think about under what situation will achieve this.
So, let's see how we can use conditional entropy to capture syntagmatic relation.
Now of course, this conditional entropy gives us directly one way to measure the association of two words.
Because it tells us to what extent, we can predict the one word given that we know the presence or absence of another word.
Now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case, listed here.
That is, the conditional entropy of the word given itself.
So here, we listed this conditional entropy in the middle.
So, it's here.
So, what is the value of this?
Now, this means we know where the meat occurs in the sentence.
And we hope to predict whether the meat occurs in the sentence.
And of course, this is 0 because there's no incident anymore.
Once we know whether the word occurs in the segment, we'll already know the answer of the prediction.
So this is zero.
And that's also when this conditional entropy reaches the minimum.
So now, let's look at some other cases.
So this is a case of knowing the and trying to predict the meat.
And this is a case of knowing eats and trying to predict the meat.
Which one do you think is smaller?
No doubt smaller entropy means easier for prediction.
Which one do you think is higher?
Which one is not smaller?
Well, if you at the uncertainty, then in the first case, the doesn't really tell us much about the meat.
So knowing the occurrence of the doesn't really help us reduce entropy that much.
So it stays fairly close to the original entropy of meat.
Whereas in the case of eats, eats is related to meat.
So knowing presence of eats or absence of eats, would help us predict whether meat occurs.
So it can help us reduce entropy of meat.
So we should expect the sigma term, namely this one, to have a smaller entropy.
And that means there is a stronger association between meat and eats.
So we now also know when this w is the same as this meat, then the conditional entropy would reach its minimum, which is 0.
And for what kind of words would either reach its maximum?
Well, that's when this stuff is not really related to meat.
And like the for example, it would be very close to the maximum, which is the entropy of meat itself.
So this suggests that when you use conditional entropy for mining syntagmatic relations, the hours would look as follows.
For each word W1, we're going to enumerate the overall other words W2.
And then, we can compute the conditional entropy of W1 given W2.
We thought all the candidate was in ascending order of the conditional entropy because we're out of favor, a world that has a small entropy.
Meaning that it helps us predict the time of the word W1.
And then, we're going to take the top ring of the candidate words as words that have potential syntagmatic relations with W1.
Note that we need to use a threshold to find these words.
The stresser can be the number of top candidates take, or absolute value for the conditional entropy.
Now, this would allow us to mine the most strongly correlated words with a particular word, W1 here.
But, this algorithm does not help us mine the strongest that K syntagmatical relations from an entire collection.
Because in order to do that, we have to ensure that these conditional entropies are comparable across different words.
In this case of discovering the mathematical relations for a targeted word like W1, we only need to compare the conditional entropies for W1, given different words.
And in this case, they are comparable.
All right.
So, the conditional entropy of W1, given W2, and the conditional entropy of W1, given W3 are comparable.
They all measure how hard it is to predict the W1.
But, if we think about the two pairs, where we share W2 in the same condition, and we try to predict the W1 and W3.
Then, the conditional entropies are actually not comparable.
You can think of about this question.
Why?
So why are they not comfortable?
Well, that was because they have a different outer bounds.
Right?
So those outer bounds are precisely the entropy of W1 and the entropy of W3.
And they have different upper bounds.
So we cannot really compare them in this way.
So how do we address this problem?
Well later, we'll discuss, we can use mutual information to solve this problem.
.
This lecture is about link analysis for web search.
In this lecture we're going to talk about web search, and particularly focusing on how to do link analysis and use the results to improve search.
The main topic of this lecture is to look at the ranking algorithms for web search.
In the previous lecture, we talked about how to create index.
Now that we have got index, we want to see how we can improve ranking of pages on the web.
Standard IR models can also be applied here, in fact they are important building blocks for supporting web search, but they aren't sufficient, mainly for the following reasons.
First, on the web we tend to have very different information needs.
For example, people might search for a web page or entry page, and this is different from the traditional library search where people are primarily interested in collecting literature information.
So these kind of queries are often called navigational queries, the purpose is to navigate into a particular targeted page.
So for such queries, we might benefit from using link information.
Secondly, documents have additional information.
And on the web, web pages are web format.
There are a lot of other groups, such as the layout, the title, or link information again.
So this has provided an opportunity to use extra context information of the document to improve scoring.
And finally, information quality varies a lot.
So that means we have to consider many factors to improve the ranking algorithm.
This would give us a more robust way to rank the pages making it the harder for any spammer to just manipulate the one signal to improve the ranking of a page.
So as a result people have made a number of major extensions to the ranking algorithms.
One line is to exploit links to improve scoring and that's the main topic of this lecture.
People have also proposed algorithms to exploit large scale implicit feedback information in the form of clickthroughs.
That's of course in the category of feedback techniques, and machinery is often used there.
In general, in web search the ranking algorithms are based on machinery algorithms to combine all kinds of features.
And many of them are based on the standard original models such as BM25 that we talked about, or queried iCode to score different parts of documents or to, provide additional features based on content matching.
But link information is also very useful so they provide additional scoring signals.
So let's look at links in more detail on the web.
So this is a snapshot of some part of the web, let's say.
So we can see there are many links that link different pages together.
And in this case you can also look at the, the center here.
There is a description of a link that's pointing to the document on the right side.
Now this description text is called anchor text.
If you think about this text, it's actually quite useful because it provides some extra description of that page being pointed to.
So, for example, if someone wants to bookmark Amazon.com front page, the person might say, the big online bookstore, and then with a link to Amazon, right?
So the description here is actually very similar to what the user would type in in the query box when they are looking for such a page.
That's why it's very useful for, for, ranking pages.
Suppose someone types in a query like online bookstore or big online bookstore, right.
The query would match this anchor text in the page here.
And then this actually provides evidence for matching the page that's been pointed to, that is the Amazon entry page.
So if you match the anchor text that describes the link to a page, actually that provides good evidence for the relevance of the page being pointing to.
So anchor text is very useful.
If you look at the bottom part of this picture, you can also see there are some patterns of links, and these links might indicate the utility of a document.
So for example, on the right side you can see this page has received many in, in links.
That means many other pages are pointing to this page.
And this shows that this page is quite useful.
On the left side you can see, this is a page that points to many other pages.
So, this is a theater page that would allow you to actually see a lot of other pages.
So we can call the first case authority page and the second case a hub page.
This means the link information can help in two ways.
One is to provide extra text for matching.
The other is to provide some additional scores for the web pages to characterize how likely a page is a hub, how likely a page is a authority.
So people then, of course, propose ideas to leverage this, this link information.
Google's PageRank, which was a main technique that they used in early days, is a good example.
And that, that is the algorithm to capture page popularity, basically to score authority.
So the intuitions here are, links are just like citations in the literature.
Think about one page pointing to another page.
This is very similar to one paper citing another paper.
So, of course, then if a page is cited often, then we can assume this page to be more useful in general, right?
So that's a very good intuition.
Now, page rank is essentially to take advantage of this intuition to implement the, with the principle approach.
Intuitively it's essentially doing citation counting or in link counting.
It just improves this simple idea in, in two ways.
One is would consider indirect citations.
So that means you don't just look at the how many in links you have, you also look at the what are those pages that are pointing to you.
If those pages, themselves, have a lot of in links, well that means a lot.
In some sense you will get some credit from that.
But if those pages that are pointing to you are not are being pointed to by other pages, they themselves don't have many in links, then, well, you don't get that much credit.
So that's the idea of getting indirect citation.
Right, so you can also understand this idea by looking at, again, the research papers.
If you are cited by, let's say ten papers, and those ten papers are, just workshop papers and that, or some papers that are not very influential, right, so although you got ten in links, that's not as good as if you have, you're cited by ten papers that themselves have attracted a lot of other citations.
So this is a case where we would like to consider indirect links and PageRank does that.
The other idea is, it's good to smooth the citations.
Or, or, or assume that basically every page is having a non-zero pseudo citation count.
Essentially, you are trying to imagine there are many virtual links that will link all the pages together so that you, you actually get pseudo citations from everyone.
The, the reason why they want to do that is this would allow them to solve the problem elegantly with linear algebra technique.
So I think maybe the best way to understand the page rank is through think of this as through computer, the probability of a random surfer, visiting every web page, right.
This lecture is about text categorization.
In this lecture, we're going to talk about text categorization.
This is a very important technique for text data mining and analytics.
It is relevant to discovery of various different kinds of knowledge as shown here.
First, it's related to topic mining and analysis.
And, that's because it has to do with analyzing text to data based on some predefined topics.
Secondly, it's also related to opinion mining and sentiment analysis, which has to do with discovery knowledge about the observer, the human sensor.
Because we can categorize the authors, for example, based on the content of the articles that they have written, right?
We can, in general, categorize the observer based on the content that they produce.
Finally, it's also related to text-based prediction.
Because, we can often use text categorization techniques to predict some variables in the real world that are only remotely related to text data.
And so, this is a very important technique for text to data mining.
This is the overall plan for covering the topic.
First, we're going to talk about what is text categorization and why we're interested in doing that in this lecture?
And now, we're going to talk about how to do text categorization for how to evaluate the categorization results.
So, the problem of text categorization is defined as follows.
We're given a set of predefined categories possibly forming a hierarchy or so.
And often, also a set of training examples or training set of labeled text objects which means the text objects have already been enabled with known categories.
And then, the task is to classify any text object into one or more of these predefined categories.
So, the picture on this slide shows what happens.
When we do text categorization, we have a lot of text objects to be processed by a categorization system and the system will, in general, assign categories through these documents.
As shown on the right and the categorization results, and we often assume the availability of training examples and these are the documents that are tag with known categories.
And these examples are very important for helping the system to learn patterns in different categories.
And, this would further help the system then know how to recognize the categories of new text objects that it has not seen.
So, here are some specific examples of text categorization.
And in fact, there are many examples, here are just a few.
So first, text objects can vary, so we can categorize a document, or a passage, or a sentence, or collections of text.
As in the case of clustering, the units to be analyzed can vary a lot, so this creates a lot of possibilities.
Secondly, categories can also vary.
Allocate in general, there's two major kinds of categories.
One is internal categories.
These are categories that categorize content of text object.
For example, topic categories or sentiment categories and they generally have to do with the content of the text objects throughout the categorization of the content.
The other kind is external categories that can characterize an entity associated with the text object.
For example, authors are entities associated with the content that they produce.
And so, we can use their content in determining which author has written, which part, for example, and that's called author attribution.
Or, we can have any other mininal categories associate with text data as long as there is minimal connection between the entity and text data.
For example, we might collect a lot of reviews about a restaurant or a lot of reviews about a product, and then, this text data can help us infer properties of a product or a restaurant.
In that case, we can treat this as a categorization problem.
We can categorize restaurants or categorize products based on their corresponding reviews.
So, this is an example for external category.
Here are some specific examples of the applications.
News categorization is very common as being started a lot.
News agencies would like to assign predefined categories to categorize news generated everyday.
And, these virtual article categorizations are not important aspect.
For example, in the biomedical domain, there's MeSH annotations.
MeSH stands for Medical Subject Heading, and this is ontology of terms, characterize content of literature articles in detail.
Another example of application is spam email detection or filtering, right?
So, we often have a spam filter to help us distinguish spams from legitimate emails and this is clearly a binary classification problem.
Sentiment categorization of product reviews or tweets is yet another kind of applications where we can categorize, comparing to positive or negative or positive and negative or neutral.
So, you can have send them to categories, assign the two text content.
Another application is automatic email routing or sorting, so, you might want to automatically sort your emails into different folders and that's one application of text categorization where each folder is a category.
The results are another important kind of applications of routing emails to the right person to handle, so, in helpdesk, email messaging is generally routed to a particular person to handle.
Different people tend to handle different kinds of requests.
And in many cases, a person would manually assign the messages to the right people.
But, if you can imagine, you can't be able to automatically text categorization system to help routing request.
And, this is a class file, the incoming request in the one of the categories where each category actually corresponds to a person to handle the request.
And finally, author attribution, as I just mentioned, is yet another application, and it's another example of using text to actually infer properties of some other entities.
And, there are also many variants of the problem formulation.
And so, first, we have the simplest case, which is a binary categorization, where there are only two categories.
And, there are many examples like that, information retrieval or search engine.
Applications with one distinguishing relevant documents from non-relevant documents for a particular query.
Spam filtering just distinguishing spams from non-spams, so, also two categories.
Sometimes, classifications of opinions can be in two categories, positive and a negative.
A more general case would be K-category categorization and there are also many applications like that, there could be more than two categories.
So, topic categorization is often such an example where you can have multiple topics.
Email routing would be another example when you may have multiple folders or if you route the email to the right person to handle it, then there are multiple people to classify.
So, in all these cases, there are more than two kinds of categories.
Another variation is to have hierarchical categorization where categories form a hierarchy.
Again, topical hierarchy is very common.
Yet another variation is joint categorization.
That's when you have multiple categorization tasks that are related and then you hope to kind of join the categorization.
Further leverage the dependency of these tasks to improve accuracy for each individual task.
Among all these binary categorizations is most fundamental and part of it also is because it's simple and probably it's because it can actually be used to perform all the other categorization tasks.
For example, a K-category categorization task can be actually performed by using binary categorization.
Basically, we can look at each category separately and then the binary categorization problem is whether object is in this category or not, meaning in other categories.
And, the hierarchical categorization can also be done by progressively doing flat categorization at each level.
So, we have, first, we categorize all the objects into, let's say, a small number of high-level categories, and inside each category, we have further categorized to sub-categories, etc.
So, why is text categorization important?
Well, I already showed that you, several applications but, in general, there are several reasons.
One is text categorization helps enrich text representation and that's to achieve more understanding of text data that's all it was useful for text analysis.
So, now with categorization text can be represented in multiple levels.
The keyword conditions that's often used for a lot text processing tasks.
But we can now also add categories and they provide two levels of transition.
Semantic categories assigned can also be directly or indirectly useful for application.
So, for example, semantic categories could be already very useful or other attribution might be directly useful.
Another example is when semantic categories can facilitate aggregation of text content and this is another case of applications of text categorization.
For example, if we want to know the overall opinions about a product, we could first categorize the opinions in each individual view as positive or negative and then, that would allow us to easy to aggregate all the sentiment, and it would tell us about the 70% of the views are positive and 30% are negative, etc.
So, without doing categorization, it will be much harder to aggregate such opinions to provide a concise way of coding text in some sense based on all of the vocabulary.
And, sometimes you may see in some applications, text with categorizations called a text coded, encoded with some control of vocabulary.
The second kind of reasons is to use text categorization to infer properties of entities, and text categories allows us to infer the properties of such entities that are associate with text data.
So, this means we can use text categorization to discover knowledge about the world.
In general, as long as we can associate the entity with text of data, we can always the text of data to help categorize the corresponding entities.
So, it's used for single information network that will connect the other entities with text data.
The obvious entities that can be directly connected are authors.
But, you can also imagine the author's affiliations or the author's age and other things can be actually connected to text data indirectly.
Once we have made the connection, then we can make a prediction about those values.
So, this is a general way to allow us to use text mining through, so the text categorization to discover knowledge about the world.
Very useful, especially in big text data analytics where we are often just using text data as extra sets of data extracted from humans to infer certain decision factors often together with non-textual data.
Specifically with text, for example, we can also think of examples of inferring properties of entities.
For example, discovery of non-native speakers of a language.
And, this can be done by categorizing the content of speakers.
Another example is to predict the party affiliation of a politician based on the political speech.
And, this is again an example of using text data to infer some knowledge about the real world.
In nature, the problems are all the same, and that's as we defined and it's a text categorization problem.
This lecture is a overview of text retrieval methods.
In the previous lecture we introduced you to the problem of text retrieval.
We explained that the main problem is to design a ranking function to rank documents for a query.
In this lecture, we will give a overview of different ways of designing this ranking function.
So the problem is the following.
We have a query that has a sequence of words, and a document that, that's also a sequence of words, and we hope to define the function f that can compute a score based on the query and document.
So the main challenge you here is with designing a good ranking function that can rank all the relevant documents, on top of all the non-relevant ones.
Now clearly this means our function must be able to measure the likelihood that a document d is relevant to a query q.
That also means we have to have some way to define relevance.
In particular in order to implement the program to do that we have to have a computational definition of relevance, and we achieve this goal by designing a retrieval model, which gives us a formalization of relevance.
Now, over many decades, researchers have designed many different kinds of retrieval models, and they fall into different categories.
First, one fair many of the models are based on the similarity idea.
Basically, we assume that if a document is more similar to the query than another document is, then we would say the first document is more relevant than the second one.
So in this case, the ranking function is defined as the similarity between the query and the document.
One well known example in this case is vector space model, which we will cover more in detail later in the lecture.
The second kind of models are called probabilistic models.
In this family of models, we follow a very different strategy.
While we assume that queries and documents are all observations from random variables, and we assume there is a binary random variable called R here, to indicate whether a document is relevant to a query.
We then define the score of document with respect to a query as is a probability that this random variable R is equal to 1, given a particular document and query.
There are different cases of such a general idea.
One is classic probabilistic model, another is language model, yet another is divergence-from-randomness model.
In a later lecture, we will talk more about the, one case, which is language model.
The third kind of models of this is probabilistic inference.
So here the idea is to associate uncertainty to inference rules.
And we can then quantify the probability that we can show that the query follows from the document.
Finally, there is also a family of models that are using axiomatic thinking.
Here the idea is to define a set of constraints that we hope a good retrieval function to satisfy.
So in this case the problem is you seek a good ranking function that can satisfy all the desired constraints.
Interestingly, although these different models are based on different thinking, in the end the retrieval function tends to be very similar.
And these functions tend to also involve similar variables.
So now let's take a look at the, the common form of a state of that retrieval model and examine some of the common ideas used in all these models.
First, these models are all based on the assumption of using bag of words for representing text.
And we explained this in the natural language processing lecture.
Bag of words representation remains the main representation used in all the search engines.
So, with this assumption, the score of a query like a presidential campaign news, with respect to a document d here, would be based on scores computed at, based on each individual word.
And that means the score would depend on the score of each word, such as presidential, campaign, and news.
Here we can see there are three different components, each corresponding to how well the document matches each of the query words.
Inside of these functions, we see a number of heuristics views.
So for example, one factor that affects the function g here is how many times does the word presidential occur in the document?
This is called a Term Frequency or TF.
We might also denote as c of presidential and d. In general if the word occurs more frequently in the document then the value of this function would be larger.
Another factor is how long is the document, and this is so to use the document length for score.
In general, if a term occurs in a long document that many times, it's not as significant as if it occurred the same number of times in a short document.
Because in the long document any term is expected to occur more frequently.
Finally, there is this factor called a document frequency, and that is we also want to look at how often presidential occurs in the entire collection.
And we call this Document Frequency, or DF, of presidential.
And in some other models we might also use a probability to characterize this information.
So here, I show the probability of presidential in the collection.
So all these are trying to characterize the popularity of the term in the collection.
In general, matching a rare term in the collection is contributing more to the overall score then matching a common term.
So this captures some of the main ideas used in pretty much all the state of the art retrieval models.
So now, a natural question is which model works the best?
Now, it turns out that many models work equally well, so here I listed the four major models that are generally regarded as a state of the art retrieval models.
Pivoted length normalization, BM25, query likelihood, PL2.
When optimized these models tend to perform similarly and this was, discussed in detail in this reference at the end of this lecture.
Among all these, BM25 is probably the most popular.
It's most likely that this has been used in virtually all the search engines, and you will also often see this method discussed in research papers.
And we'll talk more about this method later in some other lectures.
So, to summarize, the main points made in this lecture are, first the design of a good ranking function pre-requires a computational definition of relevance, and we achieve this goal by designing a proper retrieval model.
Second, many models are equally effective but we don't have a single winner here.
Researchers are still actively working on this problem, trying to find a truly optimal retrieval model.
Finally, the state of the art ranking functions tend to rely on the following ideas.
First, bag of words representation.
Second, TF and the document frequency of words.
Such information is used when ranking function to determine the overall contribution of matching a word, and document length.
These are often combined in interesting ways and we'll discuss how exactly they are combined to rank documents in the lectures later.
There are two suggested additional readings if you have time.
The first is a paper where you can find a detailed discussion and comparison of multiple state of the art models.
The second, is a book with a chapter that gives a broad review of different retrieval models.
>> This lecture is about topic mining and analysis.
We're going to talk about its motivation and task definition.
In this lecture we're going to talk about different kind of mining task.
As you see on this road map, we have just covered mining knowledge about language, namely discovery of word associations such as paradigmatic and relations and syntagmatic relations.
Now, starting from this lecture, we're going to talk about mining another kind of knowledge, which is content mining, and trying to discover knowledge about the main topics in the text.
And we call that topic mining and analysis.
In this lecture, we're going to talk about its motivation and the task definition.
So first of all, let's look at the concept of topic.
So topic is something that we all understand, I think, but it's actually not that easy to formally define.
Roughly speaking, topic is the main idea discussed in text data.
And you can think of this as a theme or subject of a discussion or conversation.
It can also have different granularities.
For example, we can talk about the topic of a sentence.
A topic of article, aa topic of paragraph or the topic of all the research articles in the research library, right, so different grand narratives of topics obviously have different applications.
Indeed, there are many applications that require discovery of topics in text, and they're analyzed then.
Here are some examples.
For example, we might be interested in knowing about what are Twitter users are talking about today?
Are they talking about NBA sports, or are they talking about some international events, etc.?
Or we are interested in knowing about research topics.
For example, one might be interested in knowing what are the current research topics in data mining, and how are they different from those five years ago?
Now this involves discovery of topics in data mining literatures and also we want to discover topics in today's literature and those in the past.
And then we can make a comparison.
We might also be also interested in knowing what do people like about some products like the iPhone 6, and what do they dislike?
And this involves discovering topics in positive opinions about iPhone 6 and also negative reviews about it.
Or perhaps we're interested in knowing what are the major topics debated in 2012 presidential election?
And all these have to do with discovering topics in text and analyzing them, and we're going to talk about a lot of techniques for doing this.
In general we can view a topic as some knowledge about the world.
So from text data we expect to discover a number of topics, and then these topics generally provide a description about the world.
And it tells us something about the world.
About a product, about a person etc.
Now when we have some non-text data, then we can have more context for analyzing the topics.
For example, we might know the time associated with the text data, or locations where the text data were produced, or the authors of the text, or the sources of the text, etc.
All such meta data, or context variables can be associated with the topics that we discover, and then we can use these context variables help us analyze patterns of topics.
For example, looking at topics over time, we would be able to discover whether there's a trending topic, or some topics might be fading away.
Soon you are looking at topics in different locations.
We might know some insights about people's opinions in different locations.
So that's why mining topics is very important.
Now, let's look at the tasks of topic mining and analysis.
In general, it would involve first discovering a lot of topics, in this case, k topics.
And then we also would like to know, which topics are covered in which documents, to what extent.
So for example, in document one, we might see that Topic 1 is covered a lot, Topic 2 and Topic k are covered with a small portion.
And other topics, perhaps, are not covered.
Document two, on the other hand, covered Topic 2 very well, but it did not cover Topic 1 at all, and it also covers Topic k to some extent, etc., right?
So now you can see there are generally two different tasks, or sub-tasks, the first is to discover k topics from a collection of text laid out.
What are these k topics?
Okay, major topics in the text they are.
The second task is to figure out which documents cover which topics to what extent.
So more formally, we can define the problem as follows.
First, we have, as input, a collection of N text documents.
Here we can denote the text collection as C, and denote text article as d i.
And, we generally also need to have as input the number of topics, k. But there may be techniques that can automatically suggest a number of topics.
But in the techniques that we will discuss, which are also the most useful techniques, we often need to specify a number of topics.
Now the output would then be the k topics that we would like to discover, in order as theta sub one through theta sub k. Also we want to generate the coverage of topics in each document of d sub i And this is denoted by pi sub i j.
And pi sub ij is the probability of document d sub i covering topic theta sub j.
So obviously for each document, we have a set of such values to indicate to what extent the document covers, each topic.
And we can assume that these probabilities sum to one.
Because a document won't be able to cover other topics outside of the topics that we discussed, that we discovered.
So now, the question is, how do we define theta sub i, how do we define the topic?
Now this problem has not been completely defined until we define what is exactly theta.
So in the next few lectures, we're going to talk about different ways to define theta.
So, I showed you how we rewrite the into a form that looks like a, the formula on this slide.
After we make the assumption about smoothing the language model based on the collection of the language model.
Now, if we look at the, this rewriting, it actually would give us two benefits.
The first benefit is, it helps us better understand that this ranking function.
In particular, we're going to show that from this formula we can see smoothing is the correction that we model will give us something like a TF-IDF weighting and length normalization.
The second benefit is that it also allows us to compute the query likelihood more efficiently.
In particular, we see that the main part of the formula is a sum over the matching query terms.
So, this is much better than if we take the sum over all the words.
After we smooth the document the language model, we essentially have nonzero probabilities for all the words.
So, this new form of the formula is much easier to score, or to compute.
It's also interesting to note that the last of term here is actually independent of the document.
Since our goal is to rank the documents for the same query, we can ignore this term for ranking.
Because it's going to be the same for all the documents.
Ignoring it wouldn't effect the order of the documents.
Inside the sum, we also see that each matched query term would contribute a weight.
And this weight actually, is very interesting because it looks like TF-IDF weighting.
First, we can already see it has a frequency of the word in the query, just like in the vector space model.
When we take adult product, we see the word frequency in the query to show up in such a sum.
And so naturally, this part will correspond to the vector element from the document vector.
And here, indeed, we can see it actually encodes a weight that has similar factor to TF-IDF weighting.
I let you examine it.
Can you see it?
Can you see which part is capturing TF, and which part is capturing IDF weighting?
So if you want, you can pause the video to think more about it.
So, have you noticed that this p sub-seen is related to the term frequency in the sense that if a word occurs very frequently in the document, then the S probability here will tend to be larger.
Right?
So, this means this term is really doing something like TF weighting.
Have you also noticed that this time in the denominator is actually achieving the factor of IDF?
Why?
Because this is the popularity of the term in the collection, but it's in the denominator.
So, if the probability in the collection is larger than the weight is actually smaller.
And, this means a popular term.
We actually have a smaller weight.
And, this is precisely what IDF weighting is doing.
Only not, we now have a different form of TF and IDF.
Remember, IDF has a log, logarithm of document frequency, but here we have something different.
But intuitively, it achieves a similar effect.
Interestingly, we also have something related to the length normalization.
Again, can you see which factor is related to the length in this formula.
Well, I just say that, that this term is related to IDF weighting.
This, this collection probability.
But, it turns out this term here is actually related to a document length normalization.
In particular, D might be related to document N, length.
So, it, it encodes how much probability mass we want to give to unseen words.
How much smoothing you are allowed to do.
Intuitively, if a document is long, then we need to do less smoothing.
Because we can assume that it is large enough that, we have probably observed all of the words that the author could have written.
But if the document is short, the unseen are expected to be to be large, and we need to do more smoothing.
It's like that there are words that have not been retained yet by the author.
So, this term appears to paralyze long documents tend to be longer than, larger than for long document.
But note that the also occurs here.
And so, this may not actually be necessary, penalizing long documents, and in fact is not so clear here.
But as we will see later, when we consider some specific smoothing methods, it turns out that they do penalize long documents.
Just like in TF-IDF weighting and the document ends formulas in the vector space model.
So, that's a very interesting observation because it means we don't even have to think about the specific way of doing smoothing.
We just need to assume that if we smooth with this language model, then we would have a formula that looks like a TF-IDF weighting and document length normalization.
What's also interesting that we have a very fixed form of the ranking function.
And see, we have not heuristically put a logarithm here.
In fact, if you can think about, why we would have a logarithm here?
If you look at the assumptions that we have made, it will be clear.
It's because we have used a logarithm of query likelihood for scoring.
And, we turned the product into a sum of logarithm of probability.
And, that's why we have this logarithm.
Note that if we only want to heuristically implement a TF weighting and IDF weighting, we don't necessarily have to have a logarithm here.
Imagine if we drop this logarithm, we would still have TF and IDF weighting.
But, what's nice with probabilistic modeling is that we are automatically given a logarithm function here.
And, that's basically, a fixed reform of the formula that we did not really have to hueristically line.
And in this case, if you try to drop this logarithm the model probably won't, won't work as well as if you keep the logarithm.
So, a nice property of probabilistic modeling is that by following some assumptions and the probability rules, we'll get a formula automatically.
And, the formula would have a particular form, like in this case.
And, if we hueristically design the formula, we may not necessarily end up having such a specific form.
So to summarize, we talked about the need for smoothing a document and model.
Otherwise, it would give zero probability for unseen words in the document.
And, that's not good for scoring a query with such an unseen word.
It's also necessary, in general, to improve the acc, accuracy of estimating the model representing the topic of this document.
The general idea of smoothing in retrieval is to use the connection language model to give us some clue about which unseen word would have a higher probability.
That is the probability of the unseen word is assumed to be proportional to its probability in the collection.
With this assumption, we've shown that we can derive a general ranking formula for query likelihood.
That has a fact of TF-IDF waiting and document length normalization.
We also see that through some rewriting, the scoring of such ranking function, is primarily based on sum of weights on matched query terms, just like in the vector space model.
But, the actual ranking function is given us automatically by the probability rules and assumptions we have made.
Unlike in the vector space model, where we have to heuristically think about the form of the function.
However, we still need to address the question, how exactly we should we should smooth a document image model?
How exactly we should use the reference language model based on the connection to adjusting the probability of the maximum.
And, this is the topic of the next to that.
This lecture is about Probabilistic Topic Models for topic mining and analysis.
In this lecture, we're going to continue talking about the topic mining and analysis.
We're going to introduce probabilistic topic models.
So this is a slide that you have seen earlier, where we discussed the problems with using a term as a topic.
So, to solve these problems intuitively we need to use more words to describe the topic.
And this will address the problem of lack of expressive power.
When we have more words that we can use to describe the topic, that we can describe complicated topics.
To address the second problem we need to introduce weights on words.
This is what allows you to distinguish subtle differences in topics, and to introduce semantically related words in a fuzzy manner.
Finally, to solve the problem of word ambiguity, we need to split ambiguous word, so that we can disambiguate its topic.
It turns out that all these can be done by using a probabilistic topic model.
And that's why we're going to spend a lot of lectures to talk about this topic.
So the basic idea here is that, improve the replantation of topic as one distribution.
So what you see now is the older replantation.
Where we replanted each topic, it was just one word, or one term, or one phrase.
But now we're going to use a word distribution to describe the topic.
So here you see that for sports.
We're going to use the word distribution over theoretical speaking all the words in our vocabulary.
So for example, the high probability words here are sports, game, basketball, football, play, star, etc.
These are sports related terms.
And of course it would also give a non-zero probability to some other word like Trouble which might be related to sports in general, not so much related to topic.
In general we can imagine a non zero probability for all the words.
And some words that are not read and would have very, very small probabilities.
And these probabilities will sum to one.
So that it forms a distribution of all the words.
Now intuitively, this distribution represents a topic in that if we assemble words from the distribution, we tended to see words that are ready to dispose.
You can also see, as a very special case, if the probability of the mass is concentrated in entirely on just one word, it's sports.
And this basically degenerates to the symbol foundation of a topic was just one word.
But as a distribution, this topic of representation can, in general, involve many words to describe a topic and can model several differences in semantics of a topic.
Similarly we can model Travel and Science with their respective distributions.
In the distribution for Travel we see top words like attraction, trip, flight etc.
Whereas in Science we see scientist, spaceship, telescope, or genomics, and, you know, science related terms.
Now that doesn't mean sports related terms will necessarily have zero probabilities for science.
In general we can imagine all of these words we have now zero probabilities.
It's just that for a particular topic in some words we have very, very small probabilities.
Now you can also see there are some words that are shared by these topics.
When I say shared it just means even with some probability threshold, you can still see one word occurring much more topics.
In this case I mark them in black.
So you can see travel, for example, occurred in all the three topics here, but with different probabilities.
It has the highest probability for the Travel topic, 0.05.
But with much smaller probabilities for Sports and Science, which makes sense.
And similarly, you can see a Star also occurred in Sports and Science with reasonably high probabilities.
Because they might be actually related to the two topics.
So with this replantation it addresses the three problems that I mentioned earlier.
First, it now uses multiple words to describe a topic.
So it allows us to describe a fairly complicated topics.
Second, it assigns weights to terms.
So now we can model several differences of semantics.
And you can bring in related words together to model a topic.
Third, because we have probabilities for the same word in different topics, we can disintegrate the sense of word.
In the text to decode it's underlying topic, to address all these three problems with this new way of representing a topic.
So now of course our problem definition has been refined just slightly.
The slight is very similar to what you've seen before except we have added refinement for what our topic is.
Now each topic is word distribution, and for each word distribution we know that all the probabilities should sum to one with all the words in the vocabulary.
So you see a constraint here.
And we still have another constraint on the topic coverage, namely pis.
So all the Pi sub ij's must sum to one for the same document.
So how do we solve this problem?
Well, let's look at this problem as a computation problem.
So we clearly specify it's input and output and illustrate it here on this side.
Input of course is our text data.
C is our collection but we also generally assume we know the number of topics, k. Or we hypothesize a number and then try to bind k topics, even though we don't know the exact topics that exist in the collection.
And V is the vocabulary that has a set of words that determines what units would be treated as the basic units for analysis.
In most cases we'll use words as the basis for analysis.
And that means each word is a unique.
Now the output would consist of as first a set of topics represented by theta I's.
Each theta I is a word distribution.
And we also want to know the coverage of topics in each document.
So that's.
That the same pi ijs that we have seen before.
So given a set of text data we would like compute all these distributions and all these coverages as you have seen on this slide.
Now of course there may be many different ways of solving this problem.
In theory, you can write the [INAUDIBLE] program to solve this problem, but here we're going to introduce a general way of solving this problem called a generative model.
And this is, in fact, a very general idea and it's a principle way of using statistical modeling to solve text mining problems.
And here I dimmed the picture that you have seen before in order to show the generation process.
So the idea of this approach is actually to first design a model for our data.
So we design a probabilistic model to model how the data are generated.
Of course, this is based on our assumption.
The actual data aren't necessarily generating this way.
So that gave us a probability distribution of the data that you are seeing on this slide.
Given a particular model and parameters that are denoted by lambda.
So this template of actually consists of all the parameters that we're interested in.
And these parameters in general will control the behavior of the probability risk model.
Meaning that if you set these parameters with different values and it will give some data points higher probabilities than others.
Now in this case of course, for our text mining problem or more precisely topic mining problem we have the following plans.
First of all we have theta i's which is a word distribution snd then we have a set of pis for each document.
And since we have n documents, so we have n sets of pis, and each set the pi up.
The pi values will sum to one.
So this is to say that we first would pretend we already have these word distributions and the coverage numbers.
And then we can see how we can generate data by using such distributions.
So how do we model the data in this way?
And we assume that the data are actual symbols drawn from such a model that depends on these parameters.
Now one interesting question here is to think about how many parameters are there in total?
Now obviously we can already see n multiplied by K parameters.
For pi's.
We also see k theta i's.
But each theta i is actually a set of probability values, right?
It's a distribution of words.
So I leave this as an exercise for you to figure out exactly how many parameters there are here.
Now once we set up the model then we can fit the model to our data.
Meaning that we can estimate the parameters or infer the parameters based on the data.
In other words we would like to adjust these parameter values.
Until we give our data set the maximum probability.
I just said, depending on the parameter values, some data points will have higher probabilities than others.
What we're interested in, here, is what parameter values will give our data set the highest probability?
So I also illustrate the problem with a picture that you see here.
On the X axis I just illustrate lambda, the parameters, as a one dimensional variable.
It's oversimplification, obviously, but it suffices to show the idea.
And the Y axis shows the probability of the data, observe.
This probability obviously depends on this setting of lambda.
So that's why it varies as you change the value of lambda.
What we're interested here is to find the lambda star.
That would maximize the probability of the observed data.
So this would be, then, our estimate of the parameters.
And these parameters, note that are precisely what we hoped to discover from text data.
So we'd treat these parameters as actually the outcome or the output of the data mining algorithm.
So this is the general idea of using a generative model for text mining.
First, we design a model with some parameter values to fit the data as well as we can.
After we have fit the data, we will recover some parameter value.
We will use the specific parameter value And those would be the output of the algorithm.
And we'll treat those as actually the discovered knowledge from text data.
By varying the model of course we can discover different knowledge.
So to summarize, we introduced a new way of representing topic, namely representing as word distribution and this has the advantage of using multiple words to describe a complicated topic.It also allow us to assign weights on words so we have more than several variations of semantics.
We talked about the task of topic mining, and answers.
When we define a topic as distribution.
So the importer is a clashing of text articles and a number of topics and a vocabulary set and the output is a set of topics.
Each is a word distribution and also the coverage of all the topics in each document.
And these are formally represented by theta i's and pi i's.
And we have two constraints here for these parameters.
The first is the constraints on the worded distributions.
In each worded distribution the probability of all the words must sum to 1, all the words in the vocabulary.
The second constraint is on the topic coverage in each document.
A document is not allowed to recover a topic outside of the set of topics that we are discovering.
So, the coverage of each of these k topics would sum to one for a document.
We also introduce a general idea of using a generative model for text mining.
And the idea here is, first we're design a model to model the generation of data.
We simply assume that they are generative in this way.
And inside the model we embed some parameters that we're interested in denoted by lambda.
And then we can infer the most likely parameter values lambda star, given a particular data set.
And we can then take the lambda star as knowledge discovered from the text for our problem.
And we can adjust the design of the model and the parameters to discover various kinds of knowledge from text.
As you will see later in the other lectures.
This lecture is about that Latent Dirichlet Allocation or LDA.
In this lecture, we are going to continue talking about topic models.
In particular, we are going to talk about some extension of PLSA, and one of them is LDA or Latent Dirichlet Allocation.
So the plan for this lecture is to cover two things.
One is to extend the PLSA with prior knowledge and that would allow us to have in some sense a user-controlled PLSA, so it doesn't apply to they just listen to data, but also would listen to our needs.
The second is to extend the PLSA as a generative model, a fully generative model.
This has led to the development of Latent Dirichlet Allocation or LDA.
So first, let's talk about the PLSA with prior knowledge.
Now in practice, when we apply PLSA to analyze text data, we might have additional knowledge that we want to inject to guide the analysis.
The standard PLSA is going to blindly listen to the data by using maximum [inaudible].
We are going to just fit data as much as we can and get some insight about data.
This is also very useful, but sometimes a user might have some expectations about which topics to analyze.
For example, we might expect to see retrieval models as a topic in information retrieval or we also may be interesting in certain aspects, such as battery and memory, when looking at opinions about a laptop because the user is particularly interested in these aspects.
A user may also have knowledge about topic coverage and we may know which topic is definitely not covering which document or is covering the document.
For example, we might have seen those tags, topic tags assigned to documents.
And those tags could be treated as topics.
If we do that then a document account will be generated using topics corresponding to the tags already assigned to the document.
If the document is not assigned a tag, we're going to say there is no way for using that topic to generate document.
The document must be generated by using the topics corresponding to that assigned tags.
So question is how can we incorporate such knowledge into PLSA.
It turns out that there is a very elegant way of doing that and that would incorporate such knowledge as priors on the models.
And you may recall in Bayesian inference, we use prior together with data to estimate parameters and this is precisely what would happen.
So in this case, we can use maximum a posteriori estimate also called MAP estimate and the formula is given here.
Basically, this is to maximize the posteriori distribution probability.
And this is a combination of the likelihood of data and the prior.
So what would happen is that we are going to have an estimate that listens to the data and also listens to our prior preferences.
We can use this prior which is denoted as p of lambda to encode all kinds of preferences and the constraints.
So for example, we can use this to encode the need of having precise background of the topic.
Now this could be encoded as a prior because we can say the prior for the parameters is only a non-zero if the parameters contain one topic that is equivalent to the background language model.
In other words, in other cases if it is not like that, we are going to say the prior says it is impossible.
So the probability of that kind of models I think would be zero according to our prior.
So now we can also for example use the prior to force particular choice of topic to have a probability of a certain number.
For example, we can force document D to choose topic one with probability of one half or we can prevent topic from being used in generating document.
So we can say the third topic should not be used in generating document D, we will set to the Pi zero for that topic.
We can also use the prior to favor a set of parameters with topics that assign high probability to some particular words.
In this case, we are not going to say it is impossible but we can just strongly favor certain kind of distributions and you will see example later.
The MAP can be computed using a similar EM algorithm as we have used for the maximum likelihood estimate.
With just some modifications, most of the parameters would reflect the prior preferences and in such an estimate if we use a special form of the prior code or conjugate the prior, then the functional form of the prior will be similar to the data.
As a result, we can combine the two and the consequence is that you can basically convert the inference of the prior into the inference of having additional pseudo data because the two functional forms are the same and they can be combined.
So the effect is as if we had more data and this is convenient for computation.
It does not mean conjugate prior is the best way to define prior.
So now let us look at the specific example.
Suppose the user is particularly interested in battery life of a laptop and we are analyzing reviews.
So the prior says that the distribution should contain one distribution that would assign high probability to battery and life.
So we could say well there is distribution that is kind of concentrated on battery life and prior says that one of distributions should be very similar to this.
Now if we use MAP estimate with conjugate prior, which is the original prior, the original distribution based on this preference, then the only difference in the EM is that when we re-estimate words distributions, we are going to add additional counts to reflect our prior.
So here you can see the pseudo counts are defined based on the probability of words in a prior.
So battery obviously would have high pseudo counts and similarly life would have also high pseudo counts.
All the other words would have zero pseudo counts because their probability is zero in the prior and we see this is also controlled by a parameter mu and we are going to add a mu much by the probability of W given prior distribution to the connected accounts when we re-estimate this word distribution.
So this is the only step that is changed and the change is happening here.
And before we just connect the counts of words that we believe have been generated from this topic but now we force this distribution to give more probabilities to these words by adding them to the pseudo counts.
So in fact we artificially inflated their probabilities.
To make this distribution, we also need to add this many pseudo counts to the denominator.
This is total sum of all the pseudo counts we have added for all the words This would make this a gamma distribution.
Now this is intuitively very reasonable way of modifying EM and theoretically speaking, this works and it computes the MAP estimate.
It is useful to think about the two specific extreme cases of mu.
Now, [inaudible] the picture.
Think about what would happen if we set mu to zero.
Well that essentially to remove this prior.
So mu in some sense indicates our strengths on prior.
Now what would happen if we set mu to positive infinity.
Well that is to say that prior is so strong that we are not going to listen to the data at all.
So in the end, you see in this case we are going to make one of the distributions fixed to the prior.
You see why?
When mu is infinitive, we basically let this one dominate.
In fact we are going to set this one to precise this distribution.
So in this case, it is this distribution.
And that is why we said the background language model is in fact a way to impose the prior because it would force one distribution to be exactly the same as what we give, that is background distribution.
So in this case, we can even force the distribution to entirely focus on battery life.
But of course this would not work well because it cannot attract other words.
It would affect the accuracy of counting topics about battery life.
So in practice, mu is set somewhere in between of course.
So this is one way to impose a prior.
We can also impose some other constraints.
For example, we can set any parameters that will constantly include zero as needed.
For example, we may want to set one of the Pi's to zero and this would mean we do not allow that topic to participate in generating that document.
And this is only reasonable of course when we have prior analogy that strongly suggests this.
In general, we can use the empirical count of events in the observed data to estimate the probabilities.
And a commonly used technique is called a maximum likelihood estimate, where we simply normalize the observe accounts.
So if we do that, we can see, we can compute these probabilities as follows.
For estimating the probability that we see a water current in a segment, we simply normalize the count of segments that contain this word.
So let's first take a look at the data here.
On the right side, you see a list of some, hypothesizes the data.
These are segments.
And in some segments you see both words occur, they are indicated as ones for both columns.
In some other cases only one will occur, so only that column has one and the other column has zero.
And in all, of course, in some other cases none of the words occur, so they are both zeros.
And for estimating these probabilities, we simply need to collect the three counts.
So the three counts are first, the count of W1.
And that's the total number of segments that contain word W1.
It's just as the ones in the column of W1.
We can count how many ones we have seen there.
The segment count is for word 2, and we just count the ones in the second column.
And these will give us the total number of segments that contain W2.
The third count is when both words occur.
So this time, we're going to count the sentence where both columns have ones.
And then, so this would give us the total number of segments where we have seen both W1 and W2.
Once we have these counts, we can just normalize these counts by N, which is the total number of segments, and this will give us the probabilities that we need to compute original information.
Now, there is a small problem, when we have zero counts sometimes.
And in this case, we don't want a zero probability because our data may be a small sample and in general, we would believe that it's potentially possible for a [INAUDIBLE] to avoid any context.
So, to address this problem, we can use a technique called smoothing.
And that's basically to add some small constant to these counts, and so that we don't get the zero probability in any case.
Now, the best way to understand smoothing is imagine that we actually observed more data than we actually have, because we'll pretend we observed some pseudo-segments.
I illustrated on the top, on the right side on the slide.
And these pseudo-segments would contribute additional counts of these words so that no event will have zero probability.
Now, in particular we introduce the four pseudo-segments.
Each is weighted at one quarter.
And these represent the four different combinations of occurrences of this word.
So now each event, each combination will have at least one count or at least a non-zero count from this pseudo-segment.
So, in the actual segments that we'll observe, it's okay if we haven't observed all of the combinations.
So more specifically, you can see the 0.5 here after it comes from the two ones in the two pseudo-segments, because each is weighted at one quarter.
We add them up, we get 0.5.
And similar to this, 0.05 comes from one single pseudo-segment that indicates the two words occur together.
And of course in the denominator we add the total number of pseudo-segments that we add, in this case, we added a four pseudo-segments.
Each is weighed at one quarter so the total of the sum is, after the one.
So, that's why in the denominator you'll see a one there.
So, this basically concludes the discussion of how to compute a these four syntagmatic relation discoveries.
Now, so to summarize, syntagmatic relation can generally be discovered by measuring correlations between occurrences of two words.
We've introduced the three concepts from information theory.
Entropy, which measures the uncertainty of a random variable X.
Conditional entropy, which measures the entropy of X given we know Y.
And mutual information of X and Y, which matches the entropy reduction of X due to knowing Y, or entropy reduction of Y due to knowing X.
They are the same.
So these three concepts are actually very useful for other applications as well.
That's why we spent some time to explain this in detail.
But in particular, they are also very useful for discovering syntagmatic relations.
In particular, mutual information is a principal way for discovering such a relation.
It allows us to have values computed on different pairs of words that are comparable and so we can rank these pairs and discover the strongest syntagmatic from a collection of documents.
Now, note that there is some relation between syntagmatic relation discovery and [INAUDIBLE] relation discovery.
So we already discussed the possibility of using BM25 to achieve waiting for terms in the context to potentially also suggest the candidates that have syntagmatic relations with the candidate word.
But here, once we use mutual information to discover syntagmatic relations, we can also represent the context with this mutual information as weights.
So this would give us another way to represent the context of a word, like a cat.
And if we do the same for all the words, then we can cluster these words or compare the similarity between these words based on their context similarity.
So this provides yet another way to do term weighting for paradigmatic relation discovery.
And so to summarize this whole part about word association mining.
We introduce two basic associations, called a paradigmatic and a syntagmatic relations.
These are fairly general, they apply to any items in any language, so the units don't have to be words, they can be phrases or entities.
We introduced multiple statistical approaches for discovering them, mainly showing that pure statistical approaches are visible, are variable for discovering both kind of relations.
And they can be combined to perform joint analysis, as well.
These approaches can be applied to any text with no human effort, mostly because they are based on counting of words, yet they can actually discover interesting relations of words.
We can also use different ways with defining context and segment, and this would lead us to some interesting variations of applications.
For example, the context can be very narrow like a few words, around a word, or a sentence, or maybe paragraphs, as using differing contexts would allows to discover different flavors of paradigmatical relations.
And similarly, counting co-occurrences using let's say, visual information to discover syntagmatical relations.
We also have to define the segment, and the segment can be defined as a narrow text window or a longer text article.
And this would give us different kinds of associations.
These discovery associations can support many other applications, in both information retrieval and text and data mining.
So here are some recommended readings, if you want to know more about the topic.
The first is a book with a chapter on collocations, which is quite relevant to the topic of these lectures.
The second is an article about using various statistical measures to discover lexical atoms.
Those are phrases that are non-compositional.
For example, hot dog is not really a dog that's hot, blue chip is not a chip that's blue.
And the paper has a discussion about some techniques for discovering such phrases.
The third one is a new paper on a unified way to discover both paradigmatical relations and a syntagmatical relations, using random works on word graphs.
This lecture is about how to mine text data with social network as context.
In this lecture we're going to continue discussing contextual text mining.
In particular, we're going to look at the social network of others as context.
So first, what's our motivation for using network context for analysis of text?
The context of a text article can form a network.
For example the authors of research articles might form collaboration networks.
But authors of social media content might form social networks.
For example, in Twitter people might follow each other.
Or in Facebook as people might claim friends of others, etc.
So such context connects the content of the others.
Similarly, locations associated with text can also be connected to form geographical network.
But in general you can can imagine the metadata of the text data can form some kind of network if they have some relations.
Now there is some benefit in jointly analyzing text and its social network context or network context in general.
And that's because we can use network to impose some constraints on topics of text.
So for example it's reasonable to assume that authors connected in collaboration networks tend to write about the similar topics.
So such heuristics can be used to guide us in analyzing topics.
Text also can help characterize the content associated with each subnetwork.
And this is to say that both kinds of data, the network and text, can help each other.
So for example the difference in opinions expressed that are in two subnetworks can be reviewed by doing this type of joint analysis.
So here briefly you could use a model called a network supervised topic model.
In this slide we're going to give some general ideas.
And then in the next slide we're going to give some more details.
But in general in this part of the course we don't have enough time to cover these frontier topics in detail.
But we provide references that would allow you to read more about the topic to know the details.
But it should still be useful to know the general ideas.
And to know what they can do to know when you might be able to use them.
So the general idea of network supervised topic model is the following.
Let's start with viewing the regular topic models.
Like if you had an LDA as sorting optimization problem.
Of course, in this case, the optimization objective function is a likelihood function.
So we often use maximum likelihood estimator to obtain the parameters.
And these parameters will give us useful information that we want to obtain from text data.
For example, topics.
So we want to maximize the probability of tests that are given the parameters generally denoted by number.
The main idea of incorporating network is to think about the constraints that can be imposed based on the network.
In general, the idea is to use the network to impose some constraints on the model parameters, lambda here.
For example, the text at adjacent nodes of the network can be similar to cover similar topics.
Indeed, in many cases, they tend to cover similar topics.
So we may be able to smooth the topic distributions on the graph on the network so that adjacent nodes will have very similar topic distributions.
So they will share a common distribution on the topics.
Or have just a slight variations of the topic of distributions, of the coverage.
So, technically, what we can do is simply to add a network and use the regularizers to the likelihood of objective function as shown here.
So instead of just optimize the probability of test data given parameters lambda, we're going to optimize another function F. This function combines the likelihood with a regularizer function called R here.
And the regularizer defines the the parameters lambda and the Network.
It tells us basically what kind of parameters are preferred from a network constraint perspective.
So you can easily see this is in effect implementing the idea of imposing some prior on the model parameters.
Only that we're not necessary having a probabilistic model, but the idea is the same.
We're going to combine the two in one single objective function.
So, the advantage of this idea is that it's quite general.
Here the top model can be any generative model for text.
It doesn't have to be PLSA or LEA, or the current topic models.
And similarly, the network can be also in a network.
Any graph that connects these text objects.
This regularizer can also be any regularizer.
We can be flexible in capturing different heuristics that we want to capture.
And finally, the function F can also vary, so there can be many different ways to combine them.
So, this general idea is actually quite, quite powerful.
It offers a general approach to combining these different types of data in single optimization framework.
And this general idea can really be applied for any problem.
But here in this paper reference here, a particular instantiation called a NetPLSA was started.
In this case, it's just for instantiating of PLSA to incorporate this simple constraint imposed by network.
And the prior here is the neighbors on the network must have similar topic distribution.
They must cover similar topics in similar ways.
And that's basically what it says in English.
So technically we just have a modified objective function here.
Let's define both the texts you can actually see in the network graph G here.
And if you look at this formula, you can actually recognize some part fairly familiarly.
Because they are, they should be fairly familiar to you by now.
So can you recognize which part is the likelihood for the test given the topic model?
Well if you look at it, you will see this part is precisely the PLSA log-likelihood that we want to maximize when we estimate parameters for PLSA alone.
But the second equation shows some additional constraints on the parameters.
And in particular, we'll see here it's to measure the difference between the topic coverage at node u and node v. The two adjacent nodes on the network.
We want their distributions to be similar.
So here we are computing the square of their differences and we want to minimize this difference.
And note that there's a negative sign in front of this sum, this whole sum here.
So this makes it possible to find the parameters that are both to maximize the PLSA log-likelihood.
That means the parameters will fit the data well and, also to respect that this constraint from the network.
And this is the negative sign that I just mentioned.
Because this is an negative sign, when we maximize this object in function we'll actually minimize this statement term here.
So if we look further in this picture we'll see the results will weight of edge between u and v here.
And that space from out network.
If you have a weight that says well, these two nodes are strong collaborators of researchers.
These two are strong connections between two people in a social network.
And they would have weight.
Then that means it would be more important that they're topic coverages are similar.
And that's basically what it says here.
And finally you see a parameter lambda here.
This is a new parameter to control the influence of network constraint.
We can see easily, if lambda is set to 0, we just go back to the standard PLSA.
But when lambda is set to a larger value, then we will let the network influence the estimated models more.
So as you can see, the effect here is that we're going to do basically PLSA.
But we're going to also try to make the topic coverages on the two nodes that are strongly connected to be similar.
And we ensure their coverages are similar.
So here are some of the several results, from that paper.
This is slide shows the record results of using PLSA.
And the data here is DBLP data, bibliographic data, about research articles.
And the experiments have to do with using four communities of applications.
IR information retrieval.
DM stands for data mining.
ML for machinery and web.
There are four communities of articles, and we were hoping to see that the topic mining can help us uncover these four communities.
But from these assembled topics that you have seen here that are generated by PLSA.
And PLSA is unable to generate the four communities that correspond to our intuition.
The reason was because they are all mixed together and there are many words that are shared by these communities.
So it's not that easy to use four topics to separate them.
If we use more topics, perhaps we will have more coherent topics.
But what's interesting is that if we use the NetPLSA where the network, the collaboration network in this case of authors is used to impose constraints.
And in this case we also use four topics.
But Ned Pierre said we gave much more meaningful topics.
So here we'll see that these topics correspond well to the four communities.
The first is information retrieval.
The second is data mining.
Third is machine learning.
And the fourth is web.
So that separation was mostly because of the influence of network where with leverage is a collaboration network information.
Essentially the people that form a collaborating network would then be kind of assumed to write about similar topics.
And that's why we're going to have more coherent topics.
And if you just listen to text data alone based on the occurrences, you won't get such coherent topics.
Even though a topic model, like PLSA or LDA also should be able to pick up co-occurring words.
So in general the topics that they generate represent words that co-occur each other.
But still they cannot generate such a coherent results as NetPLSA, showing that the network contest is very useful here.
Now a similar model could have been also useful to to characterize the content associated with each subnetwork of collaborations.
So a more general view of text mining in context of network is you treat text as living in a rich information network environment.
And that means we can connect all the related data together as a big network.
And text data can be associated with a lot of structures in the network.
For example, text data can be associated with the nodes of the network, and that's basically what we just discussed in the NetPLSA.
But text data can be associated with age as well, or paths or even subnetworks.
And such a way to represent texts that are in the big environment of all the context information is very powerful.
Because it allows to analyze all the data, all the information together.
And so in general, analysis of text should be using the entire network information that's related to the text data.
So here's one suggested reading.
And this is the paper about NetPLSA where you can find more details about the model and how to make such a model.
Now lets look at another behaviour of the Mixed Model and in this case lets look at the response to data frequencies.
So what you are seeing now is basically the likelihood of function for the two word document and we now in this case the solution is text.
A probability of 0.9 and the a probability of 0.1.
Now it's interesting to think about a scenario where we start adding more words to the document.
So what would happen if we add many the's to the document?
Now this would change the game, right?
So, how?
Well, picture, what would the likelihood function look like now?
Well, it start with the likelihood function for the two words, right?
As we add more words, we know that.
But we have to just multiply the likelihood function by additional terms to account for the additional.
occurrences of that.
Since in this case, all the additional terms are the, we're going to just multiply by this term.
Right?
For the probability of the.
And if we have another occurrence of the, we'd multiply again by the same term, and so on and forth.
Add as many terms as the number of the's that we add to the document, d'.
Now this obviously changes the likelihood function.
So what's interesting is now to think about how would that change our solution?
So what's the optimal solution now?
Now, intuitively you'd know the original solution, pulling the 9 versus pulling the ,will no longer be optimal for this new function.
Right?
But, the question is how should we change it.
What general is to sum to one.
So he know we must take away some probability the mass from one word and add the probability mass to the other word.
The question is which word to have reduce the probability and which word to have a larger probability.
And in particular, let's think about the probability of the.
Should it be increased to be more than 0.1?
Or should we decrease it to less than 0.1?
What do you think?
Now you might want to pause the video a moment to think more about.
This question.
Because this has to do with understanding of important behavior of a mixture model.
And indeed, other maximum likelihood estimator.
Now if you look at the formula for a moment, then you will see it seems like another object Function is more influenced by the than text.
Before, each computer.
So now as you can imagine, it would make sense to actually assign a smaller probability for text and lock it.
To make room for a larger probability for the.
Why?
Because the is repeated many times.
If we increase it a little bit, it will have more positive impact.
Whereas a slight decrease of text will have relatively small impact because it occurred just one, right?
So this means there is another behavior that we observe here.
That is high frequency words generated with high probabilities from all the distributions.
And, this is no surprise at all, because after all, we are maximizing the likelihood of the data.
So the more a word occurs, then it makes more sense to give such a word a higher probability because the impact would be more on the likelihood function.
This is in fact a very general phenomenon of all the maximum likelihood estimator.
But in this case, we can see as we see more occurrences of a term, it also encourages the unknown distribution theta sub d to assign a somewhat higher probability to this word.
Now it's also interesting to think about the impact of probability of Theta sub B.
The probability of choosing one of the two component models.
Now we've been so far assuming that each model is equally likely.
And that gives us 0.5.
But you can again look at this likelihood function and try to picture what would happen if we increase the probability of choosing a background model.
Now you will see these terms for the, we have a different form where the probability that would be even larger because the background has a high probability for the word and the coefficient in front of 0.9 which is now 0.5 would be even larger.
When this is larger, the overall result would be larger.
And that also makes this the less important for theta sub d to increase the probability before the.
Because it's already very large.
So the impact here of increasing the probability of the is somewhat regulated by this coefficient, the point of i.
If it's larger on the background, then it becomes less important to increase the value.
So this means the behavior here, which is high frequency words tend to get the high probabilities, are effected or regularized somewhat by the probability of choosing each component.
The more likely a component is being chosen.
It's more important that to have higher values for these frequent words.
If you have a various small probability of being chosen, then the incentive is less.
So to summarize, we have just discussed the mixture model.
And we discussed that the estimation problem of the mixture model and particular with this discussed some general behavior of the estimator and that means we can expect our estimator to capture these infusions.
First every component model attempts to assign high probabilities to high frequent their words in the data.
And this is to collaboratively maximize likelihood.
Second, different component models tend to bet high probabilities on different words.
And this is to avoid a competition or waste of probability.
And this would allow them to collaborate more efficiently to maximize the likelihood.
So, the probability of choosing each component regulates the collaboration and the competition between component models.
It would allow some component models to respond more to the change, for example, of frequency of the theta point in the data.
We also talked about the special case of fixing one component to a background word distribution, right?
And this distribution can be estimated by using a collection of documents, a large collection of English documents, by using just one distribution and then we'll just have normalized frequencies of terms to give us the probabilities of all these words.
Now when we use such a specialized mixture model, we show that we can effectively get rid of that one word in the other component.
And that would make this cover topic more discriminative.
This is also an example of imposing a prior on the model parameter and the prior here basically means one model must be exactly the same as the background language model and if you recall what we talked about in Bayesian estimation, and this prior will allow us to favor a model that is consistent with our prior.
In fact, if it's not consistent we're going to say the model is impossible.
So it has a zero prior probability.
That effectively excludes such a scenario.
This is also issue that we'll talk more later.
This lecture is about learning to rank.
In this lecture, we're going to continue talking about web search.
In particular, we're going to talk about using machine running to combine definite features to improve ranking function.
So the question that we address in this lecture is how we can combine many features to generate a, a single ranking function to optimize search results.
In the previous lectures, we have talked about the, a number of ways to rank documents.
We have talked about some retrieval models, like a BM25 or clear light code.
They can generate a content based scores for matching documents with a query.
And we also talked about the link-based approaches, like page rank that can give additional scores to help us improve ranking.
Now the question now is how can we combine all these features and potentially many other features to do ranking?
And this will be very useful for ranking web pages not only just to improve accuracy, but also to improve the robustness of the ranking function.
So that's it not easy for a spammer to just perturb a one or a few features to promote a page.
So the general idea of learning to rank is to use machine learning to combine these features to optimize the weight on different features to generate the optimal ranking function.
So we would assume that the given a query document pair, Q and D, we can define a number of features.
And these features can vary from content based features such as a score of the document it was respected to the query according to a retrieval function, such as BM25 or Query Light or pivot commands from a machine or PL2, et cetera.
It can also be linked based score like PageRank score.
It can be also application of retrieval models to the anchor text of the page.
Right?
Those are the types of descriptions of links that pointed to this page.
So these can all be clues about whether this document is relevant or not.
We can even include a, a feature such as whether the URL has a [INAUDIBLE], because this might be the indicator of home page or entry page.
So, all of these features can then be combined together to generate the ranking functions.
The question is of course, how can we combine them?
In this approach, we simply hypothesize that the probability that this document is random to this query is a function of all these features.
So we can hypothesize this that the probability of relevance is related to these features through a particular form of the function that has some parameters.
These parameters can control the influence of different features on the final relevance.
This is of course, just a assumption.
Whether this assumption really makes sense is still a, a big question.
However, you have to empirically evaluate the, the, the function.
But by hypothesizing that the relevance is related to those features in the particular way, we can then combine these futures to generate the potentially more powerful ranking function, a more robust ranking function.
Naturally, the next question is how do we estimate loose parameters?
You know, how do we know which features should have high weight and which features should have low weight?
So this is a task of training or learning.
All right.
So, in this approach what we will do is use some training data.
Those are the data that have been judged by users.
So that we already know the relevance judgments.
We already know which documents should be rather high for which queries and this information can be based on real judgments by users or can, this can also be approximated by just using click through information.
Where we can assume the clicked documents are better than the skipped documents or clicked documents are relevant and the skipped documents are not relevant.
So, in general, the fit such hypothesize ranging function to the training day, meaning that we will try to optimize its retrieval accuracy on the training data.
And we adjust these parameters to see how we can optimize the performance of the function on the training data in terms of some measure such as map or NDCG.
So the training data would look like a table of tuples.
H-tuple it has three elements, the query, the document and the judgment.
So, it looks very much like our relevance judgment that we talked about in evaluation of retrieval systems.
.
>> This lecture is about some practical issues that you would have to address in evaluation of text retrieval systems.
In this lecture we will continue the discussion of evaluation.
We will cover some practical issues that you will have to solve in actual evaluation of text retrieval systems.
So, in order to create a test collection, we have to create a set of queries, a set of documents and a set of relevance judgments.
It turns out that each is actually challenging to create.
So first, the documents and queries must be representative.
They must rep, represent the real queries and real documents that the users handle.
And we also have to use many queries and many documents in order to avoid biased conclusions.
For the matching of relevant documents with the queries, we also need to ensure that there exists a lot of relevant documents for each query.
If a query has only one that is a relevant document in the collection then, you know, it's not very informative to compare different methods using such a query because there is not much room for us to see difference.
So, ideally there should be more relevant documents in the collection.
But yet the queries also should represent real queries that we care about.
In terms of relevance judgements, the challenge is to ensure complete judgements of all the documents for all the queries, yet, minimizing human and fault.
Because we have to use the human labor to label these documents.
It's very labor intensive.
And as a result, it's impossible to actually label all of the documents for all the queries, especially considering a joint, data set like the web.
So, this is actually a major challenge.
It's a very difficult challenge.
For measures, it's also challenging because what we want with measures is that with accuracy reflected the perceived utility of users.
We have to consider carefully what the users care about and then design measures to measure that.
If we, your measure is not measuring the right thing, then your conclusion would, would be misled.
So it's very important.
So we're going to talk about a couple issues here.
One is the statistical significance test, and this also is the reason why we have to use a lot of queries, and the question here is how sure can you be that I observed the difference?
It doesn't simply result from the particular queries you choose.
So here are some sample results of average precision for System A and System B in two different experiments.
And you can see in the bottom, we have mean average position, all right?
So the mean, if you look at the mean average position the mean average positions are exactly the same in both experiments.
All right, so you can see this is 0.2, this is 0.4 for system B and again here its also 0.2 and 0.4.
So they are identical.
Yet if you look at the, these exact average positions for different queries, if you look at these numbers in detail, you will realize that in one case you would feel that you can trust the conclusion here given by the average.
In another case, in the other case, you will feel that, well, I'm not sure.
So, why don't you take a look at all these numbers for a moment.
Pause the video.
So, if you at the average, the main average position, we can easily say that, well, System B is better, right?
So it's, after all, it's 0.4 and then this is twice as much as 0.2.
So that's a better performance.
But if you look at these two experiments and look at the detailed results, you will see that we'll be more confident to say that in the case one.
In experiment one.
In this case because these numbers seem to be consistently better than for system B.
Where as in, experiment two, we're not sure.
because, looking at some results, like this, after system A is better.
And this is another case where system A is better.
But yet, if we look at on the average, System B is better.
So what do you think?
You know, how reliable is our conclusion if we only look at the average?
Now in this case, intuitively, we feel it's better than one, it's more reliable.
But how can we quantitatively answer this question?
And this is why we need to do statistical significance test.
So the idea of a statistical significance test is basically to assess the vary, variance across these different queries.
If there's a, a big variance that means that the results could fluctuate a lot according to different queries.
Then we should believe that unless you have used a lot of queries the results might change if we use another set of queries.
Right?
So, this is then not so if you have seen high variance then it's not very reliable.
So let's look at these results again in the second case.
So here we show two, different ways to compare them.
One is a Sign Test.
And we'll, we'll just look at the sign.
If System B is better than System A, then we have a plus sign.
When System A is better we have a minus sign etc.
Using this case if you see this, well, there are seven cases.
We actually have four cases where System B is better.
But three cases System A is better.
You know intuitively, this is almost like random results.
Right, so if you just take a random sample of to, to flip seven coins, and if you use plus to denote the head and then minus to denote the tail, and that could easily be the results of just randomly flipping, these seven coins.
So, the fact that the, the average is larger doesn't tell us anything.
You know, we can't reliably concur that.
And this can be quantitative in the measure by, a p value.
And that basically, means, the probability that this result is in fact from random fluctuation.
In this case, probability is one.
It means it surely is a random fluctuation.
Now in Wilcoxon, test, it's a non parametrical test, and we would be not only looking at the signs we'll be also looking at the magnitude of the difference.
But, we, we, we can draw a similar conclusion where you say well it's very likely to be from random.
So to illustrate this let's think about such a distribution.
And this is called a normal distribution.
We assume that the mean is zero here.
Let's say, well, we started with the assumption that there's no difference between the two systems.
But we assume that because of random fluctuations depending on the queries we might observe a difference, so the actual difference might be on the left side here or on the right side here, right?
And, and this curve kind of shows the probability that we would actually observe values that are deviating from zero here.
Now, so if we, look at this picture then we see that if a difference is observed here, then the chance is very high that this is in fact, a random observation, right.
We can define region of you know, likely observation because of random fluctuation.
And this is 95% of all outcomes.
And in this interval then the observed values may still be from random fluctuation.
But if you observe a value in this region or a difference on this side, then the difference is unlikely from random fluctuation.
Right, so there is a very small probability that you will observe such a difference just because of random fluctuation.
So in that case, we can then conclude the difference must be real.
So System B is indeed better.
So, this is the idea of the statistical significance test.
The takeaway message here is that you have used many queries to avoid jumping into a conclusion as in this case to say System B is better.
There are many different ways of doing this statistical significance test.
So now, let's talk about the other problem of making judgements and as we said earlier, it's very hard to judge all the documents completely unless it is a small data set.
So the question is, if we can't afford judging all the documents in the collection, which subset should we judge?
And the solution here is pooling.
And this is a strategy that has been used in many cases to solve this problem.
So the idea of pulling is the following.
We would first choose a diverse set of ranking methods, these are types of retrieval systems.
And we hope these methods can help us nominate likely relevance in the documents.
So the goal is to pick out the relevant documents..
It means we are to make judgements on relevant documents because those are the most useful documents from the users perspective.
So, that way we would have each to return top-K documents.
And the K can vary from systems, right.
But the point is to ask them to suggest the most likely relevant documents.
And then we simply combine all these top-K sets to form a pool of documents for human assessors to judge.
So, imagine you have many systems.
Each will return K documents, you know, take the top-K documents, and we form the unit.
Now, of course there are many documents that are duplicated, because many systems might have retrieved the same random documents.
So there will be some duplicate documents.
And there are, there are also unique documents that are only returned by one system, so the idea of having diverse set of result ranking methods is to ensure the pool is broad.
And can include as many possible random documents as possible.
And then the users with the human assessors would make complete the judgements on this data set, this pool.
And the other unjudged documents are usually just a assumed to be non-relevant.
Now if the pool is large enough, this assumption is okay.
But the, if the pool is not very large, this actually has to be reconsidered, and we might use other strategies to deal with them and there are indeed other methods to handle such cases.
And such a strategy is generally okay for comparing systems that contribute to the pool.
That means if you participate in contributing to the pool then it's unlikely that it will penalize your system because the top ranked documents have all been judged.
However, this is problematic for even evaluating a new system that may not have contributed to the pool.
In this case, you know, a new system might be penalized because it might have nominated some relevant documents that have not been judged.
So those documents might be assumed to be non-relevant.
And that, that's unfair.
So to summarize the whole part of text retrieval evaluation, it's extremely important because the problem is an empirically defined problem.
If we don't rely on users, there's no way to tell whether one method works better.
If we have inappropriate experiment design, we might misguide our research or applications.
And we might just draw wrong conclusions.
And we have seen this in some of our discussion.
So, make sure to get it right for your research or application.
The main methodology is Cranfield evaluation methodology and this is near the main paradigm used in all kinds of empirical evaluation tasks, not just a search engine variation.
Map and nDCG are the two main measures that should definitely know about and they are appropriate for comparing ranking algorithms.
You will see them often in research papers.
Perceiving up to ten documents is easier to interpret from users perspective.
So, that's also often useful.
What's not covered is some other evaluation strategy like A-B test where the system would mix two of the results of two methods randomly.
And then will show the mix of results to users.
Of course, the users don't see which result is from which method.
The users would judge those results or click on those those documents in in a search engine application.
In this case, then, the search engine can keep track of the clicked documents, and see if one method has contributed more to the clicked documents.
If the user tends to click on one the results from one method, then it's just that method may, may be better.
So this is what leverages a real users of a search engine to do evaluation.
It's called A-B Test, and it's a strategy that's often used by the modern search engines, the commercial search engines.
Another way to evaluate IR or text retrieval is user studies, and we haven't covered that.
I've put some references here that you can look at if you want to know more about that.
So there are three additional readings here, these are three mini books about evaluation.
And they are all excellent in covering a broad review of information retrieval and evaluation.
And this covered some of the things that we discussed.
But they also have a lot of others to offer.
This lecture is about the Paradigmatics Relation Discovery.
In this lecture we are going to talk about how to discover a particular kind of word association called a paradigmatical relation.
By definition, two words are paradigmatically related if they share a similar context.
Namely, they occur in similar positions in text.
So naturally our idea of discovering such a relation is to look at the context of each word and then try to compute the similarity of those contexts.
So here is an example of context of a word, cat.
Here I have taken the word cat out of the context and you can see we are seeing some remaining words in the sentences that contain cat.
Now, we can do the same thing for another word like dog.
So in general we would like to capture such a context and then try to assess the similarity of the context of cat and the context of a word like dog.
So now the question is how can we formally represent the context and then define the similarity function.
So first, we note that the context actually contains a lot of words.
So, they can be regarded as a pseudo document, a imagine document, but there are also different ways of looking at the context.
For example, we can look at the word that occurs before the word cat.
We can call this context Left1 context.
All right, so in this case you will see words like my, his, or big, a, the, et cetera.
These are the words that can occur to left of the word cat.
So we say my cat, his cat, big cat, a cat, et cetera.
Similarly, we can also collect the words that occur right after the word cat.
We can call this context Right1, and here we see words like eats, ate, is, has, et cetera.
Or, more generally, we can look at all the words in the window of text around the word cat.
Here, let's say we can take a window of 8 words around the word cat.
We call this context Window8.
Now, of course, you can see all the words from left or from right, and so we'll have a bag of words in general to represent the context.
Now, such a word based representation would actually give us an interesting way to define the perspective of measuring the similarity.
Because if you look at just the similarity of Left1, then we'll see words that share just the words in the left context, and we kind of ignored the other words that are also in the general context.
So that gives us one perspective to measure the similarity, and similarly, if we only use the Right1 context, we will capture this narrative from another perspective.
Using both the Left1 and Right1 of course would allow us to capture the similarity with even more strict criteria.
So in general, context may contain adjacent words, like eats and my, that you see here, or non-adjacent words, like Saturday, Tuesday, or some other words in the context.
And this flexibility also allows us to match the similarity in somewhat different ways.
Sometimes this is useful, as we might want to capture similarity base on general content.
That would give us loosely related paradigmatical relations.
Whereas if you use only the words immediately to the left and to the right of the word, then you likely will capture words that are very much related by their syntactical categories and semantics.
So the general idea of discovering paradigmatical relations is to compute the similarity of context of two words.
So here, for example, we can measure the similarity of cat and dog based on the similarity of their context.
In general, we can combine all kinds of views of the context.
And so the similarity function is, in general, a combination of similarities on different context.
And of course, we can also assign weights to these different similarities to allow us to focus more on a particular kind of context.
And this would be naturally application specific, but again, here the main idea for discovering pardigmatically related words is to computer the similarity of their context.
So next let's see how we exactly compute these similarity functions.
Now to answer this question, it is useful to think of bag of words representation as vectors in a vector space model.
Now those of you who have been familiar with information retrieval or textual retrieval techniques would realize that vector space model has been used frequently for modeling documents and queries for search.
But here we also find it convenient to model the context of a word for paradigmatic relation discovery.
So the idea of this approach is to view each word in our vocabulary as defining one dimension in a high dimensional space.
So we have N words in total in the vocabulary, then we have N dimensions, as illustrated here.
And on the bottom, you can see a frequency vector representing a context, and here we see where eats occurred 5 times in this context, ate occurred 3 times, et cetera.
So this vector can then be placed in this vector space model.
So in general, we can represent a pseudo document or context of cat as one vector, d1, and another word, dog, might give us a different context, so d2.
And then we can measure the similarity of these two vectors.
So by viewing context in the vector space model, we convert the problem of paradigmatical relation discovery into the problem of computing the vectors and their similarity.
So the two questions that we have to address are first, how to compute each vector, and that is how to compute xi or yi.
And the other question is how do you compute the similarity.
Now in general, there are many approaches that can be used to solve the problem, and most of them are developed for information retrieval.
And they have been shown to work well for matching a query vector and a document vector.
But we can adapt many of the ideas to compute a similarity of context documents for our purpose here.
So let's first look at the one plausible approach, where we try to match the similarity of context based on the expected overlap of words, and we call this EOWC.
So the idea here is to represent a context by a word vector where each word has a weight that's equal to the probability that a randomly picked word from this document vector, is this word.
So in other words, xi is defined as the normalized account of word wi in the context, and this can be interpreted as the probability that you would actually pick this word from d1 if you randomly picked a word.
Now, of course these xi's would sum to one because they are normalized frequencies, and this means the vector is actually probability of the distribution over words.
So, the vector d2 can be also computed in the same way, and this would give us then two probability distributions representing two contexts.
So, that addresses the problem how to compute the vectors, and next let's see how we can define similarity in this approach.
Well, here, we simply define the similarity as a dot product of two vectors, and this is defined as a sum of the products of the corresponding elements of the two vectors.
Now, it's interesting to see that this similarity function actually has a nice interpretation, and that is this.
Dot product, in fact that gives us the probability that two randomly picked words from the two contexts are identical.
That means if we try to pick a word from one context and try to pick another word from another context, we can then ask the question, are they identical?
If the two contexts are very similar, then we should expect we frequently will see the two words picked from the two contexts are identical.
If they are very different, then the chance of seeing identical words being picked from the two contexts would be small.
So this intuitively makes sense, right, for measuring similarity of contexts.
Now you might want to also take a look at the exact formulas and see why this can be interpreted as the probability that two randomly picked words are identical.
So if you just stare at the formula to check what's inside this sum, then you will see basically in each case it gives us the probability that we will see an overlap on a particular word, wi.
And where xi gives us a probability that we will pick this particular word from d1, and yi gives us the probability of picking this word from d2.
And when we pick the same word from the two contexts, then we have an identical pick, right so.
That's one possible approach, EOWC, extracted overlap of words in context.
Now as always, we would like to assess whether this approach it would work well.
Now of course, ultimately we have to test the approach with real data and see if it gives us really semantically related words.
Really give us paradigmatical relations, but analytically we can also analyze this formula a little bit.
So first, as I said, it does make sense, right, because this formula will give a higher score if there is more overlap between the two contexts.
So that's exactly what we want.
But if you analyze the formula more carefully, then you also see there might be some potential problems, and specifically there are two potential problems.
First, it might favor matching one frequent term very well, over matching more distinct terms.
And that is because in the dot product, if one element has a high value and this element is shared by both contexts and it contributes a lot to the overall sum, it might indeed make the score higher than in another case, where the two vectors actually have a lot of overlap in different terms.
But each term has a relatively low frequency, so this may not be desirable.
Of course, this might be desirable in some other cases.
But in our case, we should intuitively prefer a case where we match more different terms in the context, so that we have more confidence in saying that the two words indeed occur in similar context.
If you only rely on one term and that's a little bit questionable, it may not be robust.
Now the second problem is that it treats every word equally, right.
So if you match a word like the and it will be the same as matching a word like eats, but intuitively we know matching the isn't really surprising because the occurs everywhere.
So matching the is not as such strong evidence as matching what a word like eats, which doesn't occur frequently.
So this is another problem of this approach.
In the next chapter we are going to talk about how to address these problems.
.
This lecture is about the future of web search.
In this lecture, we're going to talk about some possible future trends of web search and intelligent information retrieval systems in general.
In order to further improve the accuracy of a search engine, it's important that to consider special cases of information need.
So one particular trend could be to have more and more specialized than customized search engines, and they can be called vertical search engines.
These vertical search engines can be expected to be more effective than the current general search engines because they could assume that users are a special group of users that might have a common information need, and then the search engine can be customized with this ser, so, such users.
And because of the customization, it's also possible to do personalization.
So the search can be personalized, because we have a better understanding of the users.
Because of the restrictions with domain, we also have some advantages in handling the documents, because we can have better understanding of documents.
For example, particular words may not be ambiguous in such a domain.
So we can bypass the problem of ambiguity.
Another trend we can expect to see, is the search engine will be able to learn over time.
It's like a lifetime learning or lifelong learning, and this is, of course, very attractive because that means the search engine will self-improve itself.
As more people are using it, the search engine will become better and better, and this is already happening, because the search engines can learn from the [INAUDIBLE] of feedback.
More users use it, and the quality of the search engine allows for the popular queries that are typed in by many users allow it to become better, so this is sort of another feature that we will see.
The third trend might be to the integration of bottles of information access.
So search, navigation, and recommendation or filtering might be combined to form a full-fledged information management system.
And in the beginning of this course, we talked about push versus pull.
These are different modes of information access, but these modes can be combined.
And similarly, in the pull mode, querying and the browsing could also be combined.
And in fact we're doing that basically, today, is the [INAUDIBLE] search endings.
We are querying, sometimes browsing, clicking on links.
Sometimes we've got some information recommended.
Although most of the cases the information recommended is because of advertising.
But in the future, you can imagine seamlessly integrate the system with multi-mode for information access, and that would be convenient for people.
Another trend is that we might see systems that try to go beyond the searches to support the user tasks.
After all, the reason why people want to search is to solve a problem or to make a decision or perform a task.
For example consumers might search for opinions about products in order to purchase a product, choose a good product by, so in this case it would be beneficial to support the whole workflow of purchasing a product, or choosing a product.
In this era, after the common search engines already provide a good support.
For example, you can sometimes look at the reviews, and then if you want to buy it, you can just click on the button to go the shopping site and directly get it done.
But it does not provide a, a good task support for many other tasks.
For example, for researchers, you might want to find the realm in the literature or site of the literature.
And then, there's no, not much support for finishing a task such as writing a paper.
So, in general, I think, there are many opportunities in the wait.
So in the following few slides, I'll be talking a little bit more about some specific ideas or thoughts that hopefully, can help you in imagining new application possibilities.
Some of them might be already relevant to what you are currently working on.
In general, we can think about any intelligent system, especially intelligent information system, as we specified by these these three nodes.
And so if we connect these three into a triangle, then we'll able to specify an information system.
And I call this Data-User-Service Triangle.
So basically the three questions you ask would be who are you serving and what kind of data are you are managing and what kind of service you provide.
Right there, this would help us basically specify in your system.
And there are many different ways to connect them depending on how you connect them, you will have a different kind of systems.
So let me give you some examples.
On the top, you can see different kinds of users.
On the left side, you can see different types of data or information, and on the bottom, you can see different service functions.
Now imagine you can connect all these in different ways.
So, for example, you can connect everyone with web pages, and the support search and browsing, what do you get?
Well, that's web search, right?
What if we connect UIUC employees with organization documents or enterprise documents to support the search and browsing, but that's enterprise search.
If you connect the scientist with literature information to provide all kinds of service, including search, browsing, or alert of new random documents or mining analyzing research trends, or provide the task with support or decision support.
For example, we might be, might be able to provide a support for automatically generating related work section for a research paper, and this would be closer to task support.
Right?
So then we can imagine this would be a literature assistant.
If we connect the online shoppers with blog articles or product reviews then we can help these people to improve shopping experience.
So we can provide, for example data mining capabilities to analyze the reviews, to compare products, compare sentiment of products and to provide task support or decision support to have them choose what product to buy.
Or we can connect customer service people with emails from the customers, and, and we can imagine a system that can provide a analysis of these emails to find that the major complaints of the customers.
We can imagine a system we could provide task support by automatically generating a response to a customer email.
Maybe intelligently attach also a promotion message if appropriate, if they detect that that's a positive message, not a complaint, and then you might take this opportunity to attach some promotion information.
Whereas if it's a complaint, then you might be able to automatically generate some generic response first and tell the customer that he or she can expect a detailed response later, etc.
All of these are trying to help people to improve the productivity.
So this shows that the opportunities are really a lot.
It's just only restricted by our imagination.
So this picture shows the trend of the technology, and also, it characterizes the, intelligent information system in three angles.
You can see in the center, there's a triangle that connects keyword queries to search a bag of words representation.
That means the current search engines basically provides search support to users and mostly model users based on keyword queries and sees the data through bag of words representation.
So it's a very simple approximation of the actual information in the documents.
But that's what the current system does.
It connects these three nodes in such a simple way, or it only provides a basic search function and doesn't really understand the user, and it doesn't really understand that much information in the documents.
Now, I showed some trends to push each node toward a more advanced function.
So think about the user node here, right?
So we can go beyond the keyword queries, look at the user search history, and then further model the user completely to understand the, the user's task environment, task need context or other information.
Okay, so this is pushing for personalization and complete user model.
And this is a major direction in research in, in order to build intelligent information systems.
On the document side, we can also see, we can go beyond bag of words implementation to have entity relation representation.
This means we'll recognize people's names, their relations, locations, etc.
And this is already feasible with today's natural processing technique.
And Google is the reason the initiative on the knowledge graph.
If you haven't heard of it, it is a good step toward this direction.
And once we can get to that level without initiating robust manner at larger scale, it can enable the search engine to provide a much better service.
In the future we would like to have knowledge representation where we can add perhaps inference rules, and then the search engine would become more intelligent.
So this calls for large-scale semantic analysis, and perhaps this is more feasible for vertical search engines.
It's easier to make progress in the particular domain.
Now on the service side, we see we need to go beyond the search of support information access in general.
So search is only one way to get access to information as well recommender systems and push and pull so different ways to get access to random information.
But going beyond access, we also need to help people digest the information once the information is found, and this step has to do with analysis of information or data mining.
We have to find patterns or convert the text information into real knowledge that can be used in application or actionable knowledge that can be used for decision making.
And furthermore the knowledge will be used to help a user to improve productivity in finishing a task, for example, a decision-making task.
Right, so this is a trend.
And, and, and so basically, in this dimension, we anticipate in the future intelligent information systems will provide intelligent and interactive task support.
Now I should also emphasize interactive here, because it's important to optimize the combined intelligence of the users and the system.
So we, we can get some help from users in some natural way.
And we don't have to assume the system has to do everything when the human, user, and the machine can collaborate in an intelligent way, an efficient way, then the combined intelligence will be high and in general, we can minimize the user's overall effort in solving problem.
So this is the big picture of future intelligent information systems, and this hopefully can provide us with some insights about how to make further innovations on top of what we handled today.
This lecture is about the similarity-based approaches to text clustering.
In this lecture we're going to to continue the discussion of how to do a text clustering.
In particular, we're going to to cover different kinds of approaches than generative models, and that is similarity-based approaches.
So the general idea of similarity-based clustering is to explicitly specify a similarity function to measure the similarity between two text objects.
Now this is in contrast with a generative model where we implicitly define the clustering bias by using a particular object to function like a [INAUDIBLE] function.
The whole process is driven by optimizing the [INAUDIBLE,] but here we explicitly provide a view of what we think are similar.
And this is often very useful because then it allows us to inject any particular view of similarity into the clustering program.
So once we have a similarity function, we can then aim at optimally partitioning, to partitioning the data into clusters or into different groups.
And try to maximize the inter-group similarity and minimize the inter-group similarity.
That is to ensure the objects that are put into the same group to be similar, but the objects that are put into different groups to be not similar.
And these are the general goals of clustering, and there is often a trade off between achieving both goals.
Now there are many different methods for doing similarity based clustering, and in general I think we can distinguish the two strategies at high level.
One is to progressively construct the hierarchy of clusters, and so this often leads to hierarchical clustering.
And we can further distinguish it two ways, to construct a hierarchy depending on whether we started with the collection to divide the connection.
Or started with individual objectives and gradually group them together, so one is bottom-up that can be called agglomerative.
Well we gradually group a similar objects into larger and larger clusters.
Until we group everything together, the other is top-down or divisive, in this case we gradually partition the whole data set into smaller and smaller clusters.
The other general strategy is to start with the initial tentative clustering and then iteratively improve it.
And this often leads for a flat clustering, one example is k-Means, so as I just said, there are many different clustering methods available.
And a full coverage of all the clustering methods would be beyond the scope of this course.
But here we are going to talk about the two representative methods, in some detail one is Hierarchical Agglomerative Clustering or HAC, the other is k-Means.
So first of it we'll get the agglomerative hierarchical clustering, in this case, we're given a similarity function to measure similarity between two objects.
And then we can gradually group similar objects together in a bottom-up fashion to form larger and larger groups.
And they always form a hierarchy, and then we can stop when some stopping criterion is met.
It could be either some number of clusters has been achieved or the threshold for similarity has been reached.
There are different variations here, and they mainly differ in the ways to compute a group similarity.
Based on the individual objects similarity, so let's illustrate how again induced a structure based on just similarity.
So start with all the text objects and we can then measure the similarity between them.
Of course based on the provided similarity function, and then we can see which pair has the highest similarity.
And then just group them together, and then we're going to see which pair is the next one to group.
Maybe these two now have the highest similarity, and then we're going to gradually group them together.
And then every time we're going to pick the highest similarity, the similarity of pairs to group.
This will give us a binary tree eventually to group everything together.
Now, depending on our applications, we can use the whole hierarchy as a structure for browsing, for example.
Or we can choose a cutoff, let's say cut here to get four clusters, or we can use a threshold to cut.
Or we can cut at this high level to get just two clusters, so this is a general idea, now if you think about how to implement this algorithm.
You'll realize that we have everything specified except for how to compute group similarity.
We are only given the similarity function of two objects, but as we group groups together, we also need to assess the similarity between two groups.
There are also different ways to do that and there are the three popular methods.
Single-link, complete-link, and average-link, so given two groups and the single-link algorithm.
Is going to define the group similarity as the similarity of the closest pair of the two groups.
Complete-link defines the similarity of the two groups as the similarity of the farthest system pair.
Average-link defines the similarity as average of similarity of all the pairs of the two groups.
So it's much easier to understand the methods by illustrating them, so here are two groups, g1 and g2 with some objects in each group.
And we know how to compute the similarity between two objects, but the question now is, how can we compute the similarity between the two groups?
And then we can in general base this on the similarities of the objects in the two groups.
So, in terms of single-link and we're just looking at the closest pair so in this case, these two paired objects will defined the similarities of the two groups.
As long as they are very close, we're going to say the two groups are very close so it is an optimistic view of similarity.
The complete link on the other hand were in some sense pessimistic, and by taking the similarity of the two farthest pair as the similarity for the two groups.
So we are going to make sure that if the two groups are having a high similarity.
Then every pair of the two groups, or the objects in the two groups will have, will be ensured to have high similarity.
Now average link is in between, so it takes the average of all these pairs.
Now these different ways of computing group similarities will lead to different clustering algorithms.
And they would in general give different results, so it's useful to take a look at their differences and to make a comparison.
First, single-link can be expected to generally the loose clusters, the reason is because as long as two objects are very similar in the two groups, it will bring the two groups together.
If you think about this as similar to having parties with people, then it just means two groups of people would be partying together.
As long as in each group there is a person that is well connected with the other group.
So the two leaders of the two groups can have a good relationship with each other and then they will bring together the two groups.
In this case, the cluster is loose, because there's no guarantee that other members of the two groups are actually very close to each other.
Sometimes they may be very far away, now in this case it's also based on individual decisions, so it could be sensitive to outliers.
The complete-link is in the opposite situation, where we can expect the clusters to be tight.
And it's also based on individual decision so it can be sensitive to outliers.
Again to continue the analogy to having a party of people, then complete-link would mean when two groups come together.
They want to ensure that even the people that are unlikely to talk to each other would be comfortable.
Always talking to each other, so ensure the whole class to be coherent.
The average link of clusters in between and as group decision, so it's going to be insensitive to outliers, now in practice which one is the best.
Well, this would depend on the application and sometimes you need a lose clusters.
And aggressively cluster objects together that maybe single-link is good.
But other times you might need a tight clusters and a complete-link might be better.
But in general, you have to empirically evaluate these methods for your application to know which one is better.
Now, next let's look at another example of a method for similarity-based clustering.
In this case, which is called k-Means clustering, we will represent each text object as a term vector.
And then assume a similarity function defined on two objects, now we're going to start with some tentative clustering results by just selecting k randomly.
selected vectors as centroids of k clusters and treat them as centers as if they represent, they each represent a cluster.
So this gives us the initial tentative cluster, then we're going to iteratively improve it.
And the process goes like this, and once we have these centroids Decide.
We're going to assign a vector to the cluster whose centroid is closest to the current vector.
So basically we're going to measure the distance between this vector, and each of the centroids, and see which one is the closest to this one.
And then just put this object into that cluster, this is to have tentative assignment of objects into clusters.
And we're going to partition all the objects into k clusters based on our tentative clustering and centroids.
Then we can do re-compute the centroid based on the locate the object in each cluster.
And this is to adjust the centroid, and then we can repeat this process until the similarity-based objective function.
In this case, it's within cluster sum of squares converges, and theoretically we can show that.
This process actually is going to minimize the within cluster sum of squares where define object and function.
Given k clusters, so it can be also shown, this process will converge to a local minimum.
I think about this process for a moment, it might remind you the Algorithm for mixture model.
Indeed this algorithm is very similar to the Algorithm for the mixture model for clustering.
More specifically we also initialize these parameters in the Algorithm so the random initialization is similar.
And then in the Algorithm, you may recall that, we're going to repeat E-step and M-step to improve our parameter estimation.
In this case, we're going to improve the clustering result iteratively by also doing two steps.
And in fact that the two steps are very similar to Algorithm, in that when we locate the vector into one of the clusters based on our tentative clustering.
It's very similar to inferring the distribution that has been used to generate the document, the mixture model.
So it is essentially similar to E-step, so what's the difference, well the difference is here.
We don't make a probabilistic allocation as in the case of E-step, the brother will make a choice.
We're going to make a call if this, there upon this closest to cluster two, then we're going to say you are in cluster two.
So there's no choice, and we're not going to say, you assume the set is belonging to a cluster two.
And so we're not going to have a probability, but we're just going to put one object into precisely one cluster.
In the E-step however, we do a probability location, so we split in counts.
And we're not going to say exactly which distribution has been used to generate a data point.
Now next, we're going to adjust the centroid, and this is very similar to M-step where we re-estimate the parameters.
That's when we'll have a better estimate of the parameter, so here we'll have a better clustering result by adjusting the centroid.
And note that centroid is based on the average of the vectors in the cluster.
So this is also similar to the M-step where we do counts,pull together counts and then normalize them.
The difference of course is also because of the difference in the E-step, and we're not going to consider probabilities when we count the points.
In this case, k-Means we're going to all make count of the objects as allocated to this cluster.
And this is only a subset of data points, but in the Algorithm, we in principle consider all the data points based on probabilistic allocations.
But in nature they are very similar and that's why it's also maximizing well defined object of functions.
And it's guaranteed to convert local minimum, so to summarize our discussion of clustering methods.
We first discussed model based approaches, mainly the mixture model.
Here we use the implicit similarity function to define the clustering bias.
There is no explicit define similarity function, the model defines clustering bias and the clustering structure is built into a generative model.
That's why we can use potentially a different model to recover different structure.
Complex generative models can be used to discover complex clustering structures.
We do not talk about in full, but we can easily design, generate a model to generate a hierarchical clusters.
We can also use prior to further customize the clustering algorithm to for example control the topic of one cluster or multiple clusters.
However one disadvantage of this approach is that there is no easy way to directly control the similarity measure.
Sometimes we want to that, but it's very hard to inject such a special definition of similarity into such a model.
We also talked about similarity-based approaches, these approaches are more flexible to actually specify similarity functions.
But one major disadvantage is that their objective function is not always very clear.
The k-Means algorithm has clearly defined the objective function, but it's also very similar to a model based approach.
The hierarchical clustering algorithm on the other hand is harder to specify the objective function.
So it's not clear what exactly is being optimized, both approaches can generate term clusters.
And document clusters, and term clusters can be in general, generated by representing each term with some text content.
For example, take the context of each term as a representation of each term, as we have done in semantic relation learning.
And then we can certainly cluster terms, based on actual text [INAUDIBLE].
Of course, term clusters can be generated by using generative models as well, as we've seen.
.
So, as we explained the different text representation tends to enable different analysis.
In particular, we can gradually add more and more deeper analysis results to represent text data.
And that would open up a more interesting representation opportunities and also analysis capacities.
So, this table summarizes what we have just seen.
So the first column shows the text representation.
The second visualizes the generality of such a representation.
Meaning whether we can do this kind of representation accurately for all the text data or only some of them.
And the third column shows the enabled analysis techniques.
And the final column shows some examples of application that can be achieved through this level of representation.
So let's take a look at them.
So as a stream text can only be processed by stream processing algorithms.
It's very robust, it's general.
And there was still some interesting applications that can be down at this level.
For example, compression of text.
Doesn't necessarily need to know the word boundaries.
Although knowing word boundaries might actually also help.
Word base repetition is a very important level of representation.
It's quite general and relatively robust, indicating they were a lot of analysis techniques.
Such as word relation analysis, topic analysis and sentiment analysis.
And there are many applications that can be enabled by this kind of analysis.
For example, thesaurus discovery has to do with discovering related words.
And topic and opinion related applications are abounded.
And there are, for example, people might be interesting in knowing the major topics covered in the collection of texts.
And this can be the case in research literature.
And scientists want to know what are the most important research topics today.
Or customer service people might want to know all our major complaints from their customers by mining their e-mail messages.
And business intelligence people might be interested in understanding consumers' opinions about their products and the competitors' products to figure out what are the winning features of their products.
And, in general, there are many applications that can be enabled by the representation at this level.
Now, moving down, we'll see we can gradually add additional representations.
By adding syntactical structures, we can enable, of course, syntactical graph analysis.
We can use graph mining algorithms to analyze syntactic graphs.
And some applications are related to this kind of representation.
For example, stylistic analysis generally requires syntactical structure representation.
We can also generate the structure based features.
And those are features that might help us classify the text objects into different categories by looking at the structures sometimes in the classification.
It can be more accurate.
For example, if you want to classify articles into different categories corresponding to different authors.
You want to figure out which of the k authors has actually written this article, then you generally need to look at the syntactic structures.
When we add entities and relations, then we can enable other techniques such as knowledge graph and answers, or information network and answers in general.
And this analysis enable applications about entities.
For example, discovery of all the knowledge and opinions about real world entities.
You can also use this level representation to integrate everything about anything from scaled resources.
Finally, when we add logical predicates, that would enable large inference, of course.
And this can be very useful for integrating analysis of scattered knowledge.
For example, we can also add ontology on top of the, extracted the information from text, to make inferences.
A good of example of application in this enabled by this level of representation, is a knowledge assistant for biologists.
And this program that can help a biologist manage all the relevant knowledge from literature about a research problem such as understanding functions of genes.
And the computer can make inferences about some of the hypothesis that the biologist might be interesting.
For example, whether a gene has a certain function, and then the intelligent program can read the literature to extract the relevant facts, doing compiling and information extracting.
And then using a logic system to actually track that's the answers to researchers questioning about what genes are related to what functions.
So in order to support this level of application we need to go as far as logical representation.
Now, this course is covering techniques mainly based on word based representation.
And these techniques are general and robust and that's more widely used in various applications.
In fact, in virtually all the text mining applications you need this level of representation and then techniques that support analysis of text in this level.
But obviously all these other levels can be combined and should be combined in order to support the sophisticated applications.
So to summarize, here are the major takeaway points.
Text representation determines what kind of mining algorithms can be applied.
And there are multiple ways to represent the text, strings, words, syntactic structures, entity-relation graphs, knowledge predicates, etc.
And these different representations should in general be combined in real applications to the extent we can.
For example, even if we cannot do accurate representations of syntactic structures, we can state that partial structures strictly.
And if we can recognize some entities, that would be great.
So in general we want to do as much as we can.
And when different levels are combined together, we can enable a richer analysis, more powerful analysis.
This course however focuses on word-based representation.
Such techniques have also several advantage, first of they are general and robust, so they are applicable to any natural language.
That's a big advantage over other approaches that rely on more fragile natural language processing techniques.
Secondly, it does not require much manual effort, or sometimes, it does not require any manual effort.
So that's, again, an important benefit, because that means that you can apply it directly to any application.
Third, these techniques are actually surprisingly powerful and effective form in implications.
Although not all of course as I just explained.
Now they are very effective partly because the words are invented by humans as basically units for communications.
So they are actually quite sufficient for representing all kinds of semantics.
So that makes this kind of word-based representation all so powerful.
And finally, such a word-based representation and the techniques enable by such a representation can be combined with many other sophisticated approaches.
So they're not competing with each other.
This lecture is a continued discussion of evaluation of text categorization.
Earlier we have introduced measures that can be used with computer provision and recall.
For each category and each document now in this lecture we're going to further examine how to combine the performance of the different categories of different documents how to aggregate them, how do we take average?
You see on the title here I indicated it's called a macro average and this is in contrast to micro average that we'll talk more about later.
So, again, for each category we're going to compute the precision require an f1 so for example category c1 we have precision p1, recall r1 and F value f1.
And similarly we can do that for category Now once we compute that and we can aggregate them, so for example we can aggregate all the precision values.
For all the categories, for computing overall precision.
And this is often very useful to summarize what we have seen in the whole data set.
And aggregation can be done many different ways.
Again as I said, in a case when you need to aggregate different values, it's always good to think about what's the best way of doing the aggregation.
For example, we can consider arithmetic mean, which is very commonly used, or you can use geometric mean, which would have different behavior.
Depending on the way you aggregate, you might have got different conclusions.
in terms of which method works better, so it's important to consider these differences and choosing the right one or a more suitable one for your task.
So the difference fore example between arithmetically and geometrically is that the arithmetically would be dominated by high values whereas geometrically would be more affected by low values.
Base and so whether you are want to emphasis low values or high values would be a question relate with all you And similar we can do that for recal and F score.
So that's how we can generate the overall precision, recall and F score.
Now we can do the same for aggregation of other all the document All right.
So it's exactly the same situation for each document on our computer.
Precision, recall, and F. And then after we have completed the computation for all these documents, we're going to aggregate them to generate the overall precision, overall recall, and overall F score.
These are, again, examining the results from different angles.
Which one's more useful will depend on your application.
In general, it's beneficial to look at the results from all these perspectives.
And especially if you compare different methods in different dimensions, it might reveal which method Is better in which measure or in what situations and this provides insightful.
Understanding the strands of a method or a weakness and this provides further insight for improving them.
So as I mentioned, there is also micro-average in contrast to the macro average that we talked about earlier.
In this case, what we do is you pool together all the decisions, and then compute the precision and recall.
So we can compute the overall precision and recall by just counting how many cases are in true positive, how many cases in false positive, etc, it's computing the values in the contingency table, and then we can compute the precision and recall just once.
In contrast, in macro-averaging, we're going to do that for each category first.
And then aggregate over these categories or we do that for each document and then aggregate all the documents but here we pooled them together.
Now this would be very similar to the classification accuracy that we used earlier, and one problem here of course to treat all the instances, all the decisions equally.
And this may not be desirable.
But it may be a property for some applications, especially if we associate the, for example, the cost for each combination.
Then we can actually compute for example, weighted classification accuracy.
Where you associate the different cost or utility for each specific decision, so there could be variations of these methods that would be more useful.
But in general macro average tends to be more information than micro average, just because it might reflect the need for understanding performance on each category or performance on each document which are needed in applications.
But macro averaging and micro averaging, they are both very common, and you might see both reported in research papers on Categorization.
Also sometimes categorization results might actually be evaluated from ranking prospective.
And this is because categorization results are sometimes or often indeed passed it to a human for various purposes.
For example, it might be passed to humans for further editing.
For example, news articles can be tempted to be categorized by using a system and then human editors would then correct them.
And all the email messages might be throughout to the right person for handling in the help desk.
And in such a case the categorizations will help prioritizing the task for particular customer service person.
So, in this case the results have to be prioritized and if the system can't give a score to the categorization decision for confidence then we can use the scores to rank these decisions and then evaluate the results as a rank list, just as in a search engine.
Evaluation where you rank the documents in responsible query.
So for example a discovery of spam emails can be evaluated based on ranking emails for the spam category.
And this is useful if you want people to to verify whether this is really spam, right?
The person would then take the rank To check one by one and then verify whether this is indeed a spam.
So to reflect the utility for humans in such a task, it's better to evaluate Ranking Chris and this is basically similar to a search again.
And in such a case often the problem can be better formulated as a ranking problem instead of a categorization problem.
So for example, ranking documents in a search engine can also be framed as a binary categorization problem, distinguish the relevant documents that are useful to users from those that are not useful, but typically we frame this as a ranking problem, and we evaluate it as a rank list.
That's because people tend to examine the results so ranking evaluation more reflects utility from user's perspective.
So to summarize categorization evaluation, first evaluation is always very important for all these tasks.
So get it right.
If you don't get it right, you might get misleading results.
And you might be misled to believe one method is better than the other, which is in fact not true.
So it's very important to get it right.
Measures must also reflect the intended use of the results for a particular application.
For example, in spam filtering and news categorization the results are used in maybe different ways.
So then we would need to consider the difference and design measures appropriately.
We generally need to consider how will the results be further processed by the user and think from a user's perspective.
What quality is important?
What aspect of quality is important?
Sometimes there are trade offs between multiple aspects like precision and recall and so we need to know for this application is high recall more important, or high precision is more important.
Ideally we associate the different cost with each different decision arrow.
And this of course has to be designed in an application specific way.
Some commonly used measures for relative comparison methods are the following.
Classification accuracy, it's very commonly used for especially balance.
[INAUDIBLE] preceding [INAUDIBLE] Scores are common and report characterizing performances, given angles and give us some [INAUDIBLE] like a [INAUDIBLE] Per document basis [INAUDIBLE] And then take a average of all of them, different ways micro versus macro [INAUDIBLE].
In general, you want to look at the results from multiple perspectives and for particular applications some perspectives would be more important than others but diagnoses and analysis of categorization methods.
It's generally useful to look at as many perspectives as possible to see subtle differences between methods or tow see where a method might be weak from which you can obtain sight for improving a method.
Finally sometimes ranking may be more appropriate so be careful sometimes categorization has got may be better frame as a ranking tasks and there're machine running methods for optimizing ranking measures as well.
So here are two suggested readings.
One is some chapters of this book where you can find more discussion about evaluation measures.
The second is a paper about comparison of different approaches to text categorization and it also has an excellent discussion of how to evaluate textual categorization.
This lecture is about the Evaluation of Text Categorization.
So we've talked about many different methods for text categorization.
But how do you know which method works better?
And for a particular application, how do you know this is the best way of solving your problem?
To understand these, we have to how to we have to know how to evaluate categorization results.
So first some general thoughts about the evaluation.
In general, for evaluation of this kind of empirical tasks such as categorization, we use methodology that was developed in 1960s by information retrieval researchers.
Called a Cranfield Evaluation Methodology.
The basic idea is to have humans create test correction, where, we already know, every document is tagged with the desired categories.
Or, in the case of search, for which query, which documents that should have been retrieved, and this is called, a ground truth.
Now, with this ground truth test correction, we can then reuse the collection to test the many different systems and then compare different systems.
We can also turn off some components in the system to see what's going to happen.
Basically it provides a way to do control experiments to compare different methods.
So this methodology has been virtually used for all the tasks that involve empirically defined problems.
So in our case, then, we are going to compare our systems categorization results with the categorization, ground truth, created by humans.
And we're going to compare our systems decisions, which documents should get which category with what categories have been assigned to those documents by humans.
And we want to quantify the similarity of these decisions or equivalently, to measure the difference between the system output and the desired ideal output generated by the humans.
So obviously, the highest similarity is the better results are.
The similarity could be measured in different ways.
And that would lead to different measures.
And sometimes it's desirable also to match the similarity from different perspectives just to have a better understanding of the results in detail.
For example, we might be also interested in knowing which category performs better and which which category is easy to categorize, etc.
In general, different categorization mistakes however, have different costs for specific applications.
So some areas might be more serious than others.
So ideally, we would like to model such differences, but if you read many papers in categorization you will see that they don't generally do that.
Instead, they will use a simplified measure and that's because it's often okay not to consider such a cost variation when we compare methods and when we are interested in knowing the relative difference of these methods.
So it's okay to introduce some bias, as long as the bias is not already with a particular method and then we should expect the more effective method to perform better than a less effective one, even though the measure is not perfect.
So the first measure that we'll introduce is called classification accuracy and this is a basic into measure the percentage of correct decisions.
So here you see that there are categories denoted by c1 through ck and there are n documents, denoted by d1 through d N. And for each pair of category and the document, we can then look at the situation.
And see if the system has said yes to this pair, basically has assigned this category to this document.
Or no, so this is denoted by Y or M, that's the systems of the decision.
And similarly, we can look at the human's decisions also, if the human has assigned a category to the document of that there will be a plus sign here.
That just means that a human.
We think of this assignment is correct and incorrect then it's a minus.
So we'll see all combinations of this Ns, yes and nos, minus and pluses.
There are four combinations in total.
And two of them are correct, and that's when we have y(+) or n(-), and then there are also two kinds of errors.
So the measure of classification accuracy is simply to count how many of these decisions are correct.
And normalize that by the total number of decisions we have made.
So, we know that the total number of decisions is n, multiplied by k. And, the number of correct decisions are basically of two kinds.
One is y plusses.
And the other is n minus this n. We just put together the count.
Now, this is a very convenient measure that will give us one number to characterize performance of a method.
And the higher, the better, of course.
But the method also has some problems.
First it has treated all the decisions equally.
But in reality, some decision errors are more serious than others.
For example, it may be more important to get the decisions right on some documents, than others.
Or maybe, more important to get the decisions right on some categories, than others, and this would call for some detailed evaluation of this results to understand the strands and of different methods, and to understand the performance of these methods.
In detail in a per category or per document basis.
One example that shows clearly the decision errors are having different causes is spam filtering that could be retrieved as two category categorization problem.
Missing a legitimate email result, is one type of error.
But letting spam to come into your folder is another type of error.
The two types of errors are clearly very different, because it's very important not to miss a legitimate email.
It's okay to occasionally let a spam email to come into your inbox.
So the error of the first, missing a legitimate email is very, is of high cost.
It's a very serious mistake and classification error, classification accuracy does not address this issue.
There's also another problem with imbalance to test set.
Imagine there's a skew to test set where most instances are category one and 98% of instances are category one.
Only 2% are in category two.
In such a case, we can have a very simple baseline that accurately performs very well and that baseline.
Sign with similar, I put all instances in the major category.
That will get us 98% accuracy in this case.
It's going to be appearing to be very effective, but in reality, this is obviously not a good result.
And so, in general, when we use classification accuracy as a measure, we want to ensure that the causes of balance.
And one above equal number of instances, for example in each class the minority categories or causes tend to be overlooked in the evaluation of classification accuracy.
So, to address these problems, we of course would like to also evaluate the results in other ways and in different ways.
As I said, it's beneficial to look at after multiple perspectives.
So for example, we can look at the perspective from each document as a perspective based on each document.
So the question here is, how good are the decisions on this document?
Now, as in the general cases of all decisions, we can think about four combinations of possibilities, depending on whether the system has said yes and depending on whether the human has said it correct or incorrect or said yes or no.
And so the four combinations are first when both the human systems said yes, and that's the true positives, when the system says, yes, and it's after the positive.
So, when the system says, yes, it's a positive.
But, when the human confirm that it is indeed correct, that becomes a true positive.
When the system says, yes, but the human says, no, that's incorrect, that's a false positive, have FP.
And when the system says no, but the human says yes, then it's a false negative.
We missed one assignment.
When both the system and human says no, then it's also correctly to assume that's true negatives.
All right, so then we can have some measures to just better characterize the performance by using these four numbers and so two popular measures are precision and recall.
And these were also proposed by information retrieval researchers 1960s for evaluating search results, but now they have become standard measures, use it everywhere.
So when the system says yes, we can ask the question, how many are correct?
What's the percent of correct decisions when the system says yes?
That's called precision.
It's true positive divided by all the cases when the system says yes, all the positives.
The other measure is called recall, and this measures whether the document has all the categories it should have.
So in this case it's divide the true positive by true positives and the false negatives.
So these are all the cases where this human Says the document should have this category.
So this represents both categories that it should have got, and so recall tells us whether the system has actually indeed assigned all the categories that it should have to this document.
This gives us a detailed view of the document, then we can aggregate them later.
And if we're interested in some documents, and this will tell us how well we did on those documents, the subsets of them.
It might be more interesting than others, for example.
And this allows us to analyze errors in more detail as well.
We can separate the documents of certain characteristics from others, and then look at the errors.
You might see a pattern A for this kind of document, this long document.
It doesn't as well for shock documents.
And this gives you some insight for inputting the method.
Similarly, we can look at the per-category evaluation.
In this case, we're going to look at the how good are the decisions on a particular category.
As in the previous case we can define precision and recall.
And it would just basically answer the questions from a different perspective.
So when the system says yes, how many are correct?
That means looking at this category to see if all the documents that are assigned with this category are indeed in this category, right?
And recall, would tell us, has the category been actually assigned to all the documents That should have this category.
It's sometimes also useful to combine precision and recall as one measure, and this is often done by using f measure.
And this is just a harmonic mean of precision.
Precision and recall defined on this slide.
And it's also controlled by a parameter beta to indicate whether precision is more important or recall is more.
When beta is set to 1, we have measure called F1, and in this case, we just take equal weight upon both procedure and recall.
F1 is very often used as a measure for categorization.
Now, as in all cases, when we combine results, you always should think about the best way of combining them, so in this case I don't know if you have thought about it and we could have combined them just with arithmetic mean, right.
So that would still give us the same range of values, but obviously there's a reason why we didn't do that and why f1 is more popular, and it's actually useful to think about difference.
And we think about that, you'll see that there is indeed some difference and some undesirable property of this arithmatic.
Basically, it will be obvious to you if you think about a case when the system says yes for all the category and document pairs.
And then try the compute the precision and recall in that case.
And see what would happen.
And basically, this kind of measure, the arithmetic mean, is not going to be as reasonable as F1 minus one [INAUDIBLE] trade off, so that the two values are equal.
There is an extreme case where you have Then F1 will be low, but the mean would still be reasonably high.
This lecture is about the feedback in the language modeling approach.
In this lecture we will continue the discussion of feedback in text retrieval.
In particular we're going to talk about the feedback in language modeling approaches.
So we derive the query likelihood ranking function by making various assumptions.
As a basic retrieval function, that formula, or those formulas worked well.
But if we think about the feedback information, it's a little bit awkward to use query likelihood to perform feedback because a lot of times the feedback information is additional information about the query.
But we assume the query is generated by assembling words from a language model in the query likelihood method.
It's kind of unnatural to sample, words that, form feedback documents.
As a result, then research is proposed, a way to generalize query likelihood function.
It's called a Kullback-Leibler divergence retrieval model.
And this model is actually, going to make the query likelihood, our retrieval function much closer to vector space model.
Yet this, form of the language model can be, regarded as a generalization of query likelihood in the sense that if it can cover query likelihood as a special case.
And in this case the feedback can be achieved through simply query model estimation or updating.
This is very similar to Rocchio which updates the query vector.
So let's see what the, is the scale of divergence, which we will model.
So, on the top, what you see is query likelihood retrieval function, all right, this one.
And then KL-divergence or also called cross entropy retrieval model is basically to generalize the frequency part, here, into a layered model.
So basically it's the difference, given by the probabilistic model here to characterize what the user's looking for versus the kind of query words there.
And this difference allows us to plotting various different ways to estimate this.
So this can be estimated in many different ways including using feedback information.
Now this is called a KL-divergence because this can be interpreted as measuring the KL-divergence of two distributions.
One is the query model denoted by this distribution.
One is the talking, the language model here.
And [INAUDIBLE] though is a [INAUDIBLE] language model, of course.
And we are not going to talk about the detail of that, and you'll find the things in references.
It's also called cross entropy, because, in, in fact, we can ignore some terms in the KL-divergence function and we will end up having actually cross entropy, and that, both are terms in information theory.
But, anyway for our purposes here you can just see the two formulas look almost identical, except that here we have a probability of a word given by a query language model.
This, and here, the sum is over all the words that are in the document, and also with the non-zero probability for the query model.
So it's kind of, again, a generalization of sum over all the matching query words.
Now you can also, easy to see, we can recover the query likelihood, which we will find here by as simple as setting this query model to the relative frequency of a word in the query, right?
This is very to easy see once you practice this.
And to here, you can eliminate this query lens, that's a constant, and then you get exactly like that.
So you can see the equivalence.
And that's also why this KL-divergence model can be regarded as a generalization of query likelihood because we can cover query likelihood as a special case, but it would also allow it to do much more than that.
So this is how we use the KL-divergence model to then do feedback.
The picture shows that we first estimate a document language model, then we estimate a query language model and we compute the KL-divergence, this is often denoted by a D here.
But this basically means, this was exactly like in vector space model because we compute the vector for the document in the computer and not the vector for the query, and then we compute the distance.
Only that these vectors are of special forms, they have probability distributions.
And then we get the results, and we can find some feedback documents.
Let's assume they are more selective sorry, mostly positive documents.
Although we could also consider both kinds of documents.
So what we could do is, like in Rocchio, we can compute another language model called feedback language model here.
Again, this is going to be another vector just like a computing centroid vector in Rocchio.
And then this model can be combined with the original query model using a linear interpolation.
And this would then give us an updated model, just like again in Rocchio.
Right, so here, we can see the parameter of our controlling amount of feedback if it's set to 0, then it says here there's no feedback.
After set to 1, we've got full feedback, we can ignore the original query.
And this is generally not desirable, right.
So this unless you are absolutely sure you have seen a lot of relevant documents and the query terms are not important.
So of course the main question here is how do you compute this theta F?
This is the big question here.
And once you can do that, the rest is easy.
So here we'll talk about one of the approaches.
And there are many approaches of course.
This approach is based on generative model and I'm going to show you how it works.
This is a user generative mixture model.
So this picture shows that the we have this model here, the feedback model that we want to estimate.
And we the basis is the feedback options.
Let's say we are observing the positive documents.
These are the collected documents by users, or random documents judged by users, or simply top ranked documents that we assumed to be random.
Now imagine how we can compute a centroid for these documents by using language model.
One approach is simply to assume these documents are generated from this language model as we did before.
What we could do is do it, just normalize the word frequency here.
And then we, we'll get this word distribution.
Now the question is whether this distribution is good for feedback.
Well you can imagine well the top rank of the words would be what?
What do you think?
Well those words would be common words, right?
As well we see in, in the language model, in the top right, the words are actually common words like, the, et cetera.
So, it's not very good for feedback, because we will be adding a lot of such words to our query when we interpret, this was the original query model.
So, this is not good, so we need to do something, in particular, we are trying to get rid of those common words.
And we all, we have seen actually one way to do that, by using background language model in the case of learning the associations with of words, right.
The words that are related to the word computer.
We could do that, and that would be another way to do this.
But here, we're going to talk about another approach, which is a more principled approach.
In this case, we're going to say, well, you, you said that there are common words here in this, these documents that should not belong to this top model, right?
So now, what we can do is to assume that, well, those words are, generally, from background language model, so they will generate a, those words like the, for example.
And if we use maximum likelihood estimated, note that if all the words here must be generated from this model, then this model is forced to assign high probabilities to a word like the, because it occurs so frequently here.
Note that in order to reduce its probability in this model, we have to have another model, which is this one to help explain the word, the, here.
And in this case, it's not appropriate to use the background language model to achieve this goal because this model will assign high probabilities to these common words.
So in this approach then, we assume this machine that which generated these words would work as follows.
We have a source controller here.
Imagine we flip a coin here to decide what distribution to use.
With the probability of lambda the coin shows up as head.
And then we're going to use the background language model.
And we can do then sample word from that model.
With probability of 1 minus lambda now, we now decide to use a unknown topic model here that we will try to estimate.
And we're going to then generate a word here.
If we make this assumption, and this whole thing will be just one model, and we call this a mixture model, because there are two distributions that are mixed here together.
And we actually don't know when each distribution is used.
Right, so again think of this whole thing as one model.
And we can still ask it for words, and it will still give us a word in a random method, right?
And of course which word will show up will depend on both this distribution and that distribution.
In addition, it would also depend on this lambda, because if you say, lambda is very high and it's going to always use the background distribution, you'll get different words.
If you say, well our lambda is very small, we're going to use this, all right?
So all these are parameters, in this model.
And then, if you're thinking this way, basically we can do exactly the same as what we did before, we're going to use maximum likelihood estimator to adjust this model to estimate the parameters.
Basically we're going to adjust, well, this parameter so that we can best explain all the data.
The difference now is that we are not asking this model alone to explain this.
But rather we're going to ask this whole model, mixture model, to explain the data because it has got some help from the background model.
It doesn't have to assign high probabilities towards like the, as a result.
It would then assign high probabilities to other words that are common here but not having high probability here.
So those would be common here.
Right?
And if they're common they would have to have high probabilities, according to a maximum likelihood estimator.
And if they are rare here, all right, so if they are rare here, then you don't get much help from this background model.
As a result, this topic model must assign high probabilities.
So the higher probability words according to the topic model will be those that are common here, but rare in the background.
Okay, so, this is basically a little bit like a idea for weighting here.
This would allow us to achieve the effect of removing these top words that are meaningless in the feedback.
So mathematically what we have is to compute the likelihood again, local likelihood of the feedback documents.
And, and note that, we also have another parameter, lambda here.
But we assume that lambda denotes noise in the feedback document.
So we are going to, let's say, set this to a parameter, let's say, say 50% of the words are noise, or 90% are noise.
And this can then be, assume it will be fixed.
If we assume this is fixed, then we only have these probabilities as parameters just like in the simplest unigram language model, we have n parameters.
n is the number of words and, then, the likelihood function will look like this.
It's very similar to the likelihood function, normal likelihood function we see before except that inside the logarithm there's a sum in here.
And this sum is because we can see the two distributions.
And which ones used would depend on lambda and that's why we have this form.
But mathematically this is the function with theta as unknown variables, right?
So, this is just a function.
All the other variables are known, except for this guy.
So, we can then choose this probability distribution to maximize this log likelihood.
The same idea as the maximum likelihood estimator.
As a mathematical problem which is to, we just have to solve this optimization problem.
We said we would try all of the theta values, and here we find one that gives this whole thing the maximum probability.
So, it's a well-defined math problem.
Once we have done that, we obtain this theta F, that can be the interpreter with the original query model to do feedback.
So here are some examples of the feedback model learned from a web document collection, and we do pseudo-feedback.
We just use the top 10 documents, and we use this mixture model.
So the query is airport security.
What we do is we first retrieve ten documents from the web database.
And this is of course pseudo-feedback, right?
And then we're going to feed to that mixture model, to this ten document set.
And these are the words learned using this approach.
This is the probability of a word given by the feedback model in both cases.
So, in both cases, you can see the highest probability of words include very random words to the query.
So, airport security for example, these query words still show up as high probabilities in each case naturally because they occur frequently in the top rank of documents.
But we also see beverage, alcohol, bomb, terrorist, et cetera.
Right, so these are relevant to this topic, and they, if combined with original query can help us match more accurately, on documents.
And also they can help us bring up documents that only managing the, some of these other words.
And maybe for example just airport and then bomb for example.
These so, this is how pseudo-feedback works.
It shows that this model really works and picks up mm, some related words to the query.
What's also interesting is that if you look at the two tables here, and you compare them, and you see in this case, when lambda is set to a small value, and we'll still see some common words here, and that means.
When we don't use the background model often, remember lambda can use the probability of using the background model to generate to the text.
If we don't rely much on background model, we still have to use this topped model to account for the common words.
Whereas if we set lambda to a very high value we would use the background model very often to explain these words, then there is no burden on expanding those common words in the feedback documents by the topping model.
So, as a result, the top of the model here is very discriminative.
It contains all the relevant words without common words.
So this can be added to the original query to achieve feedback.
So to summarize in this lecture we have talked about the feedback in language model approach.
In general, feedback is to learn from examples.
These examples can be assumed examples, can be pseudo-examples, like assume the, the top ten documents are assumed to be random.
They could be based on using fractions like feedback, based on quick sorts or implicit feedback.
We talked about the three major feedback scenarios, relevance feedback, pseudo-feedback, and implicit feedback.
We talked about how to use Rocchio to do feedback in vector-space model and how to use query model estimation for feedback in language model.
And we briefly talked about the mixture model and the basic idea and there are many other methods.
For example the relevance model is a very effective model for estimating query model.
So, you can read more about the, these methods in the references that are listed at the end of this lecture.
So there are two additional readings here.
The first one is a book that has a systematic, review and discussion of language models of more information retrieval.
And the second one is an important research paper that's about relevance based language models and it's a very effective way of computing query model.
So, I just showed you that empirically the likelihood will converge, but theoretically it can also be proved that EM algorithm will converge to a local maximum.
So here's just an illustration of what happened and a detailed explanation.
This required more knowledge about that, some of that inequalities, that we haven't really covered yet.
So here what you see is on the X dimension, we have a c0 value.
This is a parameter that we have.
On the y axis we see the likelihood function.
So this curve is the original likelihood function, and this is the one that we hope to maximize.
And we hope to find a c0 value at this point to maximize this.
But in the case of Mitsumoto we can not easily find an analytic solution to the problem.
So, we have to resolve the numerical errors, and the EM algorithm is such an algorithm.
It's a Hill-Climb algorithm.
That would mean you start with some random guess.
Let's say you start from here, that's your starting point.
And then you try to improve this by moving this to another point where you can have a higher likelihood.
So that's the ideal hill climbing.
And in the EM algorithm, the way we achieve this is to do two things.
First, we'll fix a lower bound of likelihood function.
So this is the lower bound.
See here.
And once we fit the lower bound, we can then maximize the lower bound.
And of course, the reason why this works, is because the lower bound is much easier to optimize.
So we know our current guess is here.
And by maximizing the lower bound, we'll move this point to the top.
To here.
Right?
And we can then map to the original likelihood function, we find this point.
Because it's a lower bound, we are guaranteed to improve this guess, right?
Because we improve our lower bound and then the original likelihood curve which is above this lower bound will definitely be improved as well.
So we already know it's improving the lower bound.
So we definitely improve this original likelihood function, which is above this lower bound.
So, in our example, the current guess is parameter value given by the current generation.
And then the next guess is the re-estimated parameter values.
From this illustration you can see the next guess is always better than the current guess.
Unless it has reached the maximum, where it will be stuck there.
So the two would be equal.
So, the E-step is basically to compute this lower bound.
We don't directly just compute this likelihood function but we compute the length of the variable values and these are basically a part of this lower bound.
This helps determine the lower bound.
The M-step on the other hand is to maximize the lower bound.
It allows us to move parameters to a new point.
And that's why EM algorithm is guaranteed to converge to a local maximum.
Now, as you can imagine, when we have many local maxima, we also have to repeat the EM algorithm multiple times.
In order to figure out which one is the actual global maximum.
And this actually in general is a difficult problem in numeral optimization.
So here for example had we started from here, then we gradually just climb up to this top.
So, that's not optimal, and we'd like to climb up all the way to here, so the only way to climb up to this gear is to start from somewhere here or here.
So, in the EM algorithm, we generally would have to start from different points or have some other way to determine a good initial starting point.
To summarize in this lecture we introduced the EM algorithm.
This is a general algorithm for computing maximum maximum likelihood estimate of all kinds of models, so not just for our simple model.
And it's a hill-climbing algorithm, so it can only converge to a local maximum and it will depend on initial points.
The general idea is that we will have two steps to improve the estimate of.
In the E-step we roughly [INAUDIBLE] how many there are by predicting values of useful hidden variables that we would use to simplify the estimation.
In our case, this is the distribution that has been used to generate the word.
In the M-step then we would exploit such augmented data which would make it easier to estimate the distribution, to improve the estimate of parameters.
Here improve is guaranteed in terms of the likelihood function.
Note that it's not necessary that we will have a stable convergence of parameter value even though the likelihood function is ensured to increase.
There are some properties that have to be satisfied in order for the parameters also to convert into some stable value.
Now here data augmentation is done probabilistically.
That means, we're not going to just say exactly what's the value of a hidden variable.
But we're going to have a probability distribution over the possible values of these hidden variables.
So this causes a split of counts of events probabilistically.
And in our case we'll split the word counts between the two distributions.
This lecture is a brief introduction to the course.
We're going to cover the objectives of the course, the prerequisites and course formats, reference books and how to complete the course.
The objectives of the course are the following.
First, we would like to cover the basic context and practical techniques of text data mining.
So this means we will not be able to cover some advanced techniques in detail, but whether we choose the practical use for techniques and then treat them in order.
We're going to also cover the basic concepts that are very useful for many applications.
The second objective is to cover more general techniques for text or data mining, so we emphasize the coverage of general techniques that can be applicable to any text in any natural language.
We also hope that these techniques to either automatically work on problems without any human effort or only requiring minimum human effort.
So these criteria have helped others to choose techniques that can be applied to many applications.
This is in contrast to some more detailed analysis of text data, particularly using natural language processing techniques.
Now such techniques are also very important.
And they are indeed, necessary for some of the applications, where we would like to go in-depth to understand text, they are in more detail.
Such detail in understanding techniques, however, are generally not scalable and they tend to require a lot of human effort.
So they cannot be easy to apply to any domain.
So as you can imagine in practice, it would be beneficial to combine both kinds of techniques using the general techniques that we'll be covering in this course as a basis and improve these techniques by using more human effort whenever it's appropriate.
We also would like to provide a hands-on experience to you in multiple aspects.
First, you'll do some experiments using a text mining toolkit and implementing text mining algorithms.
Second, you will have opportunity to experiment with some algorithms for text mining and analytics to try them on some datasets and to understand how to do experiments.
And finally, you have opportunity to participate in a competition of text-based prediction task.
You're expected to know the basic concepts of computer science.
For example, the data structures and some other really basic concepts in computer science.
You are also expected to be familiar with programming and comfortable with programming, particularly with C++.
This course, however is not about programming.
So you are not expected to do a lot of coding, but we're going to give you C++ toolkit that's fairly sophisticated.
So you have to be comfortable with handling such a toolkit and you may be asked to write a small amount of code.
It's also useful if you know some concepts and techniques in probability and statistics, but it's not necessary.
Knowing such knowledge would help you understand some of the algorithm in more depth.
The format of the course is lectures plus quizzes that will be given to you in the regular basis and there is also optional programming assignment.
Now, we've made programming assignments optional.
Not because it's not important, but because we suspect that the not all of you will have the need for computing resources to do the program assignment.
So naturally, we would encourage all of you to try to do the program assignments, if possible as that will be a great way to learn about the knowledge that we teach in this course.
There's no required reading for this course, but I was list some of the useful reference books here.
So we expect you to be able to understand all the essential materials by just watching the actual videos and you should be able to answer all the quiz questions by just watching the videos.
But it's always good to read additional books in the larger scope of knowledge, so here is this the four books.
The first is a textbook about statistical language processing.
Some of the chapters [INAUDIBLE] are especially relevant to this course.
The second one is a textbook about information retrieval, but it has broadly covered a number of techniques that are really in the category of text mining techniques.
So it's also useful, because of that.
The third book is actually a collection of silly articles and it has broadly covered all the aspects of mining text data.
The mostly relevant chapters are also listed here.
In these chapters, you can find some in depth discussion of cutting edge research on the topics that we discussed in this course.
And the last one is actually a book that Sean Massung and I are currently writing and we're going to make the rough draft chapters available at this URL listed right here.
You can also find additional reference books and other readings at the URL listed at the bottom.
So finally, some information about how to complete the course this information is also on the web.
So I just briefly go over it and you can complete the course by earning one of the following badges.
One is Course Achievement Badge.
To earn that, you have to have at least a 70% average score on all the quizzes combined.
It does mean every quiz has to be 70% or better.
The second batch here, this is a Course Mastery Badge and this just requires a higher score, 90% average score for the quizzes.
There are also three optional programming badges.
I said earlier that we encourage you to do programming assignments, but they're not necessary, they're not required.
The first is Programming Achievement Badge.
This is similar to the call switching from the badge.
Here would require you to get at least 70% average score on programming assignments.
And similarly, the mastery badge is given to those who can score 90% average score or better.
The last badge is a Text Mining Competition Leader Badge and this is given to those of you who do well in the competition task.
And specifically, we're planning to give the badge to the top 30% in the leaderboard.
This lecture is a continuing discussion of generative probabilistic models for tax classroom.
In this lecture, we're going to do a finishing discussion of generative probabilistic models for text crossing.
So this is a slide that you have seen before and here, we show how we define the mixture model for text crossing and what the likelihood function looks like.
And we can also compute the maximum likelihood estimate, to estimate the parameters.
In this lecture, we're going to do talk more about how exactly we're going to compute the maximum likelihood estimate.
As in most cases the Algorithm can be used to solve this problem for mixture models.
So here's the detail of this Algorithm for document clustering.
Now, if you have understood how Algorithm works for topic models like TRSA, and I think here it would be very similar.
And we just need to adapt a little bit to this new mixture model.
So as you may recall Algorithm starts with initialization of all the parameters.
So this is the same as what happened before for topic models.
And then we're going to repeat until the likelihood converges and in each step we'll do E step and M step.
In M step, we're going to infer which distribution has been used to generate each document.
So I have to introduce a hidden variable Zd for each document and this variable could take a value from the range of 1 through k, representing k different distributions.
More specifically basically, we're going to apply base rules to infer which distribution is more likely to have generated this document, or computing the posterior probability of the distribution given the document.
And we know it's proportional to the probability of selecting this distribution p of Z the i.
And the probability of generating this whole document from the distribution which is the product of the probabilities of world for this document as you see here.
Now, as you all clear this use for kind of remember, the normalizer or the constraint on this probability.
So in this case, we know the constraint on this probability in E-Step is that all the probabilities of Z equals i must sum to 1.
Because the documented must have been generated from precisely one of these k topics.
So the probability of being generated from each of them should sum to 1.
And if you know this constraint, then you can easily compute this distribution as long as you know what it is proportional to.
So once you compute this product that you see here, then you simply normalize these probabilities, to make them sum to 1 over all the topics.
So that's E-Step, after E-Step we want to know which distribution is more likely to have generated this document d, and which is unlikely.
And then in M-Step we're going to re-estimate all the parameters based on the in further z values or in further knowledge about which distribution has been used to generate which document.
So the re-estimation involves two kinds of parameters 1 is p of theta and this is the probability of selecting a particular distribution.
Before we observe anything, we don't have any knowledge about which cluster is more likely.
But after we have observed that these documents, then we can crack the evidence to infer which cluster is more likely.
And so this is proportional to the sum of the probability of Z sub d j is equal to i.
And so this gives us all the evidence about using topic i, theta i to generate a document.
Pull them together and again, we normalize them into probabilities.
So this is for key of theta sub i.
Now the other kind of parameters are the probabilities of words in each distribution, in each cluster.
And this is very similar to the case piz and here we just report the kinds of words that are in documents that are inferred to have been generated from a particular topic of theta i here.
This would allows to then estimate how many words have actually been generated from theta i.
And then we'll normalize again these accounts in the probabilities so that the probabilities on all the words would sum to up.
Note that it's very important to understand these constraints as they are precisely the normalizing in all these formulas.
And it's also important to know that the distribution is over what?
For example, the probability of theta is over all the k topics, that's why these k probabilities will sum to 1.
Whereas the probability of a word given theta is a probability distribution over all the words.
So there are many probabilities and they have to sum to 1.
So now, let's take a look at a simple example of two clusters.
I've two clusters, I've assumed some initialized values for the two distributions.
And let's assume we randomly initialize two probability of selecting each cluster as 0.5, so equally likely.
And then let's consider one document that you have seen here.
There are two occurrences of text and two occurrences of mining.
So there are four words together and medical and health did not occur in this document.
So let's think about the hidden variable.
Now for each document then we much use a hidden variable.
And before in piz, we used one hidden variable for each work because that's the output from one mixture model.
So in our case the output from the mixture model or the observation from mixture model is a document, not a word.
So now we have one hidden variable attached to the document.
Now that hidden variable must tell us which distribution has been used to generate the document.
So it's going to take two values, one and two to indicate the two topics.
So now how do we infer which distribution has been used generally d?
Well it's been used base rule, so it looks like this.
In order for the first topic theta 1 to generate a document, two things must happen.
First, theta sub 1 must have been selected.
So it's given by p of theta 1.
Second, it must have also be generating the four words in the document.
Namely, two occurrences of text and two occurrences of sub mining.
And that's why you see the numerator has the product of the probability of selecting theta 1 and the probability of generating the document from theta 1.
So the denominator is just the sum of two possibilities of generality in this document.
And you can plug in the numerical values to verify indeed in this case, the document is more likely to be generated from theta 1, much more likely than from theta 2.
So once we have this probability, we can easily compute the probability of Z equals 2, given this document.
How?
Well, we can use the constraint.
That's going to be 1 minus 100 over 101.
So now it's important that you note that in such a computation there is a potential problem of underflow.
And that is because if you look at the original numerator and the denominator, it involves the competition of a product of many small probabilities.
Imagine if a document has many words and it's going to be a very small value here that can cause the problem of underflow.
So to solve the problem, we can use a normalize.
So here you see that we take a average of all these two math solutions to compute average at the screen called a theta bar.
And this average distribution would be comparable to each of these distributions in terms of the quantities or the magnitude.
So we can then divide the numerator and the denominator both by this normalizer.
So basically this normalizes the probability of generating this document by using this average word distribution.
So you can see the normalizer is here.
And since we have used exactly the same normalizer for the numerator and the denominator.
The whole value of this expression is not changed but by doing this normalization you can see we can make the numerators and the denominators more manageable in that the overall value is not going to be very small for each.
And thus we can avoid the underflow problem.
In some other times we sometimes also use logarithm of the product to convert this into a sum of log of probabilities.
This can help preserve precision as well, but in this case we cannot use algorithm to solve the problem.
Because there is a sum in the denominator, but this kind of normalizes can be effective for solving this problem.
So it's a technique that's sometimes useful in other situations in other situations as well.
Now let's look at the M-Step.
So from the E-Step we can see our estimate of which distribution is more likely to have generated a document at d. And you can see d1's more like got it from the first topic, where is d2 is more like from second topic, etc.
Now, let's think about what we need to compute in M-step well basically we need to re-estimate all the parameters.
First, look at p of theta 1 and p of theta 2.
How do we estimate that?
Intuitively you can just pool together these z, the probabilities from E-step.
So if all of these documents say, well they're more likely from theta 1, then we intuitively would give a higher probability to theta 1.
In this case, we can just take an average of these probabilities that you see here and we've obtain a 0.6 for theta 1.
So 01 is more likely and then theta 2.
So you can see probability of What about these word of probabilities?
Well we do the same, and intuition is the same.
So we're going to see, in order to estimate the probabilities of words in theta 1, we're going to look at which documents have been generated from theta 1.
And we're going to pull together the words in those documents and normalize them.
So this is basically what I just said.
More specifically, we're going to do for example, use all the kinds of text in these documents for estimating the probability of text given theta 1.
But we're not going to use their raw count or total accounts.
Instead, we can do that discount them by the probabilities that each document is likely been generated from theta 1.
So these gives us some fractional accounts.
And then these accounts would be then normalized in order to get the probability.
Now, how do we normalize them?
Well these probability of these words must assign to 1.
So to summarize our discussion of generative models for clustering.
Well we show that a slight variation of topic model can be used for clustering documents.
And this also shows the power of generating models in general.
By changing the generation assumption and changing the model slightly we can achieve different goals, and we can capture different patterns and types of data.
So in this case, each cluster is represented by unigram language model word distribution and that is similar to topic model.
So here you can see the word distribution actually generates a term cluster as a by-product.
A document that is generated by first choosing a unigram language model.
And then generating all the words in the document are using just a single language model.
And this is very different from again copy model where we can generate the words in the document by using multiple unigram language models.
And then the estimated model parameters are given both topic characterization of each cluster and the probabilistic assignment of each document into a cluster.
And this probabilistic assignment sometimes is useful for some applications.
But if we want to achieve harder clusters mainly to partition documents into disjointed clusters.
Then we can just force a document into the cluster corresponding to the words distribution that's most likely to have generated the document.
We've also shown that the Algorithm can be used to compute the maximum likelihood estimate.
And in this case, we need to use a special number addition technique to avoid underflow.
.
This lecture is about the Inverted Index Construction.
In this lecture, we will continue the discussion of system implementation.
In particular, we're going to discuss how to construct the inverted index.
The construction of the inverted index is actually very easy if the data set is very small.
It's very easy to construct a dictionary and then store the postings in a file.
The problem's that when our data is not able to fit to the memory, then we have to use some special method to deal with it.
And unfortunately, in most retrieval a petitions, the data set would be large and they generally cannot be, loaded into the memory at once.
And there are many approaches to solving that problem, and sorting-based method, is quite common and works in four steps as shown here.
First, we collect the the local termID, document ID, and frequency tuples.
Basically, you overlook kinds of terms in a small set of documents, and, and then, once you collect those counts, you can sort those counts based on terms so that you build a local, a partial inverted index.
And these are called, runs.
And then, you write them into a temporary file on the disk.
And then, you merge in step three with do pair-wise merging of these runs, and here, you eventually merge all the runs, we generate a single inverted index.
So this is an illustration of this method.
On the left, you see some documents.
And on the right, we have, show a term lexicon and a document ID lexicon.
And these lexicon's are to map a stream based representations of document IDs or terms into integer representations.
Or, and, map back from, integers to the screen representation.
And the reason why we want, are interested in using integers represent these IDs, is because, integers are often easier to handle.
For example, integers can be used as index for array and they are also easy to compress.
So this is a, one reason why we, tend to map these streams into integers so that so that we don't have to, carry these streams around.
So how does this approach work?
Well, it's very simple.
We're going to scan these documents sequentially, and then pause the documents and a count the frequencies of terms.
And in this, stage we generally sort the frequencies by document IDs because we process each document that sequentially.
So, we first encounter all the terms in, the first document.
Therefore, the document IDs, are all once in this stage.
And so, and, this would be followed by document IDs 2.
And, and they're naturally sort in this order just because we process the data in this order.
At some point, the, we will run out of memory and that would have to, to write them into the disk.
But before we do that, we're going to a sort them, just, use whatever memory we have, we can sort them, and then, this time, we're going to sort based on term IDs.
Note that here, we're using, this, the term IDs as a key to sort.
So, all the entries that share the same term would be grouped together.
In this case, we can see all the, all the IDs of documents that match term one would be grouped together.
And we're going to write this into the disk as a temporary file.
And that would, allow us to use the memory to process the next batch of documents, and we're going to do that for all the documents.
So we're going to write a lot of temporary files into the disk.
And then, the next stage is to do merge sort.
Basically, we're going to, merge them and the sort them.
Eventually, we will get a single inverted index where the, their entries are sorted based on term IDs.
And on the top, we can see these are the order entries for the documents that match term ID 1.
So this is basically how we can do, the construction of inverted index, even though that they're or cannot be, or loaded into the memory.
Now, we mentioned earlier that because the po, postings are very large, it's desirable to compress them.
So let's now talk a little bit about how we compress inverted index.
Well, the idea of compression, in general, is you leverage skewed distributions of values.
And we generally have to use variable lengths in coding instead of the fixed lengths in coding as we', using, by defaulting a program language like C++.
And so, how can we leverage the skewed distributions of values to, compress these values?
Well, in general, we would use fewer bits to encode those frequent words at a cost of using, longer bits from the code than those, rare values.
So in our case, let's think about how we can compress the tf, term frequency.
If you can picture what the inverted index would look like and you'll see in postings there are a lot of, term frequencies.
Those are the frequencies of terms, in all those documents.
Now, we, if you think about it, what kind of values are most frequent there?
You probably will, be able to guess that the small numbers tend to occur far more frequently than large numbers.
Why?
Well, think of about the distribution of words, and this is due to Zipf's law and many words occur just, rarely.
So we see a lot of small numbers, therefore, we can use fewer bits for the small, but highly frequent integers, and at the cost of using more bits for large integers.
This is a trade-off, of course.
If the values are distributed uniformly and this won't save us any, spacing.
But because we tend to see many small values, they're very frequent.
We can save on average even though sometimes, when we see a large number we have to use a lot of bits.
What about the document IDs that we also saw in postings.
Well, they are not, distributed in a skewed way, right?
So, how can we deal with that?
Well, it turns out you can use a trick called the d-gap, and that, that is to store the difference of these term IDs.
And we can, imagine if a term has matched many documents, then there will be a long list of document IDs.
So when we take the gap, and when we take difference between adjacent document IDs, those gaps will be small.
So we'll again see a lot of small numbers, whereas, if a term occurred in only a few documents, then the gap would be large.
The larger numbers will not be frequent, so this creates some skewed distribution that would allow us to, to compress these values.
This is also possible because in order to uncover or uncompress these document IDs, we have to sequentially process the data because we stored the difference.
And in order to recover the, the exact document ID, we have to first recover the previous document ID, and then, we can add the difference to the previous document ID to restore the, the current document ID.
Now, this was possible because we only needed to have sequential access to those document IDs.
Once we look up a term we fetch all the document IDs that match the term, then we sequentially process them.
So it's very natural that's why this, trick actually works.
And there are many different methods for encoding.
So binary code is a common used code in, in just any program.
Language that we use basically a fixed length in coding.
Unary code and gamma code, and delta code are all possible in this and there are many other possible in this.
So let's look at some of them in more detail.
Binary code is really equal-length in coding.
And that's a property for the randomly distributed values.
The unary coding is is a variable and it's important [INAUDIBLE].
In this case, integer that is, I've missed one or we encode that as x minus 1, So for example, 3 would be encoded as two 1s followed by a 0, whereas 5 would be encoded as four 1s followed by 0, et cetera.
So now, now you can imagine how many bits do we have to use for a large number like 100.
So, how many bits do I have to use for exactly for a number like 100?
Well, exactly, we have to use 100 bits, but so, it's the same number of bits as the value of this number.
So, this is very inefficient.
If you were likely to see some large numbers, imagine if you occasionally see a number like 1000, you have to use 1000 bits.
So, this only works where if you are absolutely sure that there would be no large numbers.
Mostly very frequent, they're often using very small numbers.
Now, how do you decode this code?
Since these are variables lengths in coding methods, and you can't just count how many bits and then just stop.
Right?
You can say eight bits or 32 bits, then you, you will start another code.
There are variable lengths, so, you have to rely on some mechanism.
In this case for unary, you can see it's very easy to see the boundary.
Now you can easily see 0 would signal the end of encoding.
So you just count how many 1s you have seen, and then you hit the 0.
You know you have finished one number, you start another number.
Now which is to start at unary code is to aggressive in rewarding small numbers.
And if you occasionally can see a very big number, it will be a disaster.
So what about some other less aggressive method?
Well, gamma coding is one of them.
And in this method, we can do, use unary coding for a transformed form of the value.
So it's 1 plus the flow of log of x.
So the magnitude of this value is much lower than the original, x.
So that's why we have four using urinary code for that so, and so we, first we have the urinary code for coding this log of s. And this will be followed by a uniform code or binary code, and this is basically the same uniform code and binary code are the same.
And we're going to use this code to code the remaining part of the value of x.
And this is basically, precisely, x minus 1, 2 to the flow of log of x.
So the unary code or basically code with a flow of log of x, well, I added one there, and here.
But the remaining part will, we using uniform code to actually code the difference between the x and and this, 2 to the log of x.
And, and it's easy to to show that for this this value, there's difference.
We only need to use up to, this many bits and in flow of log of x bits.
And this is easy to understand, if the difference is too large then we would have a higher flow of log of x.
So, here are some examples.
For example, 3 is encoded as 101.
The first two digits are the unary code.
Right.
So, this is for the value 2.
Right.
And so, that means log of x, the flow of log of x is 1, because we will actually use unary code to encode 1 plus the flow of log of x.
Since this is 2, then we know that the floor of log of x is actually 1.
So but, the difference is 1, and that 1 is encoded here at the end.
So that's why we have 101 for 3.
Now, similarly 5 is encoded as 110 followed by 01.
And in this case, the unary code encodes 3.
So, this is the unary code for 110 and so the floor of log of x is 2.
And that means, we will compute the difference between 5 and the 2 to the 2, and that's 1, and so we now have again 1 at the end.
But this time, we're going to use two bits because with this level of flow of log of x, we could have more numbers, 5, 6, 7.
They would all share the same prefix here, 110.
So, in order to differentiate them, we have to use two bits, in the end to differentiate them.
So you can imagine 6 would be, 10 here in the end instead of 01, after 110.
It's also true that the form of a gamma code is always, the first odd number of bits, and in the center, there was a 0.
That's the end of the unary code.
And before that, or to, on the left side of this 0, there will be all 1s.
And on the right side of this 0, it's binary coding or uniform coding.
So how can you decode such a code?
Well, you again first do unary coding, right?
Once you hit 0, you know you have got the unary code.
And this also will tell you how many bits you have to read further to decode the uniform code.
So this is how you can decode a gamma code.
There is also delta code, but that's basically same as gamma code, except that you replace the unary prefix with the gamma code.
So that's even less conservative than gamma code, in terms of avoiding the small integers.
So that means it's okay if you occasionally see a large number.
It's, it's, you know, it's okay with delta code.
It's also fine with gamma code.
It's really a big loss for unary code, and they are all operating, of course, at different degrees of favoring short favoring small integers.
And that also means they would appropriate for sorting distribution.
But none of them is perfect for all distributions.
And which method works, the best would have to depend on the actual distribution in your data set.
For inverted index, compression, people have found that gamma coding seems to work well.
So how to uncompress inverted index?
We just, talked about this.
Firstly, you decode those encode integers.
And we just, I think discussed how we decode unary coding and gamma coding.
So I won't repeat.
What about the document IDs that might be compressed using d-gap?
Well, we're going to do sequential decoding.
So suppose the encoded idealist is x1, x2, x3 et cetera.
We first decode x1 to obtain the first document ID, ID1.
Then, we will decode x2, which is actually the difference between the second ID and the first one.
So we have to add the decoded value of x2 to ID1 to recover the value of the, the ID at this secondary position, right.
So this is where you can see the advantage of, converting document IDs into integers.
And that allows us to do this kind of compression, and we just repeat until we decode all the documents.
Every time we use the document ID in the previous position to help recover the document ID in the next position.
This lecture is about the contextual text mining.
Contextual text mining is related to multiple kinds of knowledge that we mine from text data, as I'm showing here.
It's related to topic mining because you can make topics associated with context, like time or location.
And similarly, we can make opinion mining more contextualized, making opinions connected to context.
It's related to text based prediction because it allows us to combine non-text data with text data to derive sophisticated predictors for the prediction problem.
So more specifically, why are we interested in contextual text mining?
Well, that's first because text often has rich context information.
And this can include direct context such as meta-data, and also indirect context.
So, the direct context can grow the meta-data such as time, location, authors, and source of the text data.
And they're almost always available to us.
Indirect context refers to additional data related to the meta-data.
So for example, from office, we can further obtain additional context such as social network of the author, or the author's age.
Such information is not in general directly related to the text, yet through the process, we can connect them.
There could be other text data from the same source, as this one through the other text can be connected with this text as well.
So in general, any related data can be regarded as context.
So there could be removed or rated for context.
And so what's the use?
What is text context used for?
Well, context can be used to partition text data in many interesting ways.
It can almost allow us to partition text data in other ways as we need.
And this is very important because this allows us to do interesting comparative analyses.
It also in general, provides meaning to the discovered topics, if we associate the text with context.
So here's illustration of how context can be regarded as interesting ways of partitioning of text data.
So here I just showed some research papers published in different years.
On different venues, different conference names here listed on the bottom like the SIGIR or ACL, etc.
Now such text data can be partitioned in many interesting ways because we have context.
So the context here just includes time and the conference venues.
But perhaps we can include some other variables as well.
But let's see how we can partition this interesting of ways.
First, we can treat each paper as a separate unit.
So in this case, a paper ID and the, each paper has its own context.
It's independent.
But we can also treat all the papers within 1998 as one group and this is only possible because of the availability of time.
And we can partition data in this way.
This would allow us to compare topics for example, in different years.
Similarly, we can partition the data based on the menus.
We can get all the SIGIR papers and compare those papers with the rest.
Or compare SIGIR papers with KDD papers, with ACL papers.
We can also partition the data to obtain the papers written by authors in the U.S., and that of course, uses additional context of the authors.
And this would allow us to then compare such a subset with another set of papers written by also seen in other countries.
Or we can obtain a set of papers about text mining, and this can be compared with papers about another topic.
And note that these partitionings can be also intersected with each other to generate even more complicated partitions.
And so in general, this enables discovery of knowledge associated with different context as needed.
And in particular, we can compare different contexts.
And this often gives us a lot of useful knowledge.
For example, comparing topics over time, we can see trends of topics.
Comparing topics in different contexts can also reveal differences about the two contexts.
So there are many interesting questions that require contextual text mining.
Here I list some very specific ones.
For example, what topics have been getting increasing attention recently in data mining research?
Now to answer this question, obviously we need to analyze text in the context of time.
So time is context in this case.
Is there any difference in the responses of people in different regions to the event, to any event?
So this is a very broad an answer to this question.
In this case of course, location is the context.
What are the common research interests of two researchers?
In this case, authors can be the context.
Is there any difference in the research topics published by authors in the USA and those outside?
Now in this case, the context would include the authors and their affiliation and location.
So this goes beyond just the author himself or herself.
We need to look at the additional information connected to the author.
Is there any difference in the opinions of all the topics expressed on one social network and another?
In this case, the social network of authors and the topic can be a context.
Other topics in news data that are correlated with sudden changes in stock prices.
In this case, we can use a time series such as stock prices as context.
What issues mattered in the 2012 presidential campaign, or presidential election?
Now in this case, time serves again as context.
So, as you can see, the list can go on and on.
Basically, contextual text mining can have many applications.
So let's plug in these model masses into the ranking function to see what we will get, okay?
This is a general smoothing.
So a general ranking function for smoothing with subtraction and you have seen this before.
And now we have a very specific smoothing method, the JM smoothing method.
So now let's see what what's a value for office of D here.
And what's the value for p sub c here?
Right, so we may need to decide this in order to figure out the exact form of the ranking function.
And we also need to figure out of course alpha.
So let's see.
Well this ratio is basically this, right, so, here, this is the probability of c board on the top, and this is the probability of unseen war or, in other words basically 11 times basically the alpha here, this, so it's easy to see that.
This can be then rewritten as this.
Very simple.
So we can plug this into here.
And then here, what's the value for alpha?
What do you think?
So it would be just lambda, right?
And what would happen if we plug in this value here, if this is lambda.
What can we say about this?
Does it depend on the document?
No, so it can be ignored.
Right?
So we'll end up having this ranking function shown here.
And in this case you can easy to see, this a precisely a vector space model because this part is a sum over all the matched query terms, this is an element of the query map.
What do you think is a element of the document up there?
Well it's this, right.
So that's our document left element.
And let's further examine what's inside of this logarithm.
Well one plus this.
So it's going to be nonnegative, this log of this, it's going to be at least 1, right?
And these, this is a parameter, so lambda is parameter.
And let's look at this.
Now this is a TF.
Now we see very clearly this TF weighting here.
And the larger the count is, the higher the weighting will be.
We also see IDF weighting, which is given by this.
And we see docking the lan's relationship here.
So all these heuristics are captured in this formula.
What's interesting that we kind of have got this weighting function automatically by making various assumptions.
Whereas in the vector space model, we had to go through those heuristic design in order to get this.
And in this case note that there's a specific form.
And when you see whether this form actually makes sense.
All right so what do you think is the denominator here, hm?
This is a math of document.
Total number of words, multiplied by the probability of the word given by the collection, right?
So this actually can be interpreted as expected account over word.
If we're going to draw, a word, from the connection that we model.
And, we're going to draw as many as the number of words in the document.
If you do that, the expected account of a word, w, would be precisely given by this denominator.
So, this ratio basically, is comparing the actual count, here.
The actual count of the word in the document with expected count given by this product if the word is in fact following the distribution in the clutch this.
And if this counter is larger than the expected counter in this part, this ratio would be larger than one.
So that's actually a very interesting interpretation, right?
It's very natural and intuitive, it makes a lot of sense.
And this is one advantage of using this kind of probabilistic reasoning where we have made explicit assumptions.
And, we know precisely why we have a logarithm here.
And, why we have these probabilities here.
And, we also have a formula that intuitively makes a lot of sense and does TF-IDF weighting and documenting and some others.
Let's look at the, the Dirichlet Prior Smoothing.
It's very similar to the case of JM smoothing.
In this case, the smoothing parameter is mu and that's different from lambda that we saw before.
But the format looks very similar.
The form of the function looks very similar.
So we still have linear operation here.
And when we compute this ratio, one will find that is that the ratio is equal to this.
And what's interesting here is that we are doing another comparison here now.
We're comparing the actual count.
Which is the expected account of the world if we sampled meal worlds according to the collection world probability.
So note that it's interesting we don't even see docking the lens here and lighter in the JMs model.
All right so this of course should be plugged into this part.
So you might wonder, so where is docking lens.
Interestingly the docking lens is here in alpha sub d so this would be plugged into this part.
As a result what we get is the following function here and this is again a sum over all the match query words.
And we're against the queer, the query, time frequency here.
And you can interpret this as the element of a document vector, but this is no longer a single dot product, right?
Because we have this part, I know that n is the name of the query, right?
So that just means if we score this function, we have to take a sum over all the query words, and then do some adjustment of the score based on the document.
But it's still, it's still clear that it does documents lens modulation because this lens is in the denominator so a longer document will have a lower weight here.
And we can also see it has tf here and now idf.
Only that this time the form of the formula is different from the previous one in JMs one.
But intuitively it still implements TFIDF waiting and document lens rendition again, the form of the function is dictated by the probabilistic reasoning and assumptions that we have made.
Now there are also disadvantages of this approach.
And that is, there's no guarantee that there's such a form of the formula will actually work well.
So if we look about at this geo function, all those TF-IDF waiting and document lens rendition for example it's unclear whether we have sub-linear transformation.
Unfortunately we can see here there is a logarithm function here.
So we do have also the, so it's here right?
So we do have the sublinear transformation, but we do not intentionally do that.
That means there's no guarantee that we will end up in this, in this way.
Suppose we don't have logarithm, then there's no sub-linear transformation.
As we discussed before, perhaps the formula is not going to work so well.
So that's an example of the gap between a formal model like this and the relevance that we have to model, which is really a subject motion that is tied to users.
So it doesn't mean we cannot fix this.
For example, imagine if we did not have this logarithm, right?
So we can take a risk and we're going to add one, or we can even add double logarithm.
But then, it would mean that the function is no longer a proper risk model.
So the consequence of the modification is no longer as predictable as what we have been doing now.
So, that's also why, for example, PM45 remains very competitive and still, open channel how to use public risk models as they arrive, better model than the PM25.
In particular how do we use query like how to derive a model and that would work consistently better than DM 25.
Currently we still cannot do that.
Still interesting open question.
So to summarize this part, we've talked about the two smoothing methods.
Jelinek-Mercer which is doing the fixed coefficient linear interpolation.
Dirichlet Prior this is what add a pseudo counts to every word and is doing adaptive interpolation in that the coefficient would be larger for shorter documents.
In most cases we can see, by using these smoothing methods, we will be able to reach a retrieval function where the assumptions are clearly articulate.
So they are less heuristic.
Explaining the results also show that these, retrieval functions.
Also are very effective and they are comparable to BM 25 or pm lens adultation.
So this is a major advantage of probably smaller where we don't have to do a lot of heuristic design.
Yet in the end that we naturally implemented TF-IDF weighting and doc length normalization.
Each of these functions also has precise ones smoothing parameter.
In this case of course we still need to set this smoothing parameter.
There are also methods that can be used to estimate these parameters.
So overall, this shows by using a probabilistic model, we follow very different strategies then the vector space model.
Yet, in the end, we end up uh,with some retrievable functions that look very similar to the vector space model.
With some advantages in having assumptions clearly stated.
And then, the form dictated by a probabilistic model.
Now, this also concludes our discussion of the query likelihood probabilistic model.
And let's recall what assumptions we have made in order to derive the functions that we have seen in this lecture.
Well we basically have made four assumptions that I listed here.
The first assumption is that the relevance can be modeled by the query likelihood.
And the second assumption with med is, are query words are generated independently that allows us to decompose the probability of the whole query into a product of probabilities of old words in the query.
And then, the third assumption that we have made is, if a word is not seen, the document or in the late, its probability proportional to its probability in the collection.
That's a smoothing with a collection ama model.
And finally, we made one of these two assumptions about the smoothing.
So we either used JM smoothing or Dirichlet prior smoothing.
If we make these four assumptions then we have no choice but to take the form of the retrieval function that we have seen earlier.
Fortunately the function has a nice property in that it implements TF-IDF weighting and document machine and these functions also work very well.
So in that sense, these functions are less heuristic compared with the vector space model.
And there are many extensions of this, this basic model and you can find the discussion of them in the reference at the end of this lecture.
This lecture is about the, the basic measures for evaluation of text original systems.
In this lecture, we're going to discuss how we design basic measures  to quantitatively, compare two original  systems.
This is a slide that you have seen earlier in the lecture, where we talk about the grand evaluation methodology.
We can have a test collection that consists of queries, documents and relevance judgements.
We can then run two systems on these da, data sets to, quantitatively evaluate your performance.
And we raised to the question about, which settles results is better is System A better or System B better?
So let's now talk about how to actually quantify their performance.
Suppose we have a total of, of 10 random documents in the current folder for this query.
Now, the relevance judgements shown on the right, did not include all the ten obviously.
And we have only seen three rendered documents there but we can imagine there are other random documents in judging for this query.
So now, intuitively we thought that System A is better because it did not have much noise.
And in particular we have seen, amount of three results, two of them are relevant but in System B we have five results and only three of them are relevant.
So intuitively, it looks like System A is more accurate.
And this can be captured by a matching order precision.
Where we simply compute to what extent all the retrieval results are relevant.
If you have 100% precision that would mean all the retrieval documents are relevant.
So, in this case the system A has a Precision of two out of three.
System B as three over five.
And this shows that System A is better by Precision.
But we also talked about System B might be preferred by some other users hold like to retrieve as many relevant documents as possible.
So, in that case we have to compare the number of relevant documents that retrieve.
And there is an other measure called a Recall.
This measures the completeness of coverage of relevant documents in your retriever result.
So, we just assume that there are ten relevant documents in the collection.
And here we've got two of them in System A, so the recall is two out of ten.
Where as system B has got a three, so it's a three out of ten.
Now ,we can see by recall System B is better and these two measures turned out to be the very basic measures for evaluating search engine.
And they are very important because they are also widely used in many other testing variation problems.
For example, if you look at the applications of machine learning you tend to see precision recall numbers being reported for all kinds of tasks.
Okay, so now, let's define these two measures more precisely and these measures are to evaluate a set of retrieval documents.
So that means we are considering that approximation of a set of relevant documents.
We can distinguish it four cases, depending on the situation of a document.
A document that can be retrieved or not retrieved, right?
Because we're talking about the set of result.
The document can be also relevant or not relevant, depending on whether the user thinks this is a useful document.
So, we can now have counts of documents in each of the four categories.
We can have a to represent the number of documents that are retrieved and relevant, b for documents that are not retrieved but relevant, etc.
Now, with this table, then we have defined precision.
As the, ratio of, the relevant retriever documents A to the total number of retriever documents.
So this is just you know, a divided by the sum of a and c. The sum of this column.
Signal recall is defined by dividing a by the sum of a and b.
So that's, again, to divide a by the sum of the rule, instead of the column.
All right, so we going to see precision and recall is all focused on looking at the a, that's the number of retrieval relevant documents, but we're going to use different denominators.
Okay, so what would be an ideal result?
Well, you can able to see in ideal case we have precision and recall, all to be 1.0 that means we have got 1% of all the random documents in our results.
And all the results that we return are relevant.
[INAUDIBLE] There's no single not relevant document returned.
The reality however, high recall tends to be associated with low precision And you can imagine why that is the case.
As you go down the distant to try to get as many relevant actions as possible.
You tend to in time a lot of non relevant documents, so the precision goes down.
Look at this set, can also be defined by a cutoff in a ranked list.
That's why, although these two measures are defined for a set of retrieved documents, they are actually very useful for evaluating a ranked list.
They are the fundamental measures in tension retrieval and many other tasks.
We often are interested in to the precision up to ten documents for web search.
This means we look at the, how many documents among the top results are actually relevant.
Now, this is a very meaningful measure, because it tells us how many relevant documents a user can expect to see.
On the first page of search results, where they typically show ten results.
So, precision and recall are, the basic measures and we need to use them to further evaluate a search engine but they are the building blocks really.
We just to say that there tends to be a trade off between precision and recall.
So, naturally it would be interesting to  combine them and here's one measure that's often used, called f measure.
And it's harmonic mean of precision and recall, it's defined on this slide.
So you can see it first computed, inverse of R and P here and then it would be interpreted to by using a co, coefficients.
Depending on the parameter Beta and after some transformation we can easily see it would be of this form.
And in many cases it's just a combination of precision and recall.
And, and Beta is a parameter that's often set to one.
It can control the emphasis on precision or recall.
When we set, beta to one we end up by having a special case of F measure, often called F1.
This is a popular measure, that is often used as a combined precision and recall.
And the formula looks very simple it's just this, here.
Now it's easy to see that if you have, a larger precision or larger recall than F measure would be high.
But what's interesting is that, the trade off between precision and recall, is captured in an interesting way in F1.
So, in order to understand that, we, can first look at the natural question.
Why not just the, combining them using a simple arithmetic mean as a [INAUDIBLE] here.
That would be likely the most natural way of combining them.
So, what do you think?
If you want to think more, you can pause the media.
So why is this not as good as F1?
Or what's the problem with this?
Now, if you think about the arithmetic mean, you can see that this is the sum of, of multiple terms.
In this case, this is the sum of precision and recall.
In the case of the sum, the total value tends to be dominated by the large values.
That means if you have a very high P or a very high R, then you really don't care about the, whether the other varies is low.
So, the whole sum would be high.
Now, this is not the desirable because one can easily have a perfect recall.
We can have a perfect recall is it?
Can you imagine how?
It's probably very easy to imagine that we simply retrieve all the document in the collection, then we have a perfect recall and this will give us 0.5 as the average.
But search results are clearly not very useful for users, even though the, the average using this formula would be relatively high.
Now, in contrast, you can see F1 will reward a case where precision and recall are roughly but similar.
So, it would paralyze a case where you have extremely high matter for one of them.
So, this means F1 encodes a different trade off between that.
Now this example shows actually, a very important methodology here.
When we try to solve a problem, you might naturally think of one solution.
Let's say, in this case, it's this arithmetic mean.
But it's important that not to settle on this solution.
It's important to think whether you have other ways to combine them.
And once you think about the multiple variance.
It's important to analyze their difference and then think about which one makes more sense.
In this case, if you think more carefully you will feel that if one problem makes more sense.
Then the simple arithmetic mean.
Although in other cases, there may be, different results.
But in this case, the arithmetic mean, seems not reasonable.
But if you don't pay attention to these subtle differences, you might just, take an easy way to combine them and then go ahead with it.
And here later you'll find that, hm, the measure doesn't seem to work well.
Right so, at this methodology is actually very important in general in solving problem and try to think about the best solution.
Try to understand that the problem, very well and then know why you needed this measure, and why you need to combine precision and recall.
And then use that to guide you in finding a good way to solve the problem.
To summarize, we talk about precision, which addresses the question, are the retrieval results all relevant?
We'll also talk about the recall, which addresses the question, have all the relevant documents been retrieved?
These two are the two basic measures in testing retrieval in variation.
They are are used for, for many other tasks as well.
We'll talk about F measure as a way to combine precision and recall.
We also talked about the trade off between precision and recall.
And this turns out to depend on the users search tasks and we'll discuss this point more in the later lecture.
.
So let's take a look at this in detail.
So in this random surfing model.
And any page would assume random surfer would choose the next page to visit.
So this is a small graph here.
That's, of course an oversimplification of the complicate it well.
But let's say there are four documents here.
Right, D1, D2, D3 and D4.
And let's assume that a random surfer or random walker can be any of these pages.
And then the random surfer could decide to just randomly jump into any page.
Or follow a link and then visit the next page.
So if the random server is at d1.
Then, you know, with some probability that random surfer will follow the links.
Now there two outlinks here.
One is pointing to this D3.
The other is pointing to D4.
So the random surfer could pick any of these two to reach e3 and d4.
But it also assumes that the random surfer might, get bored sometimes.
So the random surfer would decide to ignore the actual links, and simply randomly jump to any page on the web.
So, if it does that, eh, it would be able to reach any of the other pages even though there is no link directly from to that page.
So this is the assume the randoms of.
Imagine a random server is really doing surfing like this, then we can ask the question.
How likely on average the server would actually reach a particular page d1, or d2, or d3.
That's the average probability of visiting a particular page.
And this probability is precisely what page rank computes.
So the page rank score of the document is the average probability that the surfer visits a particular page.
Now, intuitively this will basically kept you the [INAUDIBLE] link account.
Why?
Because if a page has a lot of in-links then it would have a higher chance of being visited, because there will be more opportunities of having the surfer to follow a link to come to this page.
And this is why the random surfing model actually captures the idea of counting the in links.
Note that is also considers the indirect in links.
Why?
Because if the pages that point to you have themselves a lot of in links, that would mean the random server would very likely reach one of them.
And therefore it increases the chance of visiting you.
So this is a nice way to capture both indirect and direct links.
So mathematically, how can we compute this problem enough to see that we need to take a look at how this problem [INAUDIBLE] in computing.
So first let's take a look at the transition matching sphere.
And this is just a matrix with values indicating how likely a rand, the random surfer will go from one page to another.
So each rule stands for a starting page.
For example, rule one would indicate the probability of going to any other four pages from e1.
And here we see there are only non two non zero entries.
Each is 1 over 2, a half.
So this is because if you look at the graph, d1 is pointing to d3 and d4.
There's no link from d1 to d1 server or d2, so we've got 0s for the first two columns and 0.5 for d3 and d4.
In general, the M in this matrix M sub i j is the probability of going from d, i, to d, j.
And obviously for each rule, the values should sum to one, because the surfer will have to go to precisely one of these other pages.
Right?
So this is a transition matrix.
Now how can we compute the probability of a server visiting a page?
Well if you look at the, the server model, then basically we can compute the probability of reaching a page as follows.
So, here on the left-hand side, you see it's the probability of visiting page DJ at time t plus 1 because it's the next time cont.
On the right hand side, you can see the question involves the probability of, at page ei at time t. So you can see the subsequent index t, here.
And that indicates that's the probability that the server was at a document at time t. So the equation basically captures the two possibilities of reaching at d j at time t plus 1.
What are these two possibilities?
Well one is through random surfing, and one is through following a link as we just explained.
So the first part captures the probability that the random server would reach this page by following a link.
And you can see, and the random surfer chooses this strategy was probably the [INAUDIBLE] as we assumed.
And so there is a factor of one minus alpha here.
But the main part is really sum over all the possible pages that the server could have been at time t, right?
There were N pages, so it's a sum over all the possible N pages.
Inside the sum is the product of two probabilities.
One is the probability that the server was at d i at time t. That's p sub t of d i.
The other is the transition probability from di to dj.
And so in order to reach this dj page, the surfer must first be at di at time t. And then also would have to follow the link to go from di to dj, so the probability is the probability of being at di at time t, not divide by the probability of, going from that page to the top of the page dj here.
The second part is a similar sum.
The only difference is that now the transition probability is uniform, transition probability.
And this part captures the probability of reaching this page, through random jumping.
Right.
So, the form is exactly the same.
And in, in, this also allows us to see why PageRank essentially assumes smoothing of the transition matrix.
If you think about this 1 over N as coming from another transition matrix that has all the elements being 1 over N, the uniform matrix.
Then you can see very clearly essentially we can merge the two parts.
Because they are of the same form, we can imagine there's a difference of metrics that's a combination of this m and that uniform matrix where every element is 1 over n. In this sense, page one uses this idea of smoothing and ensuring that there's no 0, entry in such a transition matrix.
Of course this is, time depend, calculation of probabilities.
Now, we can imagine if we want to compute average probabilities, the average probabilities probably would satisfy this equation without considering the time index.
So let's drop the time index and just assume that they would be equal.
Now this would give us N equations.
Because for each page we have such a equation.
And if you look at the what variables we have in these equations, there are also precisely N variables, right?
So this basically means we now have a system of n equations with n variables, and these are linear equations.
So basically, now the problem boils down to solve this system of equations and here I also show that the equations in the metric form.
It's the vector P here equals a metrics or the transports of the metrics here.
And multiply it by the vector again.
Now if you still remember some knowledge that you learned from linear algebra and then you will realize this is precisely the equation for item vector.
Right?
When [INAUDIBLE] metrics by this method you get the same value as this method.
And this can solved by using an iterative algorithm.
So is it, because she's here, on the ball, easily taken from the previous, slide.
So you see the, relationship between the, the page source of different pages.
And in this iterative approach or power approach we simply start with, randomly the p. And then we repeatedly just updated this p by multiplying.
The metrics here by this P-Vector.
So I also show a concrete example here.
So you can see this now, if we assume.
How far is point two.
Then with the example that we show here on this slide we have the original transition metrics here.
Right?
That encodes, that encodes the graph.
The actual links.
And we have this smoothing transition metrics, uniform transition metrics, representing random jumping.
And we can combine them together with interpolation to form another metrics that would be like this.
So essentially we can imagine now the looks.
Like this can be captured by that.
There are virtual links between all the pages now.
So the page rank algorithm will just initialize the p vector first, and then just computed the updating of this p vector by using this, metrics multiplication.
Now if you rewrite this metrics multi, multiplication in terms of just a, an individual equations, you'll see this.
And this is a, basically, the updating formula for this particular page is a, page ranking score.
So you can also see, even if you want to compute the value of this updated score for d1, you basically multiple this rule.
Right?
By this column, I will take the total product of the two, right?
And that will give us the value for this value.
So this is how we updated the vector.
We started with some initial values for these guys.
For, for this, and then, we just revise the scores which generate a new set of scores.
And the updated formula is this one.
So we just repeatedly apply this, and here it converges.
And when the metrics is like this.
Where there is no zero values and it can be guaranteed to converge.
And at that point we will just, have the PageRank scores for all the pages.
Now we typically set the initial values just to 1 over n. So interestingly, this update formula can be also interpreted as propagating scores on the graph.
All right.
Can you see why?
Well if you look at this formula and then compare that with this graph, and can you imagine how we might be able to interpret this as essentially propagating scores over the graph.
I hope you will see that indeed we can imagine we have values initialized on each of these page.
All right, so we can have values here that say, that's one over four for each.
And then welcome to use these matrix to update this, the scores.
And if you look at the equation here, this one, basically we're going to combine the scores of the pages that possible would lead to, reaching this page.
So we'll look at all the pages that are pointing to this page.
And then combine their scores and the propagated score, the sum of the scores to this document D1.
We look after the, the scores that represented the probability that the random server would be visiting the other pages before it reaches the D1.
And then just do the propagation to simulate the probability of reaching this, this page D 1.
So there are two interpretations here.
One is just the matrix multiplication.
And we repeated that.
Multiply the vector by this metrics.
The other is to just think of it as propagating the scores repeatedly on the web.
So in practice the composition of PageRank score is actually efficient because the metrices are sparse and there are some ways to transform the equation so you avoid actually literally computing the values of all of those elements.
Sometimes you may also normalize the equation, and that will give you a somewhat different form of the equation, but then the ranking of pages will not change.
The results of this potential problem of zero out link problem.
In that case if the page does not have any outlook, then the probability of these pages will, will not sum to 1.
Basically, the probability of reaching the next page from this page will not sum to 1.
Mainly because we have lost some probability mass when we assume that there's some probability that the server will try to follow links but then there's no link to follow, right?
And one possible solution is simply to use page specific damping factor and that, that could easily fix this.
Basically that's to say, how far do we want from zero for a page with no outlink.
In that case the server would just have to render them [INAUDIBLE] to another page instead of trying to follow the link.
So there are many extensions of page rank.
One extension is to do top-specific page rank.
Note that page rank doesn't really use the query format machine, right?
So, [INAUDIBLE] so we can make page rank, appear specific, however.
So, for example, in the topic specific page rank, we can simply assume when the surfer, is bored.
The surfer is not going to randomly jump into any page on the web.
Instead, it's going to jump, to only those pages that are to a query.
For example, if the query is about sports then we could assume that when it's doing random jumping, it's going to randomly jump to a sports page.
By doing this then we canbuy a PageRank to topic align with sports.
And then if you know the current query is about sports then you can use this specialized PageRank score to rank the options.
That would be better than if you use a generic PageRank score.
PageRank is also general algorithm that can be used in many other.
Locations for network analysis, particular for example for social networks.
We can imagine if you compute their PageRank scores for social network, where a link might indicate friendship relation, you'll get some meaningful scores for people.
.
This lecture is about the syntagmatic relation discovery and mutual information.
In this lecture we are going to continue discussing syntagmatic relation discovery.
In particular, we are going to talk about another the concept in the information series, we called it mutual information and how it can be used to discover syntagmatic relations.
Before we talked about the problem of conditional entropy and that is the conditional entropy computed different pairs of words.
It is not really comparable, so that makes it harder with this cover, strong synagmatic relations globally from corpus.
So now we are going to introduce mutual information, which is another concept in the information series that allows us to, sometimes, normalize the conditional entropy to make it more comparable across different pairs.
In particular, mutual information in order to find I(X:Y), matches the entropy reduction of X obtained from knowing Y.
More specifically the question we are interested in here is how much of an entropy of X can we obtain by knowing Y.
So mathematically it can be defined as the difference between the original entropy of X, and the condition of Y of X given Y.
And you might see, as you can see here it can also be defined as reduction of entropy of Y because of knowing X.
Now normally the two conditional interface H of X given Y and the entropy of Y given X are not equal, but interestingly, the reduction of entropy by knowing one of them, is actually equal.
So, this quantity is called a Mutual Information in order to buy I here.
And this function has some interesting properties, first it is also non-negative.
This is easy to understand because the original entropy is always not going to be lower than the possibility reduced conditional entropy.
In other words, the conditional entropy will never exceed the original entropy.
Knowing some information can always help us potentially, but will not hurt us in predicting x.
The signal property is that it is symmetric like additional entropy is not symmetrical, mutual information is, and the third property is that It reaches its minimum, zero, if and only if the two random variables are completely independent.
That means knowing one of them does not tell us anything about the other and this last property can be verified by simply looking at the equation above and it reaches 0 if and only the conditional entropy of X [INAUDIBLE] Y is exactly the same as original entropy of X.
So that means knowing why it did not help at all and that is when X and a Y are completely independent.
Now when we fix X to rank different Ys using conditional entropy would give the same order as ranking based on mutual information because in the function here, H(X) is fixed because X is fixed.
So ranking based on mutual entropy is exactly the same as ranking based on the conditional entropy of X given Y, but the mutual information allows us to compare different pairs of x and y.
So, that is why mutual information is more general and in general, more useful.
So, let us examine the intuition of using mutual information for Syntagmatical Relation Mining.
Now, the question we ask forcing that relation mining is, whenever "eats" occurs, what other words also tend to occur?
So this question can be framed as a mutual information question, that is, which words have high mutual information was eats, so computer the missing information between eats and other words.
And if we do that, and it is basically a base on the same as conditional we will see that words that are strongly associated with eats, will have a high point.
Whereas words that are not related will have lower mutual information.
For this, I will give some example here.
The mutual information between "eats" and "meats", which is the same as between "meats" and "eats," because the information is symmetrical is expected to be higher than the mutual information between eats and the, because knowing the does not really help us as a predictor.
It is similar, and knowing eats does not help us predicting, the as well.
And you also can easily see that the mutual information between a word and itself is the largest, which is equal to the entropy of this word and so, because in this case the reduction is maximum because knowing one allows us to predict the other completely.
So the conditional entropy is zero, therefore the mutual information reaches its maximum.
It is going to be larger, then are equal to the machine volume eats in other words.
In other words picking any other word and the computer picking between eats and that word.
You will not get any information larger the computation from eats and itself.
So now let us look at how to compute the mute information.
Now in order to do that, we often use a different form of mutual information, and we can mathematically rewrite the mutual information into the form shown on this slide.
Where we essentially see a formula that computes what is called a KL-divergence or divergence.
This is another term in information theory.
It measures the divergence between two distributions.
Now, if you look at the formula, it is also sum over many combinations of different values of the two random variables but inside the sum, mainly we are doing a comparison between two joint distributions.
The numerator has the joint, actual observed the joint distribution of the two random variables.
The bottom part or the denominator can be interpreted as the expected joint distribution of the two random variables, if they were independent because when two random variables are independent, they are joined distribution is equal to the product of the two probabilities.
So this comparison will tell us whether the two variables are indeed independent.
If they are indeed independent then we would expect that the two are the same, but if the numerator is different from the denominator, that would mean the two variables are not independent and that helps measure the association.
The sum is simply to take into consideration of all of the combinations of the values of these two random variables.
In our case, each random variable can choose one of the two values, zero or one, so we have four combinations here.
If we look at this form of mutual information, it shows that the mutual information matches the divergence of the actual joint distribution from the expected distribution under the independence assumption.
The larger this divergence is, the higher the mutual information would be.
So now let us further look at what are exactly the probabilities, involved in this formula of mutual information.
And here, this is all the probabilities involve, and it is easy for you to verify that.
Basically, we have first to [INAUDIBLE] probabilities corresponding to the presence or absence of each word.
So, for w1, we have two probabilities shown here.
They should sum to one, because a word can either be present or absent.
In the segment, and similarly for the second word, we also have two probabilities representing presence or absences of this word, and there is some to y as well.
And finally, we have a lot of joined probabilities that represent the scenarios of co-occurrences of the two words, and they are shown here.
And they sum to one because the two words can only have these four possible scenarios.
Either they both occur, so in that case both variables will have a value of one, or one of them occurs.
There are two scenarios.
In these two cases one of the random variables will be equal to one and the other will be zero and finally we have the scenario when none of them occurs.
This is when the two variables taking a value of zero.
So these are the probabilities involved in the calculation of mutual information, over here.
Once we know how to calculate these probabilities, we can easily calculate the new gene formation.
It is also interesting to know that there are actually some relations or constraint among these probabilities, and we already saw two of them, right?
So in the previous slide, that you have seen that the marginal probabilities of these words sum to one and we also have seen this constraint, that says the two words have these four scenarios of co-occurrency, but we also have some additional constraints listed in the bottom.
For example, this one means if we add up the probabilities that we observe the two words occur together and the probabilities when the first word occurs and the second word does not occur.
We get exactly the probability that the first word is observed.
In other words, when the word is observed.
When the first word is observed, and there are only two scenarios, depending on whether the second word is also observed.
So, this probability captures the first scenario when the second word actually is also observed, and this captures the second scenario when the second word is not observed.
So, we only see the first word, and it is easy to see the other equations also follow the same reasoning.
Now these equations allow us to compute some probabilities based on other probabilities, and this can simplify the computation.
So more specifically, if we know the probability that a word is present, like in this case, so if we know this, and if we know the probability of the presence of the second word, then we can easily compute the absence probability, right?
It is very easy to use this equation to do that, and so we take care of the computation of these probabilities of presence and absence of each word.
Now let's look at the [INAUDIBLE] distribution.
Let us assume that we also have available the probability that they occurred together.
Now it is easy to see that we can actually compute all the rest of these probabilities based on these.
Specifically for example using this equation we can compute the probability that the first word occurred and the second word did not, because we know these probabilities in the boxes, and similarly using this equation we can compute the probability that we observe only the second word.
Word.
And then finally, this probability can be calculated by using this equation because now this is known, and this is also known, and this is already known, right.
So this can be easier to calculate.
So now this can be calculated.
So this slide shows that we only need to know how to compute these three probabilities that are shown in the boxes, naming the presence of each word and the co-occurence of both words, in a segment.
.
There are many more advanced learning algorithms than the regression based reproaches.
And they generally account to theoretically optimize or retrieval method.
Like map or nDCG.
Note that the optimization objecting function that we have seen on the previous slide is not directly related to retrieval measure.
Right?
By maximizing the prediction of one or zero.
Or we don't necessarily optimize the ranking of those documents.
One can imagine that why, our prediction may not be too bad and let's say both are around 0.5.
So it's kind of in the middle of zero and one for the two documents, but the ranking can be wrong.
So we might have the, a larger value for.
D2 and then e1.
So that won't be good from retrieval perspective, even though by likelihood function, it's not bad.
In contrast, we might have another case where we predicted values.
Or around 0.9 let's say, and by the objective function, the error will be larger, but if we can get the order of the two documents correct, that's actually a better result.
So these new more advanced approaches will try to correct that problem.
Of course then the challenge is that.
That the optimization problem will be harder to solve.
And then researchers have proposed many solutions to the problem.
And you can read more of the references at the end.
Know more about the these approaches.
Now these learning to random approaches.
Are actually general, so they can also be applied to many other ranking problems, not just retrieval problem.
So here I list some for example recommender systems, computational adv, advertising, or summarization, and there are many others that you can probably encounter in your applications.
To summarize this lecture, we have talked about, using machine learning to combine much more features to incorporate a ranking without.
Actually the use of machine learning, in information retrieval has started since many decades ago.
So for example on the Rocchio feedback approach that we talked about earlier was a machine learning approach applied to to learn this feedback, but the most reasonable use of machine learning has been driven by some changes.
In the environment of applications of retrieval systems.
And first it's, mostly, driven by the availability of a lot of training data in the form of clicks rules.
Such data weren't available before.
So the data can provide a lot of useful knowledge about relevance and machine learning methods can be applied to leverage this.
Secondly it's also due by the need of combining them.
In the features.
And this is not only just because there are more features available on the web that can be naturally re-used with improved scoring.
It's also because by combining them, we can improve the robustness of ranking.
So this is designed for combating spams.
Modern search engines all use some kind of machine learning techniques to combine many features to optimize ranking and this is a major feature of these current engines such as Google, Bing.
The topic of learning to rank is still active research.
Topic in the community, and so you can expect to see new results being developed, in the next, few years.
Perhaps.
Here are some additional readings that can give you more information about.
About, how learning to rank books and also some advanced methods.
